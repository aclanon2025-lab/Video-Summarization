!pip install datasets
!pip install evaluate
import evaluate
!pip install rouge_score
from google.colab import drive
import os
import pandas as pd
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments
from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split
from google.colab import drive
import evaluate

# Disable wandb logging
os.environ["WANDB_DISABLED"] = "true"

# Mount Google Drive (if necessary)
drive.mount('/content/drive')

# Step 1: Load the dataset
df = pd.read_excel("/content/drive/My Drive/Thesis_Dataset/thesis_merged_2K.xlsx")  # Adjust path

# Ensure the dataset has two columns: 'text' and 'summary'
df = df.rename(columns={df.columns[0]: "text", df.columns[1]: "summary"})
df = df.dropna()

# Step 2: Split the dataset (80% for training, 20% for validation)
train_df, val_df = train_test_split(df, test_size=0.20, random_state=42)

# Convert pandas DataFrames into Hugging Face Dataset objects
train_dataset = Dataset.from_pandas(train_df)
val_dataset = Dataset.from_pandas(val_df)

# Combine datasets into a DatasetDict for Hugging Face Trainer
dataset_dict = DatasetDict({"train": train_dataset, "val": val_dataset})

# Step 3: Load the tokenizer and model (using Bangla T5 for summarization)
model_name = "facebook/mbart-large-50"  # mBART large model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
# Set the source and target language for mBART (Bangla)
tokenizer.src_lang = "bn_XX"  # Bangla source language token
model.config.src_lang = "bn_XX"  # Set source language in model config

# Tokenization function for summarization
def preprocess_function(examples):
    # Set language before tokenizing
    inputs = tokenizer(examples['text'], max_length=512, truncation=True, padding="max_length")
    labels = tokenizer(examples['summary'], max_length=150, truncation=True, padding="max_length")
    
    # Adding the 'labels' field for sequence-to-sequence tasks (summarization)
    inputs["labels"] = labels["input_ids"]
    
    # Replace padding token ids with -100 to ignore them in loss calculation
    inputs["labels"] = [
        [(label if label != tokenizer.pad_token_id else -100) for label in label_ids]
        for label_ids in inputs["labels"]
    ]
    
    return inputs

# Example dataset (for testing)
train_dataset = {"text": ["এটি একটি উদাহরণ", "আরেকটি উদাহরণ"], "summary": ["এটি সঙ্কলিত", "আরেকটি সঙ্কলিত"]}

# Apply tokenization to the train dataset (example)
tokenized_train = preprocess_function(train_dataset)
print(tokenized_train)

# Apply tokenization to the train and validation datasets
tokenized_datasets = dataset_dict.map(preprocess_function, batched=True)

# Step 4: Set up training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="/content/drive/MyDrive/Thesis_Dataset/results_mBART",
    logging_dir="/content/drive/MyDrive/Thesis_Dataset/logs_mBART",

    
      do_eval=True,
    eval_steps=200,  # or some suitable integer step

    learning_rate=2e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=2,
    weight_decay=0.01,
     save_strategy="no",  # add to your Seq2SeqTrainingArguments

    logging_steps=10,
    predict_with_generate=True,
    run_name="bangla_summarization_run_mBART"  # Optional: set a different run name
)

# Step 5: Define a trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['val'],
    tokenizer=tokenizer,
)

# Step 6: Train the model
trainer.train()

# Step 7: Save the fine-tuned model and tokenizer
def save_model(model, output_dir):
    for param in model.parameters():
        if not param.is_contiguous():
            param.data = param.data.contiguous()
    model.save_pretrained(output_dir)

save_model(model, "/content/drive/My Drive/Thesis_Dataset/fine_tuned_mBART")
tokenizer.save_pretrained("/content/drive/My Drive/Thesis_Dataset/fine_tuned_mBART")

# Step 12: Generate summary using the fine-tuned model
def generate_summary(text, model, tokenizer, device):
    model.to(device)
    
    # Tokenize the input text
    inputs = tokenizer(text, max_length=512, truncation=True, return_tensors="pt", padding="max_length")
    
    # Move input tensors to the same device as the model
    inputs = {key: value.to(device) for key, value in inputs.items()}
    
    # Generate summary
    summary_ids = model.generate(
        inputs['input_ids'],
        max_length=512,
        num_beams=3,
        no_repeat_ngram_size=2,
        early_stopping=True
    )
    
    # Decode and return the summary
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

import torch

# Step 1: Define device (Check if GPU is available, otherwise use CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Step 2: Move the model to the appropriate device
model.to(device)

# Step 13: Generate summaries for the validation dataset
val_df['generated_summary'] = val_df['text'].apply(lambda x: generate_summary(x, model, tokenizer, device))

# Step 14: Load the ROUGE metric from the 'evaluate' library
rouge = evaluate.load('rouge')

# Step 15: Calculate the final ROUGE score for all predictions and references
references = val_df['summary'].tolist()
predictions = val_df['generated_summary'].tolist()

# Compute the final ROUGE score across the whole validation set
final_rouge_score = rouge.compute(predictions=predictions, references=references)

# Print the final ROUGE scores
print(f"Final ROUGE Score: {final_rouge_score}")

# Save the output to a new Excel file with generated summaries and final ROUGE scores
val_df.to_excel("/content/drive/My Drive/Thesis_Dataset/test_data_with_summaries_and_rouge_mBART.xlsx", index=False)

# Print the result with generated summaries
print(val_df[['text', 'summary', 'generated_summary']])
