{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "52e0c0a563f0423b8d1cd4e634a2d3c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_16292631949741518646e2402c975565",
              "IPY_MODEL_364b17174f09438297ea8c7aad0eaf34",
              "IPY_MODEL_2378909e63b2498f867c24286e5b7c16"
            ],
            "layout": "IPY_MODEL_9516e05d49564f73bffb98da3b3b084c"
          }
        },
        "16292631949741518646e2402c975565": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cda1f5d988447a79cac39bea1650640",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e279653a276547fe89d681e05454f800",
            "value": "Transcribing‚Äáaudio‚Äáchunks:‚Äá100%"
          }
        },
        "364b17174f09438297ea8c7aad0eaf34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5b301cc3d0e4574ac78d2a9f28899f1",
            "max": 218,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0dae4f56fdeb4795b7f6b1a8d02cdd6f",
            "value": 218
          }
        },
        "2378909e63b2498f867c24286e5b7c16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdcd7432ddc548ac8af71421141f49a4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_761ea27cc89d424d8939790517c845d6",
            "value": "‚Äá218/218‚Äá[37:29&lt;00:00,‚Äá‚Äá8.62s/it]"
          }
        },
        "9516e05d49564f73bffb98da3b3b084c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cda1f5d988447a79cac39bea1650640": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e279653a276547fe89d681e05454f800": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5b301cc3d0e4574ac78d2a9f28899f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dae4f56fdeb4795b7f6b1a8d02cdd6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bdcd7432ddc548ac8af71421141f49a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "761ea27cc89d424d8939790517c845d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "52e0c0a563f0423b8d1cd4e634a2d3c8",
            "16292631949741518646e2402c975565",
            "364b17174f09438297ea8c7aad0eaf34",
            "2378909e63b2498f867c24286e5b7c16",
            "9516e05d49564f73bffb98da3b3b084c",
            "8cda1f5d988447a79cac39bea1650640",
            "e279653a276547fe89d681e05454f800",
            "c5b301cc3d0e4574ac78d2a9f28899f1",
            "0dae4f56fdeb4795b7f6b1a8d02cdd6f",
            "bdcd7432ddc548ac8af71421141f49a4",
            "761ea27cc89d424d8939790517c845d6"
          ]
        },
        "id": "GIxFzDyWNdj1",
        "outputId": "1b5c445f-c93d-4cc6-bcad-3b0b0432640a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:<>:2119: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2120: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2121: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2122: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2123: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2124: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2125: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2126: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2127: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2128: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2129: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2130: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2131: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2132: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2133: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2134: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2135: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2136: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2137: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2138: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2139: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2140: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2141: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2142: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2143: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2144: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2119: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2120: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2121: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2122: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2123: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2124: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2125: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2126: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2127: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2128: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2129: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2130: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2131: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2132: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2133: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2134: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2135: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2136: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2137: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2138: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2139: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2140: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2141: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2142: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2143: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:<>:2144: SyntaxWarning: invalid escape sequence '\\('\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2119: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\(‡¶è \": \"\\(a \",\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2120: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\(‡¶¨‡¶ø \": \"\\(b \",\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2121: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\(‡¶∏‡¶ø \": \"\\(c \",\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2122: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\(‡¶°‡¶ø \": \"\\(d \",\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2123: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\(‡¶á \": \"\\(e \",\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2124: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\(‡¶è‡¶´ \": \"\\(f \",\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2125: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\(‡¶ú‡¶ø \": \"\\(g \",\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2126: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\(‡¶è‡¶á‡¶ö \": \"\\(h \",\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2127: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\(‡¶Ü‡¶á \": \"\\(i \",\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2128: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\(‡¶ú‡ßá \": \"\\(j \",\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2129: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\(‡¶ï‡ßá \": \"\\(k \",\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2130: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\(‡¶è‡¶≤ \": \"\\(l \",\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2131: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\(‡¶è‡¶Æ \": \"\\(m \",\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2132: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\(‡¶è‡¶® \": \"\\(n \",\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2133: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\(‡¶ì \": \"\\(o \",\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2134: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\(‡¶™‡¶ø \": \"\\(p \",\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2135: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\(‡¶ï‡¶ø‡¶â \": \"\\(q \",\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2136: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\(‡¶Ü‡¶∞ \": \"\\(r \",\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2137: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\(‡¶è‡¶∏ \": \"\\(s \",\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2138: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\(‡¶ü‡¶ø \": \"\\(t \",\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2139: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\(‡¶á‡¶â \": \"\\(u \",\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2140: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\(‡¶≠‡¶ø \": \"\\(v \",\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2141: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\((‡¶°‡¶¨‡ßç‡¶≤‡¶ø‡¶â‡•§‡¶°‡¶æ‡¶¨‡¶≤‡¶ø‡¶â) \": \"\\(w \",\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2142: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\(‡¶è‡¶ï‡ßç‡¶∏ \": \"\\(x \",\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2143: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\(‡¶ì‡¶Ø‡¶º‡¶æ‡¶á |\\(‡¶ì‡ßü‡¶æ‡¶á \": \"\\(y \",\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-996556076.py:2144: SyntaxWarning: invalid escape sequence '\\('\n",
            "  r\"\\(‡¶ú‡ßá‡¶° \": \"\\(z \",\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: flask-cors in /usr/local/lib/python3.12/dist-packages (6.0.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.3.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.4)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Collecting git+https://github.com/csebuetnlp/normalizer\n",
            "  Cloning https://github.com/csebuetnlp/normalizer to /tmp/pip-req-build-uw0w2kg8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/csebuetnlp/normalizer /tmp/pip-req-build-uw0w2kg8\n",
            "  Resolved https://github.com/csebuetnlp/normalizer to commit d405944dde5ceeacb7c2fd3245ae2a9dea5f35c9\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from normalizer==0.0.1) (2024.11.6)\n",
            "Requirement already satisfied: emoji==1.4.2 in /usr/local/lib/python3.12/dist-packages (from normalizer==0.0.1) (1.4.2)\n",
            "Requirement already satisfied: ftfy==6.0.3 in /usr/local/lib/python3.12/dist-packages (from normalizer==0.0.1) (6.0.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy==6.0.3->normalizer==0.0.1) (0.2.13)\n",
            "Requirement already satisfied: banglanlptoolkit in /usr/local/lib/python3.12/dist-packages (1.1.9)\n",
            "Requirement already satisfied: transformers>=4.42.4 in /usr/local/lib/python3.12/dist-packages (from banglanlptoolkit) (4.55.4)\n",
            "Requirement already satisfied: torch>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from banglanlptoolkit) (2.8.0+cu126)\n",
            "Requirement already satisfied: bnunicodenormalizer>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from banglanlptoolkit) (0.1.7)\n",
            "Requirement already satisfied: sentencepiece==0.2.0 in /usr/local/lib/python3.12/dist-packages (from banglanlptoolkit) (0.2.0)\n",
            "Requirement already satisfied: langdetect==1.0.9 in /usr/local/lib/python3.12/dist-packages (from banglanlptoolkit) (1.0.9)\n",
            "Requirement already satisfied: pandarallel in /usr/local/lib/python3.12/dist-packages (from banglanlptoolkit) (1.6.5)\n",
            "Requirement already satisfied: pqdm in /usr/local/lib/python3.12/dist-packages (from banglanlptoolkit) (0.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect==1.0.9->banglanlptoolkit) (1.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.1->banglanlptoolkit) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.1->banglanlptoolkit) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.1->banglanlptoolkit) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.1->banglanlptoolkit) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.1->banglanlptoolkit) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.1->banglanlptoolkit) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.1->banglanlptoolkit) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.1->banglanlptoolkit) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.1->banglanlptoolkit) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.1->banglanlptoolkit) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.1->banglanlptoolkit) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.1->banglanlptoolkit) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.1->banglanlptoolkit) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.1->banglanlptoolkit) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.1->banglanlptoolkit) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.1->banglanlptoolkit) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.1->banglanlptoolkit) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.1->banglanlptoolkit) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.1->banglanlptoolkit) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.1->banglanlptoolkit) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.1->banglanlptoolkit) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.1->banglanlptoolkit) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.42.4->banglanlptoolkit) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.42.4->banglanlptoolkit) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.42.4->banglanlptoolkit) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.42.4->banglanlptoolkit) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.42.4->banglanlptoolkit) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.42.4->banglanlptoolkit) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.42.4->banglanlptoolkit) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.42.4->banglanlptoolkit) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.42.4->banglanlptoolkit) (4.67.1)\n",
            "Requirement already satisfied: dill>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from pandarallel->banglanlptoolkit) (0.3.8)\n",
            "Requirement already satisfied: pandas>=1 in /usr/local/lib/python3.12/dist-packages (from pandarallel->banglanlptoolkit) (2.2.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from pandarallel->banglanlptoolkit) (5.9.5)\n",
            "Requirement already satisfied: bounded-pool-executor in /usr/local/lib/python3.12/dist-packages (from pqdm->banglanlptoolkit) (0.0.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.42.4->banglanlptoolkit) (1.1.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1->pandarallel->banglanlptoolkit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1->pandarallel->banglanlptoolkit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1->pandarallel->banglanlptoolkit) (2025.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.3.1->banglanlptoolkit) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.3.1->banglanlptoolkit) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.42.4->banglanlptoolkit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.42.4->banglanlptoolkit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.42.4->banglanlptoolkit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.42.4->banglanlptoolkit) (2025.8.3)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.12/dist-packages (from moviepy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.32.4)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.1.12)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.0.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.37.0)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/dist-packages (from imageio<3.0,>=2.5->moviepy) (11.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2025.8.3)\n",
            "Requirement already satisfied: SpeechRecognition==3.10.0 in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from SpeechRecognition==3.10.0) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->SpeechRecognition==3.10.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->SpeechRecognition==3.10.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->SpeechRecognition==3.10.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->SpeechRecognition==3.10.0) (2025.8.3)\n",
            "Requirement already satisfied: pydub==0.25.1 in /usr/local/lib/python3.12/dist-packages (0.25.1)\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:2 https://cli.github.com/packages stable InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,202 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,782 kB]\n",
            "Fetched 12.0 MB in 3s (4,409 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "Enter your ngrok auth token: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "üîÑ Initializing model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ BanglaPunctuation loaded\n",
            "‚úÖ VideoProcessor initialized successfully!\n",
            "üîÑ Initializing model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ BanglaPunctuation loaded\n",
            "‚úÖ Model initialized successfully!\n",
            "üåê Public URL: NgrokTunnel: \"https://a9db8460d4f8.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            "üîó Access your app at: NgrokTunnel: \"https://a9db8460d4f8.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:51:51] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:51:52] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:55:07] \"POST /api/process HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üé¨ Loading video: /tmp/video_processing_1756468506036/shama_1.mp4\n",
            "‚è±Ô∏è Video duration: 163.0 minutes\n",
            "üéµ Extracting audio to: /tmp/video_processing_1756468506036/extracted_audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:55:11] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:55:14] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:55:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:55:20] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:55:23] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:55:26] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:55:29] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:55:32] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:55:35] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:55:38] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:55:41] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:55:44] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:55:47] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:55:50] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:55:53] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:55:56] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:55:59] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:56:02] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:56:05] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:56:08] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:56:11] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:56:14] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:56:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:56:20] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:56:23] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:56:26] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:56:29] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:56:32] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Audio extracted successfully! (1644.8 MB)\n",
            "‚úÖ Optimized speech recognizer initialized\n",
            "‚ö° Per-chunk amplification: True\n",
            "üåç Target language: Bengali (bn-BD)\n",
            "üéØ Audio chunk size: 45.0 seconds\n",
            "üìù Token-based final chunks: 512 tokens max\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:56:36] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:56:42] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Audio loaded: 9777.2 seconds, Avg: -29.4 dB, Peak: -0.1 dB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:56:43] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Step 1: Transcribing audio in 218 short chunks for sentence mapping...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Transcribing audio chunks:   0%|          | 0/218 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52e0c0a563f0423b8d1cd4e634a2d3c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:56:44] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:56:47] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:56:50] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:56:53] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:56:56] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:56:59] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:57:02] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:57:05] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:57:08] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:57:11] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:57:14] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:57:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:57:20] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:57:23] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:57:26] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:57:29] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:57:32] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:57:35] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:57:38] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:57:41] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:58:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 11:59:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:00:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:01:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:02:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:03:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:04:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:05:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:06:06] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:06:08] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:06:11] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:06:14] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:06:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:06:20] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:06:23] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:06:26] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:06:29] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:06:32] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:06:35] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:06:38] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:06:41] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:06:44] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:06:47] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:06:50] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:06:53] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:06:56] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:06:59] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:07:02] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:07:05] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:07:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:08:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:09:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:10:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:11:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:12:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:13:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:14:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:15:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:16:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:17:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:18:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:19:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:20:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:21:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:22:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:23:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:24:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:25:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:26:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:27:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:28:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:29:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:30:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:31:16] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:31:20] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:31:23] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:31:26] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:31:29] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:31:32] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:31:35] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:31:38] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:31:41] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:31:44] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:31:47] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:31:50] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:31:53] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:31:56] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:31:59] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:32:02] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:32:05] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:32:08] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:32:11] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:32:14] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:32:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:32:20] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:32:23] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:32:26] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:32:29] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:32:32] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:32:35] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:32:38] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:32:41] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:32:44] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:32:47] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:32:50] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:32:53] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:32:56] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:32:59] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:33:02] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:33:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Step 2: Creating sentence-level timestamp mapping with carry-over handling...\n",
            "‚úÖ Created 1107 sentences with timestamps\n",
            "üîÑ Step 3: Creating token-based chunks (max 512 tokens) with overlap and carry-over...\n",
            "‚úÖ Created 28 token-based chunks\n",
            "üìä Step 4: Preparing output data...\n",
            "üìÑ Input has ‚â§50 rows. Generating summaries directly...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:34:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:35:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame shape: (28, 15)\n",
            "Number of rows: 28\n",
            "File saved successfully to: /content/drive/MyDrive/tfidf/new.txt\n",
            "‚úÖ File copied to: /content/drive/MyDrive/tfidf/Concatenated_Summary/new.txt\n",
            "Initializing Enhanced Bangla Combined Keyword Extractor...\n",
            "Error initializing BNLP components: name 'BasicTokenizer' is not defined\n",
            "Using fallback tokenization (simple split)\n",
            "Successfully loaded 970 stopwords (no tokenization - tokenizer not available)\n",
            "\n",
            "Loading documents from: /content/drive/MyDrive/tfidf/Concatenated_Summary\n",
            "Found 213 .txt files.\n",
            "Building corpus vocabulary...\n",
            "Built corpus vocabulary: 13814 prominent terms\n",
            "Processing documents with combined unigram-bigram extraction...\n",
            "Added document: sum2.txt with 4902 unigrams, 2319 bigrams\n",
            "Added document: sum22.txt with 1861 unigrams, 1002 bigrams\n",
            "Added document: sum45.txt with 1509 unigrams, 880 bigrams\n",
            "Added document: sum51.txt with 2662 unigrams, 1650 bigrams\n",
            "Added document: sum3.txt with 3819 unigrams, 2068 bigrams\n",
            "Added document: sum9.txt with 3393 unigrams, 1733 bigrams\n",
            "Added document: sum56.txt with 2567 unigrams, 1397 bigrams\n",
            "Added document: sum5.txt with 4000 unigrams, 1614 bigrams\n",
            "Added document: sum27.txt with 1516 unigrams, 777 bigrams\n",
            "Added document: sum28.txt with 15201 unigrams, 8764 bigrams\n",
            "Added document: sum30.txt with 2488 unigrams, 1312 bigrams\n",
            "Added document: sum25.txt with 2461 unigrams, 1343 bigrams\n",
            "Added document: sum26.txt with 1853 unigrams, 770 bigrams\n",
            "Added document: sum13.txt with 5086 unigrams, 2817 bigrams\n",
            "Added document: sum20.txt with 2846 unigrams, 1547 bigrams\n",
            "Added document: sum48.txt with 2338 unigrams, 1203 bigrams\n",
            "Added document: sum47.txt with 2175 unigrams, 1255 bigrams\n",
            "Added document: sum8.txt with 2690 unigrams, 1612 bigrams\n",
            "Added document: sum6.txt with 6047 unigrams, 3306 bigrams\n",
            "Added document: sum38.txt with 2666 unigrams, 1300 bigrams\n",
            "Processed 20/213 documents...\n",
            "Added document: sum39.txt with 1314 unigrams, 620 bigrams\n",
            "Added document: sum19.txt with 6337 unigrams, 3535 bigrams\n",
            "Added document: sum23.txt with 2083 unigrams, 1039 bigrams\n",
            "Added document: sum24.txt with 2156 unigrams, 1105 bigrams\n",
            "Added document: sum34.txt with 1977 unigrams, 980 bigrams\n",
            "Added document: sum52.txt with 1081 unigrams, 558 bigrams\n",
            "Added document: sum1.txt with 4449 unigrams, 2045 bigrams\n",
            "Added document: sum36.txt with 2426 unigrams, 1455 bigrams\n",
            "Added document: sum21.txt with 3014 unigrams, 1659 bigrams\n",
            "Added document: sum15.txt with 5682 unigrams, 3506 bigrams\n",
            "Added document: sum54.txt with 4628 unigrams, 2603 bigrams\n",
            "Added document: sum58.txt with 5530 unigrams, 3228 bigrams\n",
            "Added document: sum16.txt with 2250 unigrams, 1315 bigrams\n",
            "Added document: sum29.txt with 7585 unigrams, 4253 bigrams\n",
            "Added document: sum14.txt with 3942 unigrams, 1892 bigrams\n",
            "Added document: sum55.txt with 4033 unigrams, 2396 bigrams\n",
            "Added document: sum12.txt with 2668 unigrams, 1404 bigrams\n",
            "Added document: 30.txt with 192 unigrams, 103 bigrams\n",
            "Added document: sum33.txt with 1829 unigrams, 993 bigrams\n",
            "Added document: 93.txt with 1616 unigrams, 748 bigrams\n",
            "Processed 40/213 documents...\n",
            "Added document: 70.txt with 1414 unigrams, 765 bigrams\n",
            "Added document: 81.txt with 838 unigrams, 390 bigrams\n",
            "Added document: 18.txt with 176 unigrams, 93 bigrams\n",
            "Added document: 149.txt with 700 unigrams, 426 bigrams\n",
            "Added document: sum35.txt with 3554 unigrams, 2006 bigrams\n",
            "Added document: sum40.txt with 16829 unigrams, 9767 bigrams\n",
            "Added document: 74.txt with 2149 unigrams, 1111 bigrams\n",
            "Added document: 137.txt with 983 unigrams, 450 bigrams\n",
            "Added document: 120.txt with 1461 unigrams, 825 bigrams\n",
            "Added document: 104.txt with 1867 unigrams, 1066 bigrams\n",
            "Added document: 77.txt with 1155 unigrams, 600 bigrams\n",
            "Added document: sum43.txt with 1628 unigrams, 901 bigrams\n",
            "Added document: 20.txt with 123 unigrams, 61 bigrams\n",
            "Added document: 117.txt with 1006 unigrams, 586 bigrams\n",
            "Added document: 153.txt with 2006 unigrams, 811 bigrams\n",
            "Added document: sum17.txt with 4042 unigrams, 2139 bigrams\n",
            "Added document: 45.txt with 1580 unigrams, 899 bigrams\n",
            "Added document: 11.txt with 127 unigrams, 57 bigrams\n",
            "Added document: 76.txt with 1590 unigrams, 810 bigrams\n",
            "Added document: 118.txt with 1349 unigrams, 833 bigrams\n",
            "Processed 60/213 documents...\n",
            "Added document: 22.txt with 163 unigrams, 78 bigrams\n",
            "Added document: 52.txt with 2367 unigrams, 1405 bigrams\n",
            "Added document: 7.txt with 3105 unigrams, 1872 bigrams\n",
            "Added document: 16.txt with 303 unigrams, 162 bigrams\n",
            "Added document: 84.txt with 1968 unigrams, 912 bigrams\n",
            "Added document: 38.txt with 1730 unigrams, 939 bigrams\n",
            "Added document: 33.txt with 180 unigrams, 111 bigrams\n",
            "Added document: 12.txt with 116 unigrams, 59 bigrams\n",
            "Added document: 90.txt with 3304 unigrams, 1950 bigrams\n",
            "Added document: 144.txt with 1113 unigrams, 548 bigrams\n",
            "Added document: 85.txt with 1821 unigrams, 907 bigrams\n",
            "Added document: 91.txt with 2243 unigrams, 1219 bigrams\n",
            "Added document: 6.txt with 3029 unigrams, 1556 bigrams\n",
            "Added document: 49.txt with 1705 unigrams, 1014 bigrams\n",
            "Added document: 58.txt with 1081 unigrams, 407 bigrams\n",
            "Added document: 82.txt with 2004 unigrams, 1089 bigrams\n",
            "Added document: 141.txt with 1072 unigrams, 536 bigrams\n",
            "Added document: 23.txt with 238 unigrams, 124 bigrams\n",
            "Added document: 28.txt with 266 unigrams, 134 bigrams\n",
            "Added document: 13.txt with 169 unigrams, 91 bigrams\n",
            "Processed 80/213 documents...\n",
            "Added document: 132.txt with 670 unigrams, 342 bigrams\n",
            "Added document: 139.txt with 1286 unigrams, 673 bigrams\n",
            "Added document: 9.txt with 232 unigrams, 152 bigrams\n",
            "Added document: 99.txt with 3371 unigrams, 1868 bigrams\n",
            "Added document: 32.txt with 180 unigrams, 103 bigrams\n",
            "Added document: 4.txt with 407 unigrams, 218 bigrams\n",
            "Added document: 54.txt with 1413 unigrams, 589 bigrams\n",
            "Added document: sum18.txt with 3691 unigrams, 2104 bigrams\n",
            "Added document: 148.txt with 932 unigrams, 596 bigrams\n",
            "Added document: 147.txt with 978 unigrams, 559 bigrams\n",
            "Added document: 69.txt with 1547 unigrams, 763 bigrams\n",
            "Added document: 48.txt with 1487 unigrams, 872 bigrams\n",
            "Added document: 72.txt with 1159 unigrams, 619 bigrams\n",
            "Added document: 17.txt with 241 unigrams, 127 bigrams\n",
            "Added document: 37.txt with 1990 unigrams, 1077 bigrams\n",
            "Added document: sum7.txt with 4343 unigrams, 2411 bigrams\n",
            "Added document: 39.txt with 1930 unigrams, 1060 bigrams\n",
            "Added document: 92.txt with 4131 unigrams, 2061 bigrams\n",
            "Added document: 140.txt with 804 unigrams, 360 bigrams\n",
            "Added document: 126.txt with 759 unigrams, 475 bigrams\n",
            "Processed 100/213 documents...\n",
            "Added document: 1.txt with 790 unigrams, 446 bigrams\n",
            "Added document: 129.txt with 2362 unigrams, 1483 bigrams\n",
            "Added document: 80.txt with 1418 unigrams, 737 bigrams\n",
            "Added document: 94.txt with 2365 unigrams, 1226 bigrams\n",
            "Added document: 36.txt with 2176 unigrams, 1282 bigrams\n",
            "Added document: 79.txt with 1367 unigrams, 725 bigrams\n",
            "Added document: 57.txt with 799 unigrams, 303 bigrams\n",
            "Added document: sum31.txt with 4251 unigrams, 2656 bigrams\n",
            "Added document: 131.txt with 759 unigrams, 429 bigrams\n",
            "Added document: 24.txt with 185 unigrams, 93 bigrams\n",
            "Added document: 60.txt with 1087 unigrams, 591 bigrams\n",
            "Added document: 10.txt with 219 unigrams, 128 bigrams\n",
            "Added document: 145.txt with 649 unigrams, 267 bigrams\n",
            "Added document: 86.txt with 2355 unigrams, 1213 bigrams\n",
            "Added document: 3.txt with 1193 unigrams, 560 bigrams\n",
            "Added document: 109.txt with 1427 unigrams, 792 bigrams\n",
            "Added document: 35.txt with 3535 unigrams, 1994 bigrams\n",
            "Added document: sum42.txt with 3176 unigrams, 2186 bigrams\n",
            "Added document: sum41.txt with 242 unigrams, 141 bigrams\n",
            "Added document: 53.txt with 58 unigrams, 37 bigrams\n",
            "Processed 120/213 documents...\n",
            "Added document: 103.txt with 1743 unigrams, 1076 bigrams\n",
            "Added document: 88.txt with 2052 unigrams, 1083 bigrams\n",
            "Added document: 68.txt with 1110 unigrams, 547 bigrams\n",
            "Added document: 135.txt with 898 unigrams, 421 bigrams\n",
            "Added document: 40.txt with 2197 unigrams, 1166 bigrams\n",
            "Added document: 46.txt with 2538 unigrams, 1441 bigrams\n",
            "Added document: 43.txt with 1505 unigrams, 860 bigrams\n",
            "Added document: 8.txt with 259 unigrams, 148 bigrams\n",
            "Added document: sum4.txt with 3263 unigrams, 1516 bigrams\n",
            "Added document: 101.txt with 2562 unigrams, 1385 bigrams\n",
            "Added document: 65.txt with 847 unigrams, 444 bigrams\n",
            "Added document: 152.txt with 1908 unigrams, 723 bigrams\n",
            "Added document: 98.txt with 3082 unigrams, 1635 bigrams\n",
            "Added document: 51.txt with 1398 unigrams, 761 bigrams\n",
            "Added document: 78.txt with 1800 unigrams, 886 bigrams\n",
            "Added document: 83.txt with 2130 unigrams, 1066 bigrams\n",
            "Added document: 71.txt with 1849 unigrams, 1095 bigrams\n",
            "Added document: 116.txt with 2207 unigrams, 1231 bigrams\n",
            "Added document: 134.txt with 901 unigrams, 396 bigrams\n",
            "Added document: sum32.txt with 1904 unigrams, 1125 bigrams\n",
            "Processed 140/213 documents...\n",
            "Added document: 133.txt with 961 unigrams, 453 bigrams\n",
            "Added document: 115.txt with 2289 unigrams, 1399 bigrams\n",
            "Added document: 19.txt with 282 unigrams, 129 bigrams\n",
            "Added document: 50.txt with 1661 unigrams, 1013 bigrams\n",
            "Added document: 124.txt with 1411 unigrams, 854 bigrams\n",
            "Added document: 110.txt with 2185 unigrams, 1221 bigrams\n",
            "Added document: sum50.txt with 1226 unigrams, 699 bigrams\n",
            "Added document: 97.txt with 2420 unigrams, 1368 bigrams\n",
            "Added document: sum37.txt with 4124 unigrams, 2202 bigrams\n",
            "Added document: 64.txt with 1280 unigrams, 687 bigrams\n",
            "Added document: 114.txt with 2162 unigrams, 1264 bigrams\n",
            "Added document: 29.txt with 163 unigrams, 101 bigrams\n",
            "Added document: sum49.txt with 2985 unigrams, 1670 bigrams\n",
            "Added document: sum53.txt with 6945 unigrams, 3787 bigrams\n",
            "Added document: 125.txt with 1254 unigrams, 752 bigrams\n",
            "Added document: 42.txt with 2228 unigrams, 1167 bigrams\n",
            "Added document: sum46.txt with 5102 unigrams, 2845 bigrams\n",
            "Added document: 67.txt with 1323 unigrams, 632 bigrams\n",
            "Added document: 150.txt with 2201 unigrams, 948 bigrams\n",
            "Added document: sum57.txt with 4179 unigrams, 2155 bigrams\n",
            "Processed 160/213 documents...\n",
            "Added document: 138.txt with 873 unigrams, 429 bigrams\n",
            "Added document: 130.txt with 897 unigrams, 581 bigrams\n",
            "Added document: 47.txt with 1648 unigrams, 836 bigrams\n",
            "Added document: sum11.txt with 9085 unigrams, 5395 bigrams\n",
            "Added document: 87.txt with 2029 unigrams, 980 bigrams\n",
            "Added document: 61.txt with 964 unigrams, 436 bigrams\n",
            "Added document: 108.txt with 1838 unigrams, 1052 bigrams\n",
            "Added document: sum44.txt with 2118 unigrams, 1316 bigrams\n",
            "Added document: 136.txt with 717 unigrams, 371 bigrams\n",
            "Added document: 2.txt with 1801 unigrams, 797 bigrams\n",
            "Added document: 146.txt with 588 unigrams, 394 bigrams\n",
            "Added document: 100.txt with 1658 unigrams, 825 bigrams\n",
            "Added document: 27.txt with 194 unigrams, 107 bigrams\n",
            "Added document: 25.txt with 177 unigrams, 99 bigrams\n",
            "Added document: 107.txt with 2476 unigrams, 1512 bigrams\n",
            "Added document: 66.txt with 700 unigrams, 377 bigrams\n",
            "Added document: 154.txt with 1978 unigrams, 910 bigrams\n",
            "Added document: 128.txt with 2819 unigrams, 1823 bigrams\n",
            "Added document: 63.txt with 1217 unigrams, 620 bigrams\n",
            "Added document: 95.txt with 2480 unigrams, 1307 bigrams\n",
            "Processed 180/213 documents...\n",
            "Added document: 113.txt with 1380 unigrams, 778 bigrams\n",
            "Added document: 151.txt with 3158 unigrams, 1201 bigrams\n",
            "Added document: 62.txt with 1914 unigrams, 1039 bigrams\n",
            "Added document: 41.txt with 1573 unigrams, 784 bigrams\n",
            "Added document: 143.txt with 582 unigrams, 276 bigrams\n",
            "Added document: 59.txt with 1124 unigrams, 484 bigrams\n",
            "Added document: 34.txt with 189 unigrams, 102 bigrams\n",
            "Added document: 55.txt with 811 unigrams, 363 bigrams\n",
            "Added document: 122.txt with 2043 unigrams, 1153 bigrams\n",
            "Added document: 112.txt with 2824 unigrams, 1519 bigrams\n",
            "Added document: 31.txt with 190 unigrams, 104 bigrams\n",
            "Added document: 119.txt with 481 unigrams, 338 bigrams\n",
            "Added document: 73.txt with 90 unigrams, 57 bigrams\n",
            "Added document: 123.txt with 1132 unigrams, 670 bigrams\n",
            "Added document: 5.txt with 1403 unigrams, 810 bigrams\n",
            "Added document: 102.txt with 2794 unigrams, 1666 bigrams\n",
            "Added document: 111.txt with 2378 unigrams, 1259 bigrams\n",
            "Added document: 75.txt with 1826 unigrams, 880 bigrams\n",
            "Added document: 56.txt with 1313 unigrams, 685 bigrams\n",
            "Added document: 105.txt with 1700 unigrams, 998 bigrams\n",
            "Processed 200/213 documents...\n",
            "Added document: 96.txt with 2350 unigrams, 1296 bigrams\n",
            "Added document: 89.txt with 97 unigrams, 56 bigrams\n",
            "Added document: 15.txt with 141 unigrams, 70 bigrams\n",
            "Added document: 142.txt with 472 unigrams, 217 bigrams\n",
            "Added document: 26.txt with 290 unigrams, 177 bigrams\n",
            "Added document: 121.txt with 981 unigrams, 575 bigrams\n",
            "Added document: 14.txt with 222 unigrams, 111 bigrams\n",
            "Added document: 44.txt with 1888 unigrams, 1006 bigrams\n",
            "Added document: 106.txt with 2024 unigrams, 1129 bigrams\n",
            "Added document: 127.txt with 1510 unigrams, 929 bigrams\n",
            "Added document: 21.txt with 170 unigrams, 96 bigrams\n",
            "Added document: sum10.txt with 4255 unigrams, 2098 bigrams\n",
            "Added document: new.txt with 864 unigrams, 406 bigrams\n",
            "\n",
            "Successfully processed 213 documents\n",
            "Vocabulary mappings created: 2784 terms\n",
            "\n",
            "Sample suffix mappings:\n",
            "  ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ <- ['‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡¶ü‡¶ø', '‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßá‡¶∞', '‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßá', '‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡¶ó‡ßÅ‡¶≤‡ßã']\n",
            "  ‡¶Ü‡¶Æ <- ['‡¶Ü‡¶Æ‡ßá‡¶∞', '‡¶Ü‡¶Æ‡¶∞‡¶æ']\n",
            "  ‡¶®‡¶ø <- ['‡¶®‡¶ø‡¶§‡ßá', '‡¶®‡¶ø‡¶Ø‡¶º‡ßá']\n",
            "  ‡¶™‡ßá‡¶™‡¶æ‡¶∞ <- ['‡¶™‡ßá‡¶™‡¶æ‡¶∞‡ßá', '‡¶™‡ßá‡¶™‡¶æ‡¶∞‡¶ï‡ßá', '‡¶™‡ßá‡¶™‡¶æ‡¶∞‡¶ü‡¶ø‡¶ï‡ßá', '‡¶™‡ßá‡¶™‡¶æ‡¶∞‡ßá‡¶∞', '‡¶™‡ßá‡¶™‡¶æ‡¶∞‡¶ü‡¶ø']\n",
            "  ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ <- ['‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ‡¶ü‡¶ø', '‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ‡¶ó‡ßÅ‡¶≤‡¶ø', '‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ‡¶∞', '‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ‡¶ü‡¶ø‡¶ï‡ßá', '‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ‡¶ó‡ßÅ‡¶≤‡ßã', '‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ‡¶ï‡ßá']\n",
            "  ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶ó‡¶∞‡¶ø <- ['‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶ó‡¶∞‡¶ø‡¶∞', '‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶ó‡¶∞‡¶ø‡¶§‡ßá']\n",
            "  ‡¶™‡¶æ‡¶∞ <- ['‡¶™‡¶æ‡¶∞‡¶¶‡ßá‡¶∞', '‡¶™‡¶æ‡¶∞‡ßá‡¶∞', '‡¶™‡¶æ‡¶∞‡ßá']\n",
            "  ‡¶≤‡¶ø‡¶Æ‡¶ø‡¶ü <- ['‡¶≤‡¶ø‡¶Æ‡¶ø‡¶ü‡ßá', '‡¶≤‡¶ø‡¶Æ‡¶ø‡¶ü‡ßá‡¶∞']\n",
            "  ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ‡¶ó‡ßÅ‡¶≤‡ßã <- ['‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞', '‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ‡¶ó‡ßÅ‡¶≤‡ßã‡¶§‡ßá']\n",
            "\n",
            "============================================================\n",
            "ANALYZING ALL DOCUMENTS - COMBINED UNIGRAM-BIGRAM WITH PMI DECISIONS\n",
            "============================================================\n",
            "Analyzing combined unigram-bigram keywords for all 213 documents...\n",
            "Document length: 4902 tokens, Dynamic keywords: 50\n",
            "Document length: 1861 tokens, Dynamic keywords: 37\n",
            "Document length: 1509 tokens, Dynamic keywords: 30\n",
            "Document length: 2662 tokens, Dynamic keywords: 50\n",
            "Document length: 3819 tokens, Dynamic keywords: 50\n",
            "Document length: 3393 tokens, Dynamic keywords: 50\n",
            "Document length: 2567 tokens, Dynamic keywords: 50\n",
            "Document length: 4000 tokens, Dynamic keywords: 50\n",
            "Document length: 1516 tokens, Dynamic keywords: 30\n",
            "Document length: 15201 tokens, Dynamic keywords: 50\n",
            "Document length: 2488 tokens, Dynamic keywords: 49\n",
            "Document length: 2461 tokens, Dynamic keywords: 49\n",
            "Document length: 1853 tokens, Dynamic keywords: 37\n",
            "Document length: 5086 tokens, Dynamic keywords: 50\n",
            "Document length: 2846 tokens, Dynamic keywords: 50\n",
            "Document length: 2338 tokens, Dynamic keywords: 46\n",
            "Document length: 2175 tokens, Dynamic keywords: 43\n",
            "Document length: 2690 tokens, Dynamic keywords: 50\n",
            "Document length: 6047 tokens, Dynamic keywords: 50\n",
            "Document length: 2666 tokens, Dynamic keywords: 50\n",
            "Document length: 1314 tokens, Dynamic keywords: 26\n",
            "Document length: 6337 tokens, Dynamic keywords: 50\n",
            "Document length: 2083 tokens, Dynamic keywords: 41\n",
            "Document length: 2156 tokens, Dynamic keywords: 43\n",
            "Document length: 1977 tokens, Dynamic keywords: 39\n",
            "Processed 25/213 documents...\n",
            "Document length: 1081 tokens, Dynamic keywords: 21\n",
            "Document length: 4449 tokens, Dynamic keywords: 50\n",
            "Document length: 2426 tokens, Dynamic keywords: 48\n",
            "Document length: 3014 tokens, Dynamic keywords: 50\n",
            "Document length: 5682 tokens, Dynamic keywords: 50\n",
            "Document length: 4628 tokens, Dynamic keywords: 50\n",
            "Document length: 5530 tokens, Dynamic keywords: 50\n",
            "Document length: 2250 tokens, Dynamic keywords: 45\n",
            "Document length: 7585 tokens, Dynamic keywords: 50\n",
            "Document length: 3942 tokens, Dynamic keywords: 50\n",
            "Document length: 4033 tokens, Dynamic keywords: 50\n",
            "Document length: 2668 tokens, Dynamic keywords: 50\n",
            "Document length: 192 tokens, Dynamic keywords: 5\n",
            "Document length: 1829 tokens, Dynamic keywords: 36\n",
            "Document length: 1616 tokens, Dynamic keywords: 32\n",
            "Document length: 1414 tokens, Dynamic keywords: 28\n",
            "Document length: 838 tokens, Dynamic keywords: 16\n",
            "Document length: 176 tokens, Dynamic keywords: 5\n",
            "Document length: 700 tokens, Dynamic keywords: 14\n",
            "Document length: 3554 tokens, Dynamic keywords: 50\n",
            "Document length: 16829 tokens, Dynamic keywords: 50\n",
            "Document length: 2149 tokens, Dynamic keywords: 42\n",
            "Document length: 983 tokens, Dynamic keywords: 19\n",
            "Document length: 1461 tokens, Dynamic keywords: 29\n",
            "Document length: 1867 tokens, Dynamic keywords: 37\n",
            "Processed 50/213 documents...\n",
            "Document length: 1155 tokens, Dynamic keywords: 23\n",
            "Document length: 1628 tokens, Dynamic keywords: 32\n",
            "Document length: 123 tokens, Dynamic keywords: 5\n",
            "Document length: 1006 tokens, Dynamic keywords: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:36:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document length: 2006 tokens, Dynamic keywords: 40\n",
            "Document length: 4042 tokens, Dynamic keywords: 50\n",
            "Document length: 1580 tokens, Dynamic keywords: 31\n",
            "Document length: 127 tokens, Dynamic keywords: 5\n",
            "Document length: 1590 tokens, Dynamic keywords: 31\n",
            "Document length: 1349 tokens, Dynamic keywords: 26\n",
            "Document length: 163 tokens, Dynamic keywords: 5\n",
            "Document length: 2367 tokens, Dynamic keywords: 47\n",
            "Document length: 3105 tokens, Dynamic keywords: 50\n",
            "Document length: 303 tokens, Dynamic keywords: 6\n",
            "Document length: 1968 tokens, Dynamic keywords: 39\n",
            "Document length: 1730 tokens, Dynamic keywords: 34\n",
            "Document length: 180 tokens, Dynamic keywords: 5\n",
            "Document length: 116 tokens, Dynamic keywords: 5\n",
            "Document length: 3304 tokens, Dynamic keywords: 50\n",
            "Document length: 1113 tokens, Dynamic keywords: 22\n",
            "Document length: 1821 tokens, Dynamic keywords: 36\n",
            "Document length: 2243 tokens, Dynamic keywords: 44\n",
            "Document length: 3029 tokens, Dynamic keywords: 50\n",
            "Document length: 1705 tokens, Dynamic keywords: 34\n",
            "Document length: 1081 tokens, Dynamic keywords: 21\n",
            "Processed 75/213 documents...\n",
            "Document length: 2004 tokens, Dynamic keywords: 40\n",
            "Document length: 1072 tokens, Dynamic keywords: 21\n",
            "Document length: 238 tokens, Dynamic keywords: 5\n",
            "Document length: 266 tokens, Dynamic keywords: 5\n",
            "Document length: 169 tokens, Dynamic keywords: 5\n",
            "Document length: 670 tokens, Dynamic keywords: 13\n",
            "Document length: 1286 tokens, Dynamic keywords: 25\n",
            "Document length: 232 tokens, Dynamic keywords: 5\n",
            "Document length: 3371 tokens, Dynamic keywords: 50\n",
            "Document length: 180 tokens, Dynamic keywords: 5\n",
            "Document length: 407 tokens, Dynamic keywords: 8\n",
            "Document length: 1413 tokens, Dynamic keywords: 28\n",
            "Document length: 3691 tokens, Dynamic keywords: 50\n",
            "Document length: 932 tokens, Dynamic keywords: 18\n",
            "Document length: 978 tokens, Dynamic keywords: 19\n",
            "Document length: 1547 tokens, Dynamic keywords: 30\n",
            "Document length: 1487 tokens, Dynamic keywords: 29\n",
            "Document length: 1159 tokens, Dynamic keywords: 23\n",
            "Document length: 241 tokens, Dynamic keywords: 5\n",
            "Document length: 1990 tokens, Dynamic keywords: 39\n",
            "Document length: 4343 tokens, Dynamic keywords: 50\n",
            "Document length: 1930 tokens, Dynamic keywords: 38\n",
            "Document length: 4131 tokens, Dynamic keywords: 50\n",
            "Document length: 804 tokens, Dynamic keywords: 16\n",
            "Document length: 759 tokens, Dynamic keywords: 15\n",
            "Processed 100/213 documents...\n",
            "Document length: 790 tokens, Dynamic keywords: 15\n",
            "Document length: 2362 tokens, Dynamic keywords: 47\n",
            "Document length: 1418 tokens, Dynamic keywords: 28\n",
            "Document length: 2365 tokens, Dynamic keywords: 47\n",
            "Document length: 2176 tokens, Dynamic keywords: 43\n",
            "Document length: 1367 tokens, Dynamic keywords: 27\n",
            "Document length: 799 tokens, Dynamic keywords: 15\n",
            "Document length: 4251 tokens, Dynamic keywords: 50\n",
            "Document length: 759 tokens, Dynamic keywords: 15\n",
            "Document length: 185 tokens, Dynamic keywords: 5\n",
            "Document length: 1087 tokens, Dynamic keywords: 21\n",
            "Document length: 219 tokens, Dynamic keywords: 5\n",
            "Document length: 649 tokens, Dynamic keywords: 12\n",
            "Document length: 2355 tokens, Dynamic keywords: 47\n",
            "Document length: 1193 tokens, Dynamic keywords: 23\n",
            "Document length: 1427 tokens, Dynamic keywords: 28\n",
            "Document length: 3535 tokens, Dynamic keywords: 50\n",
            "Document length: 3176 tokens, Dynamic keywords: 50\n",
            "Document length: 242 tokens, Dynamic keywords: 5\n",
            "Document length: 58 tokens, Dynamic keywords: 5\n",
            "Document length: 1743 tokens, Dynamic keywords: 34\n",
            "Document length: 2052 tokens, Dynamic keywords: 41\n",
            "Document length: 1110 tokens, Dynamic keywords: 22\n",
            "Document length: 898 tokens, Dynamic keywords: 17\n",
            "Document length: 2197 tokens, Dynamic keywords: 43\n",
            "Processed 125/213 documents...\n",
            "Document length: 2538 tokens, Dynamic keywords: 50\n",
            "Document length: 1505 tokens, Dynamic keywords: 30\n",
            "Document length: 259 tokens, Dynamic keywords: 5\n",
            "Document length: 3263 tokens, Dynamic keywords: 50\n",
            "Document length: 2562 tokens, Dynamic keywords: 50\n",
            "Document length: 847 tokens, Dynamic keywords: 16\n",
            "Document length: 1908 tokens, Dynamic keywords: 38\n",
            "Document length: 3082 tokens, Dynamic keywords: 50\n",
            "Document length: 1398 tokens, Dynamic keywords: 27\n",
            "Document length: 1800 tokens, Dynamic keywords: 36\n",
            "Document length: 2130 tokens, Dynamic keywords: 42\n",
            "Document length: 1849 tokens, Dynamic keywords: 36\n",
            "Document length: 2207 tokens, Dynamic keywords: 44\n",
            "Document length: 901 tokens, Dynamic keywords: 18\n",
            "Document length: 1904 tokens, Dynamic keywords: 38\n",
            "Document length: 961 tokens, Dynamic keywords: 19\n",
            "Document length: 2289 tokens, Dynamic keywords: 45\n",
            "Document length: 282 tokens, Dynamic keywords: 5\n",
            "Document length: 1661 tokens, Dynamic keywords: 33\n",
            "Document length: 1411 tokens, Dynamic keywords: 28\n",
            "Document length: 2185 tokens, Dynamic keywords: 43\n",
            "Document length: 1226 tokens, Dynamic keywords: 24\n",
            "Document length: 2420 tokens, Dynamic keywords: 48\n",
            "Document length: 4124 tokens, Dynamic keywords: 50\n",
            "Document length: 1280 tokens, Dynamic keywords: 25\n",
            "Processed 150/213 documents...\n",
            "Document length: 2162 tokens, Dynamic keywords: 43\n",
            "Document length: 163 tokens, Dynamic keywords: 5\n",
            "Document length: 2985 tokens, Dynamic keywords: 50\n",
            "Document length: 6945 tokens, Dynamic keywords: 50\n",
            "Document length: 1254 tokens, Dynamic keywords: 25\n",
            "Document length: 2228 tokens, Dynamic keywords: 44\n",
            "Document length: 5102 tokens, Dynamic keywords: 50\n",
            "Document length: 1323 tokens, Dynamic keywords: 26\n",
            "Document length: 2201 tokens, Dynamic keywords: 44\n",
            "Document length: 4179 tokens, Dynamic keywords: 50\n",
            "Document length: 873 tokens, Dynamic keywords: 17\n",
            "Document length: 897 tokens, Dynamic keywords: 17\n",
            "Document length: 1648 tokens, Dynamic keywords: 32\n",
            "Document length: 9085 tokens, Dynamic keywords: 50\n",
            "Document length: 2029 tokens, Dynamic keywords: 40\n",
            "Document length: 964 tokens, Dynamic keywords: 19\n",
            "Document length: 1838 tokens, Dynamic keywords: 36\n",
            "Document length: 2118 tokens, Dynamic keywords: 42\n",
            "Document length: 717 tokens, Dynamic keywords: 14\n",
            "Document length: 1801 tokens, Dynamic keywords: 36\n",
            "Document length: 588 tokens, Dynamic keywords: 11\n",
            "Document length: 1658 tokens, Dynamic keywords: 33\n",
            "Document length: 194 tokens, Dynamic keywords: 5\n",
            "Document length: 177 tokens, Dynamic keywords: 5\n",
            "Document length: 2476 tokens, Dynamic keywords: 49\n",
            "Processed 175/213 documents...\n",
            "Document length: 700 tokens, Dynamic keywords: 14\n",
            "Document length: 1978 tokens, Dynamic keywords: 39\n",
            "Document length: 2819 tokens, Dynamic keywords: 50\n",
            "Document length: 1217 tokens, Dynamic keywords: 24\n",
            "Document length: 2480 tokens, Dynamic keywords: 49\n",
            "Document length: 1380 tokens, Dynamic keywords: 27\n",
            "Document length: 3158 tokens, Dynamic keywords: 50\n",
            "Document length: 1914 tokens, Dynamic keywords: 38\n",
            "Document length: 1573 tokens, Dynamic keywords: 31\n",
            "Document length: 582 tokens, Dynamic keywords: 11\n",
            "Document length: 1124 tokens, Dynamic keywords: 22\n",
            "Document length: 189 tokens, Dynamic keywords: 5\n",
            "Document length: 811 tokens, Dynamic keywords: 16\n",
            "Document length: 2043 tokens, Dynamic keywords: 40\n",
            "Document length: 2824 tokens, Dynamic keywords: 50\n",
            "Document length: 190 tokens, Dynamic keywords: 5\n",
            "Document length: 481 tokens, Dynamic keywords: 9\n",
            "Document length: 90 tokens, Dynamic keywords: 5\n",
            "Document length: 1132 tokens, Dynamic keywords: 22\n",
            "Document length: 1403 tokens, Dynamic keywords: 28\n",
            "Document length: 2794 tokens, Dynamic keywords: 50\n",
            "Document length: 2378 tokens, Dynamic keywords: 47\n",
            "Document length: 1826 tokens, Dynamic keywords: 36\n",
            "Document length: 1313 tokens, Dynamic keywords: 26\n",
            "Document length: 1700 tokens, Dynamic keywords: 34\n",
            "Processed 200/213 documents...\n",
            "Document length: 2350 tokens, Dynamic keywords: 47\n",
            "Document length: 97 tokens, Dynamic keywords: 5\n",
            "Document length: 141 tokens, Dynamic keywords: 5\n",
            "Document length: 472 tokens, Dynamic keywords: 9\n",
            "Document length: 290 tokens, Dynamic keywords: 5\n",
            "Document length: 981 tokens, Dynamic keywords: 19\n",
            "Document length: 222 tokens, Dynamic keywords: 5\n",
            "Document length: 1888 tokens, Dynamic keywords: 37\n",
            "Document length: 2024 tokens, Dynamic keywords: 40\n",
            "Document length: 1510 tokens, Dynamic keywords: 30\n",
            "Document length: 170 tokens, Dynamic keywords: 5\n",
            "Document length: 4255 tokens, Dynamic keywords: 50\n",
            "Document length: 864 tokens, Dynamic keywords: 17\n",
            "‚úì Combined unigram-bigram analysis complete!\n",
            "\n",
            "Decision Statistics:\n",
            "  - Keep bigram (eliminate unigrams): 14885\n",
            "  - Keep unigrams (eliminate bigram): 125314\n",
            "  - Keep both: 585\n",
            "Results saved to: /content/drive/MyDrive/tfidf/all_document_combined_keywords.xlsx\n",
            "\n",
            "============================================================\n",
            "OVERALL STATISTICS\n",
            "============================================================\n",
            "üìä Total documents processed: 213\n",
            "üìö Total unigram vocabulary size: 14480\n",
            "üìö Total bigram vocabulary size: 89612\n",
            "üîó Terms with suffix mappings: 2784\n",
            "üìù Average unigrams per document: 2039.2\n",
            "üìù Average bigrams per document: 1107.9\n",
            "üìà Total unigrams processed: 434350\n",
            "üìà Total bigrams processed: 235983\n",
            "\n",
            "üîç DECISION ANALYSIS SAMPLE (First Document):\n",
            "--------------------------------------------------\n",
            "Document length: 4902 tokens, Dynamic keywords: 50\n",
            "Document: sum2.txt\n",
            "Eliminated unigrams: 116\n",
            "Total keywords: 574\n",
            "\n",
            "Top 10 PMI Decisions:\n",
            "   1. ‡¶ì‡¶Ø‡¶º‡¶æ‡¶≤‡¶æ‡¶á‡¶ï‡ßÅ‡¶Æ ‡¶Ü‡¶∏‡¶∏‡¶æ‡¶≤‡¶æ‡¶Æ        ‚Üí keep_unigrams   (PMI:   8.50) - Bigram too rare (freq=1)\n",
            "   2. ‡¶¨‡¶ø‡¶∑‡ßü‡¶ï ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡¶ì             ‚Üí keep_unigrams   (PMI:   8.50) - Bigram too rare (freq=1)\n",
            "   3. ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡¶ì ‡¶•‡¶æ‡¶ï‡¶§‡ßá             ‚Üí keep_unigrams   (PMI:   8.50) - Bigram too rare (freq=1)\n",
            "   4. ‡¶Ö‡¶§‡ßç‡¶Ø‡¶®‡ßç‡¶§ ‡¶ï‡¶æ‡¶∞‡ßç‡¶Ø‡¶ï‡¶∞‡ßÄ          ‚Üí keep_unigrams   (PMI:   8.50) - Bigram too rare (freq=1)\n",
            "   5. ‡¶∏‡¶´‡¶≤ ‡¶π‡¶≤‡¶æ‡¶Æ                  ‚Üí keep_unigrams   (PMI:   8.50) - Bigram too rare (freq=1)\n",
            "   6. ‡¶¢‡¶æ‡¶≤‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶∏‡¶Ç‡¶Ø‡ßã‡¶ó            ‚Üí keep_unigrams   (PMI:   8.50) - Bigram too rare (freq=1)\n",
            "   7. ‡¶∏‡¶Ç‡¶Ø‡ßã‡¶ó ‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü‡¶≠‡¶æ‡¶¨‡ßá          ‚Üí keep_unigrams   (PMI:   8.50) - Bigram too rare (freq=1)\n",
            "   8. ‡¶ï‡¶•‡¶æ ‡¶¨‡¶≤‡¶ø                   ‚Üí keep_unigrams   (PMI:   8.50) - Bigram too rare (freq=1)\n",
            "   9. ‡¶ß‡¶®‡¶æ‡¶§‡ßç‡¶Æ‡¶ï ‡¶¶‡¶ø‡¶ï               ‚Üí keep_unigrams   (PMI:   8.50) - Bigram too rare (freq=1)\n",
            "  10. ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶∏‡¶ø‡¶Æ‡¶æ‡¶Æ ‡¶¨‡¶≤            ‚Üí keep_unigrams   (PMI:   8.50) - Bigram too rare (freq=1)\n",
            "\n",
            "üîÑ TOP SUFFIX MERGING EXAMPLES:\n",
            "----------------------------------------\n",
            "  ‡¶á‡¶≤‡ßá‡¶ï‡¶ü‡ßç‡¶∞‡¶® ‚Üê ['‡¶á‡¶≤‡ßá‡¶ï‡¶ü‡ßç‡¶∞‡¶®‡¶ü‡¶æ', '‡¶á‡¶≤‡ßá‡¶ï‡¶ü‡ßç‡¶∞‡¶®‡¶ó‡ßÅ‡¶≤‡¶ø‡¶ï‡ßá', '‡¶á‡¶≤‡ßá‡¶ï‡¶ü‡ßç‡¶∞‡¶®‡¶¶‡ßá‡¶∞', '‡¶á‡¶≤‡ßá‡¶ï‡¶ü‡ßç‡¶∞‡¶®‡¶ï‡ßá', '‡¶á‡¶≤‡ßá‡¶ï‡¶ü‡ßç‡¶∞‡¶®‡¶ü‡¶ø', '‡¶á‡¶≤‡ßá‡¶ï‡¶ü‡ßç‡¶∞‡¶®‡¶ó‡ßÅ‡¶≤‡¶ø', '‡¶á‡¶≤‡ßá‡¶ï‡¶ü‡ßç‡¶∞‡¶®‡ßá', '‡¶á‡¶≤‡ßá‡¶ï‡¶ü‡ßç‡¶∞‡¶®‡¶ó‡ßÅ‡¶≤‡ßã', '‡¶á‡¶≤‡ßá‡¶ï‡¶ü‡ßç‡¶∞‡¶®‡¶ü‡¶ø‡¶ï‡ßá', '‡¶á‡¶≤‡ßá‡¶ï‡¶ü‡ßç‡¶∞‡¶®‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá', '‡¶á‡¶≤‡ßá‡¶ï‡¶ü‡ßç‡¶∞‡¶®‡¶∞‡¶æ', '‡¶á‡¶≤‡ßá‡¶ï‡¶ü‡ßç‡¶∞‡¶®‡ßá‡¶∞']\n",
            "  ‡¶¨‡¶≤ ‚Üê ['‡¶¨‡¶≤‡¶ï‡ßá', '‡¶¨‡¶≤‡ßá‡¶∞', '‡¶¨‡¶≤‡ßá', '‡¶¨‡¶≤‡¶§‡ßá', '‡¶¨‡¶≤‡¶ó‡ßÅ‡¶≤‡¶ø', '‡¶¨‡¶≤‡¶ó‡ßÅ‡¶≤‡ßã', '‡¶¨‡¶≤‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá', '‡¶¨‡¶≤‡¶ü‡¶ø', '‡¶¨‡¶≤‡¶Ø‡¶º‡ßá', '‡¶¨‡¶≤‡¶ü‡¶ø‡¶ï‡ßá']\n",
            "  ‡¶¨‡¶æ‡¶π‡ßÅ ‚Üê ['‡¶¨‡¶æ‡¶π‡ßÅ‡¶ü‡¶ø', '‡¶¨‡¶æ‡¶π‡ßÅ‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá', '‡¶¨‡¶æ‡¶π‡ßÅ‡¶ó‡ßÅ‡¶≤‡¶ø', '‡¶¨‡¶æ‡¶π‡ßÅ‡¶ï‡ßá', '‡¶¨‡¶æ‡¶π‡ßÅ‡¶§‡ßá', '‡¶¨‡¶æ‡¶π‡ßÅ‡¶ó‡ßÅ‡¶≤‡¶ø‡¶ï‡ßá', '‡¶¨‡¶æ‡¶π‡ßÅ‡¶ü‡¶ø‡¶ï‡ßá', '‡¶¨‡¶æ‡¶π‡ßÅ‡¶¶‡ßá‡¶∞', '‡¶¨‡¶æ‡¶π‡ßÅ‡¶ó‡ßÅ‡¶≤‡ßã', '‡¶¨‡¶æ‡¶π‡ßÅ‡¶∞']\n",
            "  ‡¶Ü‡¶Ø‡¶º‡¶® ‚Üê ['‡¶Ü‡¶Ø‡¶º‡¶®‡¶ó‡ßÅ‡¶≤‡¶ø‡¶ï‡ßá', '‡¶Ü‡¶Ø‡¶º‡¶®‡¶ó‡ßÅ‡¶≤‡¶ø', '‡¶Ü‡¶Ø‡¶º‡¶®‡ßá', '‡¶Ü‡¶Ø‡¶º‡¶®‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá', '‡¶Ü‡¶Ø‡¶º‡¶®‡¶ü‡¶ø', '‡¶Ü‡¶Ø‡¶º‡¶®‡¶Æ‡¶æ‡¶®', '‡¶Ü‡¶Ø‡¶º‡¶®‡¶¶‡ßá‡¶∞', '‡¶Ü‡¶Ø‡¶º‡¶®‡¶ï‡ßá', '‡¶Ü‡¶Ø‡¶º‡¶®‡ßá‡¶∞', '‡¶Ü‡¶Ø‡¶º‡¶®‡¶ó‡ßÅ‡¶≤‡ßã']\n",
            "  ‡¶∏‡¶Æ‡ßÄ‡¶ï‡¶∞‡¶£ ‚Üê ['‡¶∏‡¶Æ‡ßÄ‡¶ï‡¶∞‡¶£‡¶ï‡ßá', '‡¶∏‡¶Æ‡ßÄ‡¶ï‡¶∞‡¶£‡¶ü‡¶ø', '‡¶∏‡¶Æ‡ßÄ‡¶ï‡¶∞‡¶£‡ßá', '‡¶∏‡¶Æ‡ßÄ‡¶ï‡¶∞‡¶£‡¶ó‡ßÅ‡¶≤‡ßã', '‡¶∏‡¶Æ‡ßÄ‡¶ï‡¶∞‡¶£‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá', '‡¶∏‡¶Æ‡ßÄ‡¶ï‡¶∞‡¶£‡¶ó‡ßÅ‡¶≤‡¶ø', '‡¶∏‡¶Æ‡ßÄ‡¶ï‡¶∞‡¶£‡¶ó‡ßÅ‡¶≤‡¶ø‡¶ï‡ßá', '‡¶∏‡¶Æ‡ßÄ‡¶ï‡¶∞‡¶£‡ßá‡¶∞', '‡¶∏‡¶Æ‡ßÄ‡¶ï‡¶∞‡¶£‡¶ü‡¶ø‡¶ï‡ßá']\n",
            "  ‡¶∏‡ßÇ‡¶§‡ßç‡¶∞ ‚Üê ['‡¶∏‡ßÇ‡¶§‡ßç‡¶∞‡¶ü‡¶æ', '‡¶∏‡ßÇ‡¶§‡ßç‡¶∞‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá', '‡¶∏‡ßÇ‡¶§‡ßç‡¶∞‡¶ü‡¶ø‡¶ï‡ßá', '‡¶∏‡ßÇ‡¶§‡ßç‡¶∞‡ßá‡¶∞', '‡¶∏‡ßÇ‡¶§‡ßç‡¶∞‡¶ó‡ßÅ‡¶≤‡ßã', '‡¶∏‡ßÇ‡¶§‡ßç‡¶∞‡¶ï‡ßá', '‡¶∏‡ßÇ‡¶§‡ßç‡¶∞‡¶ó‡ßÅ‡¶≤‡¶ø', '‡¶∏‡ßÇ‡¶§‡ßç‡¶∞‡¶ü‡¶ø', '‡¶∏‡ßÇ‡¶§‡ßç‡¶∞‡ßá']\n",
            "  ‡¶´‡¶∞‡ßç‡¶Æ‡ßÅ‡¶≤‡¶æ ‚Üê ['‡¶´‡¶∞‡ßç‡¶Æ‡ßÅ‡¶≤‡¶æ‡¶ü‡¶ø', '‡¶´‡¶∞‡ßç‡¶Æ‡ßÅ‡¶≤‡¶æ‡¶ü‡¶æ', '‡¶´‡¶∞‡ßç‡¶Æ‡ßÅ‡¶≤‡¶æ‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá', '‡¶´‡¶∞‡ßç‡¶Æ‡ßÅ‡¶≤‡¶æ‡¶ó‡ßÅ‡¶≤‡¶ø', '‡¶´‡¶∞‡ßç‡¶Æ‡ßÅ‡¶≤‡¶æ‡¶ü‡¶ø‡¶ï‡ßá', '‡¶´‡¶∞‡ßç‡¶Æ‡ßÅ‡¶≤‡¶æ‡¶∞', '‡¶´‡¶∞‡ßç‡¶Æ‡ßÅ‡¶≤‡¶æ‡¶ï‡ßá', '‡¶´‡¶∞‡ßç‡¶Æ‡ßÅ‡¶≤‡¶æ‡¶ó‡ßÅ‡¶≤‡ßã', '‡¶´‡¶∞‡ßç‡¶Æ‡ßÅ‡¶≤‡¶æ‡¶§‡ßá']\n",
            "  ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‚Üê ['‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ï‡ßá', '‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶¶‡ßá‡¶∞', '‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ó‡ßÅ‡¶≤‡¶ø', '‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ü‡¶ø‡¶ï‡ßá', '‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá', '‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá', '‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞', '‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ó‡ßÅ‡¶≤‡ßã', '‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ü‡¶ø']\n",
            "  ‡¶â‡¶™‡¶æ‡¶¶‡¶æ‡¶® ‚Üê ['‡¶â‡¶™‡¶æ‡¶¶‡¶æ‡¶®‡¶ó‡ßÅ‡¶≤‡ßã', '‡¶â‡¶™‡¶æ‡¶¶‡¶æ‡¶®‡ßá‡¶∞', '‡¶â‡¶™‡¶æ‡¶¶‡¶æ‡¶®‡¶¶‡ßá‡¶∞', '‡¶â‡¶™‡¶æ‡¶¶‡¶æ‡¶®‡ßá', '‡¶â‡¶™‡¶æ‡¶¶‡¶æ‡¶®‡¶ó‡ßÅ‡¶≤‡¶ø', '‡¶â‡¶™‡¶æ‡¶¶‡¶æ‡¶®‡¶ó‡ßÅ‡¶≤‡¶ø‡¶ï‡ßá', '‡¶â‡¶™‡¶æ‡¶¶‡¶æ‡¶®‡¶ü‡¶ø', '‡¶â‡¶™‡¶æ‡¶¶‡¶æ‡¶®‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá', '‡¶â‡¶™‡¶æ‡¶¶‡¶æ‡¶®‡¶ï‡ßá']\n",
            "  ‡¶Æ‡ßå‡¶≤ ‚Üê ['‡¶Æ‡ßå‡¶≤‡¶ï‡ßá', '‡¶Æ‡ßå‡¶≤‡¶ü‡¶ø‡¶ï‡ßá', '‡¶Æ‡ßå‡¶≤‡¶ó‡ßÅ‡¶≤‡¶ø', '‡¶Æ‡ßå‡¶≤‡ßá', '‡¶Æ‡ßå‡¶≤‡¶¶‡ßá‡¶∞', '‡¶Æ‡ßå‡¶≤‡¶ü‡¶ø', '‡¶Æ‡ßå‡¶≤‡¶ó‡ßÅ‡¶≤‡ßã', '‡¶Æ‡ßå‡¶≤‡ßá‡¶∞', '‡¶Æ‡ßå‡¶≤‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá']\n",
            "  ‡¶Ö‡¶®‡ßÅ ‚Üê ['‡¶Ö‡¶®‡ßÅ‡¶Æ‡¶æ‡¶®', '‡¶Ö‡¶®‡ßÅ‡¶ï‡ßá', '‡¶Ö‡¶®‡ßÅ‡¶∞‡¶æ', '‡¶Ö‡¶®‡ßÅ‡¶§‡ßá', '‡¶Ö‡¶®‡ßÅ‡¶ó‡ßÅ‡¶≤‡¶ø', '‡¶Ö‡¶®‡ßÅ‡¶∞', '‡¶Ö‡¶®‡ßÅ‡¶¶‡ßá‡¶∞', '‡¶Ö‡¶®‡ßÅ‡¶ó‡ßÅ‡¶≤‡ßã', '‡¶Ö‡¶®‡ßÅ‡¶ó‡ßÅ‡¶≤‡¶ø‡¶ï‡ßá']\n",
            "  ‡¶Ø‡ßå‡¶ó ‚Üê ['‡¶Ø‡ßå‡¶ó‡¶ó‡ßÅ‡¶≤‡¶ø', '‡¶Ø‡ßå‡¶ó‡¶ü‡¶ø‡¶ï‡ßá', '‡¶Ø‡ßå‡¶ó‡¶ï‡ßá', '‡¶Ø‡ßå‡¶ó‡¶ó‡ßÅ‡¶≤‡¶ø‡¶ï‡ßá', '‡¶Ø‡ßå‡¶ó‡¶ó‡ßÅ‡¶≤‡ßã', '‡¶Ø‡ßå‡¶ó‡ßá', '‡¶Ø‡ßå‡¶ó‡¶ü‡¶ø', '‡¶Ø‡ßå‡¶ó‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá', '‡¶Ø‡ßå‡¶ó‡ßá‡¶∞']\n",
            "  ‡¶¨‡¶ø‡¶ü ‚Üê ['‡¶¨‡¶ø‡¶ü‡¶ó‡ßÅ‡¶≤‡¶ø‡¶ï‡ßá', '‡¶¨‡¶ø‡¶ü‡¶ü‡¶ø‡¶ï‡ßá', '‡¶¨‡¶ø‡¶ü‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá', '‡¶¨‡¶ø‡¶ü‡ßá‡¶∞', '‡¶¨‡¶ø‡¶ü‡¶ó‡ßÅ‡¶≤‡¶ø', '‡¶¨‡¶ø‡¶ü‡¶ü‡¶ø', '‡¶¨‡¶ø‡¶ü‡¶ï‡ßá', '‡¶¨‡¶ø‡¶ü‡¶ó‡ßÅ‡¶≤‡ßã', '‡¶¨‡¶ø‡¶ü‡ßá']\n",
            "  ‡¶∞‡¶æ‡¶∂‡¶ø ‚Üê ['‡¶∞‡¶æ‡¶∂‡¶ø‡¶∞', '‡¶∞‡¶æ‡¶∂‡¶ø‡¶ü‡¶æ', '‡¶∞‡¶æ‡¶∂‡¶ø‡¶ü‡¶ø', '‡¶∞‡¶æ‡¶∂‡¶ø‡¶§‡ßá', '‡¶∞‡¶æ‡¶∂‡¶ø‡¶ï‡ßá', '‡¶∞‡¶æ‡¶∂‡¶ø‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá', '‡¶∞‡¶æ‡¶∂‡¶ø‡¶ó‡ßÅ‡¶≤‡ßã', '‡¶∞‡¶æ‡¶∂‡¶ø‡¶ü‡¶ø‡¶ï‡ßá']\n",
            "  ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® ‚Üê ['‡¶´‡¶æ‡¶Ç‡¶∂‡¶®‡¶ó‡ßÅ‡¶≤‡ßã', '‡¶´‡¶æ‡¶Ç‡¶∂‡¶®‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá', '‡¶´‡¶æ‡¶Ç‡¶∂‡¶®‡ßá‡¶∞', '‡¶´‡¶æ‡¶Ç‡¶∂‡¶®‡ßá', '‡¶´‡¶æ‡¶Ç‡¶∂‡¶®‡¶ï‡ßá', '‡¶´‡¶æ‡¶Ç‡¶∂‡¶®‡¶ü‡¶ø‡¶ï‡ßá', '‡¶´‡¶æ‡¶Ç‡¶∂‡¶®‡¶ü‡¶ø', '‡¶´‡¶æ‡¶Ç‡¶∂‡¶®‡¶ó‡ßÅ‡¶≤‡¶ø']\n",
            "\n",
            "============================================================\n",
            "üéâ ENHANCED COMBINED UNIGRAM-BIGRAM ANALYSIS COMPLETE!\n",
            "üìÅ Check the Excel file for detailed results with PMI-based decisions\n",
            "============================================================\n",
            "Extracted Keywords:\n",
            "‡¶è‡¶ï‡¶ï ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞\n",
            "‡¶∏‡ßç‡¶•‡¶æ‡¶®‡¶æ‡¶ô‡ßç‡¶ï\n",
            "‡¶ï‡ßç‡¶Ø‡¶æ‡¶™\n",
            "‡¶∏‡¶∞‡¶≤‡¶∞‡ßá‡¶ñ‡¶æ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞\n",
            "‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶® ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞\n",
            "‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶∏‡¶Æ‡¶æ‡¶®‡ßç‡¶§‡¶∞‡¶æ‡¶≤\n",
            "‡¶§‡ßÉ‡¶§‡ßÄ‡¶Ø‡¶º ‡¶¨‡¶æ‡¶π‡ßÅ\n",
            "‡¶ï‡ßç‡¶∞‡¶∏\n",
            "‡¶â‡¶™‡¶∞‡¶∏‡ßç‡¶•\n",
            "‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ\n",
            "‡¶§‡ßç‡¶∞‡¶ø‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶ø‡¶ï ‡¶∏‡ßç‡¶•‡¶æ‡¶®‡¶æ‡¶Ç‡¶ï\n",
            "‡¶Ü‡¶ï‡ßÉ‡¶§‡¶ø ‡¶ò‡¶®‡¶¨‡¶∏‡ßç‡¶§‡ßÅ\n",
            "‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶®‡¶ø‡¶∞‡ßç‡¶£‡¶Ø‡¶º\n",
            "‡¶ï‡¶ú‡¶æ‡¶≤\n",
            "‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ó‡ßÅ‡¶≤‡ßã\n",
            "‡¶∏‡¶Æ‡¶æ‡¶®‡ßç‡¶§‡¶∞‡¶æ‡¶≤ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞\n",
            "‡¶∏‡¶Æ‡¶æ‡¶®‡ßç‡¶§‡¶∞‡¶æ‡¶≤ ‡¶è‡¶ï‡¶ï\n",
            "üíæ Combined keywords saved to: /content/drive/MyDrive/tfidf/new_combined_keywords.xlsx\n",
            "Row 0: '[00:00 - 04:30]\\n‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶∞‡¶æ‡¶∂‡¶ø‡¶∞ ‡¶¨‡¶ø‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶™‡ßç‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶Ü‡¶≤‡ßã‡¶ö‡¶®‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá, ‡¶¶‡ßÅ‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶∏‡¶Æ‡¶æ‡¶® ‡¶π‡¶§‡ßá ‡¶π‡¶¨‡ßá, ‡¶è‡¶¨‡¶Ç ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶¶‡¶ø‡¶ï ‡¶¨‡¶ø‡¶™‡¶∞‡ßÄ‡¶§ ‡¶π‡¶§‡ßá ‡¶π‡¶¨‡ßá‡•§ ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£‡¶∏‡ßç‡¶¨‡¶∞‡ßÇ‡¶™, ‡¶è ‡¶¨‡¶ø ‡¶è‡¶¨‡¶Ç ‡¶∏‡¶ø ‡¶°‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞, ‡¶Ø‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶∏‡¶Æ‡¶æ‡¶® ‡¶è‡¶¨‡¶Ç ‡¶¶‡¶ø‡¶ï ‡¶¨‡¶ø‡¶™‡¶∞‡ßÄ‡¶§‡•§ ‡¶è‡¶á ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶∏‡¶Æ‡¶æ‡¶® ‡¶π‡¶ì‡¶Ø‡¶º‡¶æ ‡¶â‡¶ö‡¶ø‡¶§, ‡¶ï‡¶æ‡¶∞‡¶£ ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶ì ‡¶¶‡¶ø‡¶ï ‡¶∏‡¶Æ‡¶æ‡¶®‡•§ ‡¶è‡¶∞‡¶™‡¶∞, ‡¶è‡¶¨‡¶ø ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶ø‡¶è‡¶´ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Ü‡¶¶‡¶ø ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ ‡¶è ‡¶•‡ßá‡¶ï‡ßá ‡¶∂‡ßá‡¶∑ ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ ‡¶¨‡¶ø ‡¶•‡ßá‡¶ï‡ßá ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶π‡¶Ø‡¶º, ‡¶è‡¶¨‡¶Ç ‡¶§‡¶æ‡¶∞‡¶æ ‡¶è‡¶ï‡ßá ‡¶Ö‡¶™‡¶∞‡ßá‡¶∞ ‡¶¨‡¶ø‡¶™‡¶∞‡ßÄ‡¶§ ‡¶¶‡¶ø‡¶ï‡ßá ‡¶•‡¶æ‡¶ï‡ßá, ‡¶Ø‡¶æ ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶¶‡¶ø‡¶ï‡¶ï‡ßá ‡¶¨‡¶ø‡¶™‡¶∞‡ßÄ‡¶§ ‡¶ï‡¶∞‡ßá ‡¶¶‡ßá‡¶Ø‡¶º‡•§\\n\\n'\n",
            "Row 1: '[04:24 - 08:00]\\n‡¶è‡¶á ‡¶Ö‡¶Ç‡¶∂‡ßá ‡¶∏‡¶Æ‡¶ú‡¶æ‡¶§‡ßÄ‡¶Ø‡¶º ‡¶Æ‡¶æ‡¶® ‡¶¨‡¶æ ‡¶∏‡¶Æ‡¶æ‡¶®‡ßç‡¶§‡¶∞‡¶æ‡¶≤ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶ß‡¶æ‡¶∞‡¶£‡¶æ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶∏‡¶Æ‡¶ú‡¶æ‡¶§‡ßÄ‡¶Ø‡¶º ‡¶Æ‡¶æ‡¶®‡ßá‡¶∞ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ó‡ßÅ‡¶≤‡ßã ‡¶∏‡¶Æ‡¶æ‡¶®‡ßç‡¶§‡¶∞‡¶æ‡¶≤ ‡¶∞‡ßá‡¶ñ‡¶æ ‡¶¨‡¶∞‡¶æ‡¶¨‡¶∞ ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá, ‡¶§‡¶¨‡ßá ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶•‡¶æ‡¶ï‡ßá‡•§ ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£‡¶∏‡ßç‡¶¨‡¶∞‡ßÇ‡¶™, ‡¶Ø‡¶¶‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ‡¶∞ ‡¶¶‡¶ø‡¶ï ‡¶è‡¶ï‡¶á ‡¶¶‡¶ø‡¶ï‡ßá ‡¶•‡¶æ‡¶ï‡ßá, ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶∏‡ßá‡¶ü‡¶ø ‡¶∏‡¶¶‡ßÉ‡¶∂ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶π‡¶¨‡ßá, ‡¶è‡¶¨‡¶Ç ‡¶Ø‡¶¶‡¶ø ‡¶¶‡¶ø‡¶ï ‡¶â‡¶≤‡ßç‡¶ü‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶§‡¶¨‡ßá ‡¶è‡¶ü‡¶ø ‡¶Ö‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡•§ ‡¶è‡¶á ‡¶ß‡¶∞‡¶®‡ßá‡¶∞ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ï‡ßá ‡¶è‡¶ï‡¶ï ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶¨‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶ï‡¶æ‡¶∞‡¶£ ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶•‡¶æ‡¶ï‡ßá ‡¶®‡¶æ‡•§ ‡¶è‡¶õ‡¶æ‡¶°‡¶º‡¶æ, ‡¶§‡ßç‡¶∞‡¶ø‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶ø‡¶ï ‡¶∏‡ßç‡¶•‡¶æ‡¶®‡¶æ‡¶Ç‡¶ï ‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶Ø‡¶º ‡¶§‡ßç‡¶∞‡¶ø‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶ø‡¶ï ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞, ‡¶Ø‡ßá‡¶Æ‡¶® ‡¶è‡¶ï‡ßç‡¶∏, ‡¶ì‡¶Ø‡¶º‡¶æ‡¶á, ‡¶ú‡ßá‡¶°, ‡¶è‡¶¨‡¶Ç ‡¶ú‡ßá‡¶°‡•§\\n\\n'\n",
            "Row 2: '[07:52 - 12:20]\\n‡¶è‡¶á ‡¶Ö‡¶Ç‡¶∂‡ßá, ‡¶§‡ßç‡¶∞‡¶ø‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶ø‡¶ï ‡¶∏‡ßç‡¶•‡¶æ‡¶®‡¶æ‡¶Ç‡¶ï ‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶Ø‡¶º ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ‡¶∞ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶® ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶á‡¶ï‡ßç‡¶Ø‡¶æ‡¶™ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§ ‡¶ì‡¶Ø‡¶º‡¶æ‡¶á ‡¶Ö‡¶ï‡ßç‡¶∑ ‡¶¨‡¶∞‡¶æ‡¶¨‡¶∞ ‡¶è‡¶ï‡¶ï ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ï‡ßá ‡¶ï‡ßç‡¶Ø‡¶æ‡¶™ ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶™ ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶™‡ßç‡¶∞‡¶ï‡¶æ‡¶∂ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶è‡¶¨‡¶Ç ‡¶ú‡ßá‡¶° ‡¶Ö‡¶ï‡ßç‡¶∑ ‡¶¨‡¶∞‡¶æ‡¶¨‡¶∞ ‡¶è‡¶ï ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶® ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£ ‡¶ï‡¶∞‡¶§‡ßá ‡¶ï‡ßá‡¶ï ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶™ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§ ‡¶π‡¶Ø‡¶º‡•§ ‡¶è‡¶ï‡ßç‡¶∏, ‡¶ì‡¶Ø‡¶º‡¶æ‡¶á, ‡¶ú‡ßá‡¶° ‡¶è‡¶∞ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶® ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø, ‡¶Æ‡ßÇ‡¶≤‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ ‡¶•‡ßá‡¶ï‡ßá ‡¶è ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ ‡¶™‡¶∞‡ßç‡¶Ø‡¶®‡ßç‡¶§ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º ‡¶è‡¶¨‡¶Ç ‡¶è‡¶á ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ü‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶Æ‡¶§‡¶≤‡ßÄ‡¶Ø‡¶º ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶¨‡¶ø‡¶¨‡ßá‡¶ö‡¶ø‡¶§ ‡¶π‡¶Ø‡¶º‡•§ ‡¶∏‡¶Æ‡¶§‡¶≤‡ßÄ‡¶Ø‡¶º ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá, ‡¶Ø‡¶¶‡¶ø ‡¶è‡¶ï‡¶æ‡¶ß‡¶ø‡¶ï ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶è‡¶ï‡¶á ‡¶∏‡¶Æ‡¶§‡¶≤‡ßá ‡¶•‡¶æ‡¶ï‡ßá, ‡¶§‡¶¨‡ßá ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶® ‡¶π‡¶¨‡ßá ‡¶ì ‡¶è‡¶ï‡ßç‡¶∏‡¶ï‡ßç‡¶∏‡ßá‡¶°‡•§\\n\\n'\n",
            "Row 3: '[12:15 - 16:05]\\n‡¶è‡¶á ‡¶Ö‡¶Ç‡¶∂‡ßá, ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶Ø‡ßã‡¶ó‡ßá‡¶∞ ‡¶§‡ßç‡¶∞‡¶ø‡¶≠‡ßÅ‡¶ú ‡¶∏‡ßÇ‡¶§‡ßç‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ‡¶§‡ßá ‡¶¶‡ßÅ‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶õ‡ßá ‡¶è‡¶¨‡¶Ç ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶≤‡¶¨‡ßç‡¶ß‡¶ø ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶∏‡¶æ‡¶Æ‡¶æ‡¶®‡ßç‡¶§‡¶∞‡¶ø‡¶ï ‡¶∏‡ßÇ‡¶§‡ßç‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶è ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶≤‡¶¨‡ßç‡¶ß‡¶ø‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§ ‡¶è‡¶∞‡¶™‡¶∞, ‡¶è‡¶ï‡¶ü‡¶ø ‡¶§‡ßç‡¶∞‡¶ø‡¶≠‡ßÅ‡¶ú‡ßá‡¶∞ ‡¶§‡ßÉ‡¶§‡ßÄ‡¶Ø‡¶º ‡¶¨‡¶æ‡¶π‡ßÅ ‡¶¨‡¶ø‡¶™‡¶∞‡ßÄ‡¶§ ‡¶¶‡¶ø‡¶ï‡ßá ‡¶ö‡¶≤‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º, ‡¶Ø‡¶æ ‡¶§‡ßç‡¶∞‡¶ø‡¶≠‡ßÅ‡¶ú‡ßá‡¶∞ ‡¶¶‡ßç‡¶¨‡¶ø‡¶§‡ßÄ‡¶Ø‡¶º ‡¶¨‡¶æ‡¶π‡ßÅ‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶¨‡¶ø‡¶™‡¶∞‡ßÄ‡¶§ ‡¶¶‡¶ø‡¶ï‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º‡•§ ‡¶è‡¶á ‡¶∏‡ßÇ‡¶§‡ßç‡¶∞ ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ, ‡¶§‡ßç‡¶∞‡¶ø‡¶≠‡ßÅ‡¶ú‡ßá‡¶∞ ‡¶§‡¶ø‡¶®‡¶ü‡¶ø ‡¶¨‡¶æ‡¶π‡ßÅ ‡¶¨‡¶ø‡¶™‡¶∞‡ßÄ‡¶§‡¶≠‡¶æ‡¶¨‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶§‡ßÉ‡¶§‡ßÄ‡¶Ø‡¶º ‡¶¨‡¶æ‡¶π‡ßÅ‡¶ü‡¶ø ‡¶¨‡¶ø‡¶™‡¶∞‡ßÄ‡¶§ ‡¶¶‡¶ø‡¶ï‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá‡•§ ‡¶è‡¶á ‡¶∏‡ßÇ‡¶§‡ßç‡¶∞‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá, ‡¶§‡ßç‡¶∞‡¶ø‡¶≠‡ßÅ‡¶ú‡¶ü‡¶ø‡¶∞ ‡¶§‡ßÉ‡¶§‡ßÄ‡¶Ø‡¶º ‡¶¨‡¶æ‡¶π‡ßÅ‡¶∞ ‡¶≤‡¶¨‡ßç‡¶ß‡¶ø‡¶ì ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡•§\\n\\n'\n",
            "Row 4: '[16:00 - 20:33]\\n‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶Ø‡ßã‡¶ó‡ßá‡¶∞ ‡¶¨‡¶π‡ßÅ‡¶≠‡ßÅ‡¶ú ‡¶∏‡ßÇ‡¶§‡ßç‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá, ‡¶è ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶≤‡¶¨‡ßç‡¶ß‡¶ø ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ï‡ßá ‡¶è‡¶ï‡¶á ‡¶ï‡ßç‡¶∞‡¶Æ‡ßá ‡¶∏‡¶æ‡¶ú‡¶æ‡¶®‡ßã ‡¶π‡¶Ø‡¶º ‡¶è‡¶¨‡¶Ç ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶≤‡¶¨‡ßç‡¶ß‡¶ø‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶è‡¶¨‡¶Ç ‡¶¶‡¶ø‡¶ï ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§ ‡¶è‡¶∞‡¶™‡¶∞, ‡¶è‡¶ï‡ßç‡¶∏ ‡¶Ö‡¶ï‡ßç‡¶∑ ‡¶¨‡¶∞‡¶æ‡¶¨‡¶∞ ‡¶ï‡¶§‡¶ó‡ßÅ‡¶≤‡ßã ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶ó‡ßá‡¶õ‡ßá, ‡¶§‡¶æ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞ ‡¶ï‡¶æ‡¶†‡¶æ‡¶Æ‡ßã ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶™‡¶æ‡¶¶‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ ‡¶è‡¶¨‡¶Ç ‡¶∂‡ßá‡¶∑ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶∂‡ßÄ‡¶∞‡ßç‡¶∑‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶æ‡¶ï‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶≤‡¶¨‡ßç‡¶ß‡¶ø‡¶ì ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£ ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡•§ ‡¶è‡¶∞‡¶™‡¶∞ ‡¶¶‡ßç‡¶¨‡¶ø‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶ø‡¶ï ‡¶∏‡ßç‡¶•‡¶æ‡¶®‡¶æ‡¶Ç‡¶ï ‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶∞ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶∏‡ßç‡¶•‡¶æ‡¶®‡¶æ‡¶®‡ßç‡¶§‡¶∞‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá‡•§\\n\\n'\n",
            "Row 5: '[20:24 - 26:22]\\n‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶Ü‡¶Ø‡¶º‡¶§‡¶æ‡¶ï‡¶æ‡¶∞ ‡¶Ü‡¶ï‡ßÉ‡¶§‡¶ø‡¶∞ ‡¶ò‡¶®‡¶¨‡¶∏‡ßç‡¶§‡ßÅ‡¶∞ ‡¶ï‡¶∞‡ßç‡¶£ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá, ‡¶Æ‡ßÇ‡¶≤‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ ‡¶•‡ßá‡¶ï‡ßá ‡¶è‡¶ï‡ßç‡¶∏ ‡¶Ö‡¶ï‡ßç‡¶∑ ‡¶¨‡¶∞‡¶æ‡¶¨‡¶∞ ‡¶è‡¶ï‡ßç‡¶∏ ‡¶ò‡¶∞ ‡¶è‡¶¨‡¶Ç ‡¶ì‡¶Ø‡¶º‡¶æ‡¶á ‡¶Ö‡¶ï‡ßç‡¶∑ ‡¶¨‡¶∞‡¶æ‡¶¨‡¶∞ ‡¶ì‡¶Ø‡¶º‡¶æ‡¶á ‡¶ò‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶ï‡¶∞‡ßç‡¶£ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§ ‡¶è‡¶∞‡¶™‡¶∞, ‡¶è‡¶ï‡¶ï ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶Ø‡¶æ‡¶∞ ‡¶´‡¶≤‡ßá ‡¶è ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡•§ ‡¶è‡¶á ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡¶Ø‡¶º, ‡¶Ø‡¶¶‡¶ø ‡¶è‡¶ï‡ßç‡¶∏ ‡¶è‡¶¨‡¶Ç ‡¶ì‡¶Ø‡¶º‡¶æ‡¶á ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶∏‡¶Æ‡¶æ‡¶® ‡¶π‡¶Ø‡¶º, ‡¶§‡¶¨‡ßá ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶≠‡¶æ‡¶ó‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá ‡¶ï‡¶∞‡ßç‡¶£ ‡¶®‡¶ø‡¶∞‡ßç‡¶£‡¶Ø‡¶º ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡•§ ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£‡¶∏‡ßç‡¶¨‡¶∞‡ßÇ‡¶™, ‡¶Ø‡¶¶‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶è‡¶ï‡ßç‡¶∏ ‡¶Ö‡¶ï‡ßç‡¶∑‡ßá‡¶∞ ‡¶¨‡¶ø‡¶™‡¶∞‡ßÄ‡¶§‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º, ‡¶§‡¶¨‡ßá ‡¶§‡¶æ‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶Æ‡¶æ‡¶á‡¶®‡¶æ‡¶∏ ‡¶π‡¶¨‡ßá‡•§\\n\\n'\n",
            "Row 6: '[26:18 - 31:22]\\n‡¶è‡¶á ‡¶Ö‡¶Ç‡¶∂‡ßá ‡¶è‡¶ï‡¶ï ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶Ü‡¶≤‡ßã‡¶ö‡¶®‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶è ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶æ‡¶®‡ßç‡¶§‡¶∞‡¶æ‡¶≤ ‡¶¶‡ßÅ‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ (‡¶è ‡¶è‡¶¨‡¶Ç ‡¶è) ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£ ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá, ‡¶Ø‡¶¶‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶è ‡¶¨‡¶∞‡¶æ‡¶¨‡¶∞ ‡¶™‡¶æ‡¶Å‡¶ö ‡¶è‡¶ï‡¶ï ‡¶¶‡ßà‡¶∞‡ßç‡¶ò‡ßç‡¶Ø‡ßá‡¶∞ ‡¶¨‡¶æ ‡¶™‡¶æ‡¶Å‡¶ö ‡¶Æ‡¶æ‡¶® ‡¶¨‡¶ø‡¶∂‡¶ø‡¶∑‡ßç‡¶ü ‡¶π‡¶Ø‡¶º, ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶∏‡ßá‡¶á ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ï‡ßá ‡¶è‡¶ï‡ßç‡¶∏ ‡¶Ö‡¶ï‡ßç‡¶∑ ‡¶¨‡¶∞‡¶æ‡¶¨‡¶∞ ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§ ‡¶è‡¶∞‡¶™‡¶∞, ‡¶Ø‡¶¶‡¶ø ‡¶¶‡ßÅ‡¶á‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶§‡¶¨‡ßá ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Ø‡ßã‡¶ó‡¶´‡¶≤ ‡¶π‡¶¨‡ßá ‡¶™‡ßç‡¶≤‡¶æ‡¶∏ - ‡¶Æ‡¶æ‡¶á‡¶®‡¶æ‡¶∏‡•§ ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£‡¶∏‡ßç‡¶¨‡¶∞‡ßÇ‡¶™, ‡¶Ø‡¶¶‡¶ø ‡¶è ‡¶è‡¶¨‡¶Ç ‡¶ú‡ßá‡¶° ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶è‡¶ï‡ßá ‡¶Ö‡¶™‡¶∞‡ßá‡¶∞ ‡¶¨‡¶ø‡¶™‡¶∞‡ßÄ‡¶§ ‡¶¶‡¶ø‡¶ï‡ßá ‡¶•‡¶æ‡¶ï‡ßá, ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Ø‡ßã‡¶ó ‡¶¨‡¶ø‡¶Ø‡¶º‡ßã‡¶ó ‡¶π‡¶¨‡ßá, ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶ø‡¶Ø‡¶º‡ßã‡¶ó ‡¶ï‡¶∞‡¶≤‡ßá ‡¶¶‡ßç‡¶¨‡¶ø‡¶§‡ßÄ‡¶Ø‡¶º ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ø‡¶æ‡¶¨‡ßá‡•§\\n\\n'\n",
            "Row 7: '[31:15 - 38:37]\\n‡¶è‡¶á ‡¶Ö‡¶Ç‡¶∂‡ßá, ‡¶è ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶≤‡¶¨‡ßç‡¶ß‡¶ø ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ï‡ßç‡¶Ø‡¶æ‡¶™ ‡¶™‡ßç‡¶≤‡¶æ‡¶∏ ‡¶∏‡¶ø‡¶ï‡ßç‡¶∏ ‡¶∏‡ßç‡¶ï‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶™‡ßç‡¶≤‡¶æ‡¶∏ ‡¶ï‡ßç‡¶Ø‡¶æ‡¶≤‡¶ï‡ßÅ‡¶≤‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶è‡¶∞‡¶™‡¶∞, ‡¶è ‡¶ì ‡¶¨‡¶ø ‡¶è‡¶∞ ‡¶≤‡¶¨‡ßç‡¶ß‡¶ø‡¶∞ ‡¶∏‡¶Æ‡¶æ‡¶®‡ßç‡¶§‡¶∞‡¶æ‡¶≤ ‡¶è‡¶ï‡¶ï ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶ï‡ßç‡¶Ø‡¶æ‡¶™ ‡¶Æ‡¶æ‡¶á‡¶®‡¶æ‡¶∏ ‡¶∏‡¶ø‡¶ï‡ßç‡¶∏ ‡¶∏‡ßç‡¶ï‡ßã‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶™‡ßç‡¶≤‡¶æ‡¶∏ ‡¶è‡¶¨‡¶Ç ‡¶á‡¶ü‡¶æ ‡¶ï‡ßç‡¶Ø‡¶æ‡¶™ ‡¶™‡ßç‡¶≤‡¶æ‡¶∏ ‡¶ï‡ßç‡¶Ø‡¶æ‡¶≤‡¶ï‡ßÅ‡¶≤‡ßá‡¶∂‡¶® ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§ ‡¶è‡¶á ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶§‡ßá, ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá ‡¶ï‡ßç‡¶Ø‡¶æ‡¶™ ‡¶Æ‡¶æ‡¶á‡¶®‡¶æ‡¶∏ ‡¶õ‡¶Ø‡¶º ‡¶∏‡ßç‡¶ï‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶™‡ßç‡¶≤‡¶æ‡¶∏ ‡¶è‡¶¨‡¶Ç ‡¶™‡ßç‡¶≤‡¶æ‡¶∏ ‡¶Æ‡¶æ‡¶á‡¶®‡¶æ‡¶∏ ‡¶∏‡¶ø‡¶ï‡ßç‡¶∏ ‡¶™‡ßç‡¶≤‡¶æ‡¶∏ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶Ø‡¶æ ‡¶≤‡¶¨‡ßç‡¶ß‡¶ø‡¶ü‡¶ø‡¶∞ ‡¶∏‡¶Æ‡¶æ‡¶®‡ßç‡¶§‡¶∞‡¶æ‡¶≤ ‡¶è‡¶ï‡¶ï ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶¨‡¶ø‡¶¨‡ßá‡¶ö‡¶ø‡¶§ ‡¶π‡¶Ø‡¶º‡•§ ‡¶™‡¶∞‡¶¨‡¶∞‡ßç‡¶§‡ßÄ‡¶§‡ßá, ‡¶è ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶ø ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ‡¶∞ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶® ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶®‡¶ø‡¶∞‡ßç‡¶£‡¶Ø‡¶º‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø, ‡¶è, ‡¶¨‡¶ø ‡¶è‡¶¨‡¶Ç ‡¶è, ‡¶è‡•§\\n\\n'\n",
            "Row 8: '[38:26 - 44:21]\\n‡¶è‡¶á ‡¶Ö‡¶Ç‡¶∂‡ßá, ‡¶è‡¶¨‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá, ‡¶¨‡¶ø‡¶Ø‡¶º‡ßá ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶è‡¶¨‡¶Ç ‡¶Ü‡¶¶‡¶ø ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ‡¶∞ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶® ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§ ‡¶è‡¶∞‡¶™‡¶∞, ‡¶è ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶§‡ßá ‡¶¨‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶è ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ‡¶∞ ‡¶∏‡ßç‡¶•‡¶æ‡¶®‡¶æ‡¶ô‡ßç‡¶ï ‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ü‡¶õ‡ßá, ‡¶è‡¶¨‡¶Ç ‡¶ï‡¶ø‡¶â ‡¶è‡¶∞ ‡¶∏‡ßç‡¶•‡¶æ‡¶®‡¶æ‡¶ô‡ßç‡¶ï‡¶ì ‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ü‡¶õ‡ßá‡•§ ‡¶è‡¶á ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá, ‡¶è, ‡¶¨‡¶ø, ‡¶è‡¶¨‡¶Ç ‡¶ì‡¶Ø‡¶º‡¶æ‡¶á ‡¶è‡¶∞ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º, ‡¶Ø‡¶æ ‡¶∏‡¶Æ‡¶æ‡¶®‡ßç‡¶§‡¶∞‡¶æ‡¶≤ ‡¶è‡¶ï‡¶ï ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá‡•§ ‡¶è‡¶ñ‡¶æ‡¶®‡ßá, ‡¶ï‡¶ø‡¶â ‡¶è‡¶∞ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶® ‡¶è‡¶¨‡¶Ç ‡¶Ö‡¶®‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶Ø ‡¶∏‡ßç‡¶•‡¶æ‡¶®‡¶æ‡¶ô‡ßç‡¶ï ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá, ‡¶è ‡¶ì ‡¶¨‡¶ø ‡¶è‡¶∞ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶® ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶Ø‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶Æ‡¶¨‡¶æ‡¶π‡ßÅ ‡¶§‡ßç‡¶∞‡¶ø‡¶≠‡ßÅ‡¶ú ‡¶ó‡¶†‡¶® ‡¶ï‡¶∞‡ßá‡•§\\n\\n'\n",
            "Row 9: '[44:15 - 50:03]\\n‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶Æ‡¶ï‡ßã‡¶£‡ßÄ ‡¶§‡ßç‡¶∞‡¶ø‡¶≠‡ßÅ‡¶ú ‡¶ó‡¶†‡¶® ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá, ‡¶è‡¶¨‡¶ø ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶ø‡¶∏‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶® ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶è‡¶¨‡¶ø, ‡¶¨‡¶ø‡¶∏‡¶ø, ‡¶è‡¶¨‡¶Ç ‡¶∏‡¶ø ‡¶¨‡¶æ‡¶π‡ßÅ‡¶∞ ‡¶¶‡ßà‡¶∞‡ßç‡¶ò‡ßç‡¶Ø ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§ ‡¶è‡¶∞‡¶™‡¶∞, ‡¶è‡¶≠‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶è‡¶¨‡¶Ç ‡¶è‡¶¨‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶™‡¶∞, ‡¶è‡¶¨‡¶ø‡¶∏‡¶ø ‡¶§‡ßç‡¶∞‡¶ø‡¶≠‡ßÅ‡¶ú ‡¶ó‡¶†‡¶® ‡¶π‡¶Ø‡¶º, ‡¶Ø‡¶æ ‡¶™‡¶ø‡¶•‡¶æ‡¶ó‡ßã‡¶∞‡¶æ‡¶∏‡ßá‡¶∞ ‡¶®‡ßÄ‡¶§‡¶ø ‡¶Æ‡ßá‡¶®‡ßá ‡¶ö‡¶≤‡ßá‡•§ ‡¶è‡¶á ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡¶Ø‡¶º, ‡¶§‡¶ø‡¶®‡¶ü‡¶ø ‡¶¨‡¶æ‡¶π‡ßÅ‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßá, ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶¶‡ßà‡¶∞‡ßç‡¶ò‡ßç‡¶Ø ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡•§\\n\\n'\n",
            "Row 10: '[49:52 - 56:15]\\n‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶¶‡ßÅ‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶°‡¶ü ‡¶ó‡ßÅ‡¶£‡¶´‡¶≤ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá, ‡¶è ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶è‡¶∞ ‡¶≤‡¶Æ‡ßç‡¶¨ ‡¶Ö‡¶≠‡¶ø‡¶ï‡ßç‡¶∑‡ßá‡¶™ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶§‡ßá ‡¶¨‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶è ‡¶°‡¶ü ‡¶¨‡¶ø ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶ø ‡¶è‡¶ï‡ßç‡¶∏ ‡¶Ü‡¶á ‡¶ï‡ßç‡¶Ø‡¶æ‡¶™ ‡¶™‡ßç‡¶≤‡¶æ‡¶∏ ‡¶¨‡¶ø ‡¶ì‡¶Ø‡¶º‡¶æ‡¶á ‡¶è‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶≤‡¶Æ‡ßç‡¶¨ ‡¶¨‡¶ø‡¶ï‡¶∏ ‡¶•‡¶ø‡¶ü‡¶æ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§ ‡¶è‡¶∞‡¶™‡¶∞, ‡¶è ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶ø ‡¶è‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡ßç‡¶ï‡ßá‡¶≤‡¶æ‡¶∞ ‡¶∞‡¶æ‡¶∂‡¶ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá, ‡¶Ø‡¶æ ‡¶∏‡ßç‡¶ï‡ßá‡¶≤‡¶æ‡¶∞ ‡¶ó‡ßÅ‡¶£‡¶´‡¶≤ ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡•§ ‡¶è‡¶á ‡¶ó‡ßÅ‡¶£‡¶´‡¶≤‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá ‡¶Ü‡¶Ø‡¶º‡ßá‡¶∞ ‡¶∂‡¶π‡¶∞‡¶ó‡ßÅ‡¶≤‡ßã ‡¶è‡¶ï‡¶∏‡¶æ‡¶•‡ßá ‡¶≤‡¶Æ‡ßç‡¶¨ ‡¶π‡¶Ø‡¶º, ‡¶è‡¶¨‡¶Ç ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶°‡¶ü ‡¶ó‡ßÅ‡¶®‡¶´‡¶≤ ‡¶™‡¶∞‡ßÄ‡¶ï‡ßç‡¶∑‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶Ü‡¶∏‡ßá‡•§\\n\\n'\n",
            "Row 11: '[56:00 - 63:25]\\n‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶´‡¶∞‡ßç‡¶Æ‡ßÅ‡¶≤‡¶æ ‡¶Ü‡¶≤‡ßã‡¶ö‡¶®‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶¶‡¶ø‡¶ï ‡¶¨‡¶∞‡¶æ‡¶¨‡¶∞ ‡¶è‡¶ï‡¶ï ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶´‡¶∞‡ßç‡¶Æ‡ßÅ‡¶≤‡¶æ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá, ‡¶è ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶è‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶ï‡ßã‡¶£ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶§‡ßá ‡¶¨‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶è‡¶∞‡¶™‡¶∞, ‡¶è ‡¶°‡¶ü ‡¶¨‡¶ø ‡¶¨‡¶æ‡¶á ‡¶¨‡¶ø ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶è‡¶ï‡¶ï ‡¶≠‡¶ø‡¶ï‡ßç‡¶ü‡¶∞ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶Ø‡¶æ ‡¶è ‡¶°‡¶ü ‡¶¨‡¶æ‡¶á ‡¶∞‡ßÅ‡¶ü ‡¶ì‡¶≠‡¶æ‡¶∞ ‡¶Ö‡¶´ ‡¶∏‡ßç‡¶ï‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶™‡ßç‡¶≤‡¶æ‡¶∏ ‡¶ì‡¶Ø‡¶º‡¶æ‡¶® ‡¶∏‡ßç‡¶ï‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶∞‡ßÅ‡¶ü ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§ ‡¶è‡¶á ‡¶´‡¶∞‡ßç‡¶Æ‡ßÅ‡¶≤‡¶æ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá, ‡¶è ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶ø ‡¶è‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡¶¨‡¶∞‡ßç‡¶§‡ßÄ ‡¶ï‡ßã‡¶£ ‡¶®‡¶ø‡¶∞‡ßç‡¶£‡¶Ø‡¶º ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º ‡¶è‡¶¨‡¶Ç ‡¶è‡¶∞ ‡¶Æ‡¶°‡ßÅ‡¶≤‡¶æ‡¶∏ ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ ‡¶è‡¶¨‡¶Ç ‡¶â‡¶™‡¶æ‡¶Ç‡¶∂‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º, ‡¶Ø‡¶æ ‡¶®‡ßá‡¶ó‡ßá‡¶ü‡¶ø‡¶≠ ‡¶π‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡•§\\n\\n'\n",
            "Row 12: '[63:19 - 69:00]\\n‡¶è‡¶á ‡¶Ö‡¶Ç‡¶∂‡ßá, ‡¶è‡¶ï‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶è‡¶ï‡ßç‡¶∏ ‡¶Ö‡¶ï‡ßç‡¶∑‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶ï‡¶§ ‡¶ï‡ßã‡¶£ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶õ‡ßá ‡¶§‡¶æ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶™‡¶¶‡ßç‡¶ß‡¶§‡¶ø ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá, ‡¶è‡¶ï‡ßç‡¶∏ ‡¶Ö‡¶ï‡ßç‡¶∑ ‡¶è‡¶¨‡¶Ç ‡¶ì‡¶Ø‡¶º‡¶æ‡¶á ‡¶Ö‡¶ï‡ßç‡¶∑‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶ï‡ßã‡¶£ ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶ï‡¶∏ ‡¶•‡¶ø‡¶ü‡¶æ = ‡¶è ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶°‡¶ü ‡¶¨‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶á‡¶®‡¶ü‡ßÅ ‡¶Ü‡¶á ‡¶ï‡ßç‡¶Ø‡¶æ‡¶™‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§ ‡¶è‡¶∞‡¶™‡¶∞, ‡¶è‡¶ï‡ßç‡¶∏ ‡¶è‡¶¨‡¶Ç ‡¶ì‡¶Ø‡¶º‡¶æ‡¶á ‡¶è‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶ï‡ßã‡¶£ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø, ‡¶è‡¶ï‡ßç‡¶∏ ‡¶ì ‡¶ì‡¶Ø‡¶º‡¶æ‡¶á ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º ‡¶è‡¶¨‡¶Ç ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶ï‡¶ú‡¶æ‡¶≤ ‡¶´‡¶æ‡¶Å‡¶ï‡¶ø ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡•§ ‡¶è‡¶á ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡¶Ø‡¶º, ‡¶è‡¶ï‡ßç‡¶∏, ‡¶ì‡¶Ø‡¶º‡¶æ‡¶á, ‡¶è‡¶¨‡¶Ç ‡¶ú‡ßá‡¶° ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶™‡¶∞, ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ï‡¶ú‡¶æ‡¶≤ ‡¶§‡ßç‡¶∞‡ßÅ‡¶ü‡¶ø ‡¶®‡¶ø‡¶∞‡ßç‡¶£‡¶Ø‡¶º ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º, ‡¶Ø‡¶æ ‡¶ï‡¶ú‡¶æ‡¶≤ ‡¶´‡¶æ‡¶≤ ‡¶´‡¶æ‡¶Å‡¶ï‡¶ø ‡¶π‡¶¨‡ßá‡•§\\n\\n'\n",
            "Row 13: '[68:37 - 74:50]\\n‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶Ü‡¶≤‡¶´‡¶æ, ‡¶¨‡¶ø‡¶ü‡¶æ, ‡¶ó‡¶æ‡¶Æ‡¶æ ‡¶è‡¶¨‡¶Ç ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ó‡ßÅ‡¶£‡¶´‡¶≤ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá, ‡¶è‡¶ï‡ßç‡¶∏, ‡¶ì‡¶Ø‡¶º‡¶æ‡¶á, ‡¶ú‡ßá‡¶°, ‡¶è‡¶¨‡¶Ç ‡¶ï‡¶∏ ‡¶è‡¶∞ ‡¶ó‡ßÅ‡¶£‡¶´‡¶≤ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§ ‡¶è‡¶∞‡¶™‡¶∞, ‡¶è ‡¶™‡ßç‡¶≤‡¶æ‡¶∏ ‡¶¨‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶ø ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ï‡¶∏ ‡¶á‡¶®‡¶≠‡¶æ‡¶∞‡ßç‡¶∏, ‡¶ï‡¶∏ ‡¶¨‡¶æ‡¶á ‡¶∞‡ßÅ‡¶ü ‡¶ì‡¶≠‡¶æ‡¶∞ ‡¶ì‡¶≠‡¶æ‡¶∞ ‡¶Ö‡¶´ ‡¶∏‡ßç‡¶ï‡¶Ø‡¶º‡¶æ‡¶∞, ‡¶∏‡ßç‡¶ï‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶™‡ßç‡¶≤‡¶æ‡¶∏ ‡¶ü‡ßÅ ‡¶∏‡ßç‡¶ï‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶è ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶∂‡ßÇ‡¶®‡ßç‡¶Ø ‡¶π‡¶Ø‡¶º ‡¶è‡¶¨‡¶Ç ‡¶è ‡¶è‡¶∞ ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ ‡ß© ‡¶π‡¶Ø‡¶º‡•§ ‡¶è ‡¶∏‡ßç‡¶ï‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶™‡ßç‡¶≤‡¶æ‡¶∏ ‡ß® ‡¶è, ‡¶¨‡¶ø ‡¶∏‡ßç‡¶ï‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶™‡ßç‡¶≤‡¶æ‡¶∏ ‡ß©, ‡¶è‡¶¨‡¶Ç ‡¶è ‡¶∏‡ßç‡¶ï‡¶Ø‡¶º‡¶æ‡¶∞ + ‡ß® ‡¶è = ‡ß© ‡¶π‡¶¨‡ßá‡•§\\n\\n'\n",
            "Row 14: '[74:45 - 81:30]\\n‡¶è‡¶á ‡¶Ö‡¶Ç‡¶∂‡ßá ‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶ï ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡¶õ‡ßá‡¶® ‡¶ï‡¶ø‡¶≠‡¶æ‡¶¨‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶â‡¶™‡¶æ‡¶Ç‡¶∂ ‡¶è ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶¨‡¶∞‡¶æ‡¶¨‡¶∞ ‡¶¨‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶è‡¶∞ ‡¶â‡¶™‡¶æ‡¶Ç‡¶∂‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶≤‡¶Æ‡ßç‡¶¨ ‡¶ï‡¶∞‡ßá ‡¶ó‡ßÅ‡¶£ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§ ‡¶§‡¶ø‡¶®‡¶ø ‡¶¨‡¶≤‡ßá‡¶®, ‡¶Ø‡¶¶‡¶ø ‡¶¶‡ßÅ‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶è‡¶ï‡ßá ‡¶Ö‡¶™‡¶∞‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶¨‡¶∞‡ßç‡¶§‡ßÄ ‡¶π‡¶Ø‡¶º, ‡¶§‡¶¨‡ßá ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶ï‡ßã‡¶£ ‡¶•‡¶æ‡¶ï‡ßá, ‡¶Ø‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡ßç‡¶ï‡ßá‡¶≤‡¶æ‡¶∞ ‡¶∞‡¶æ‡¶∂‡¶ø‡•§ ‡¶§‡¶ø‡¶®‡¶ø ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡ßá‡¶® ‡¶Ø‡ßá, ‡¶Ø‡¶¶‡¶ø ‡¶¶‡ßÅ‡¶á‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ï‡ßá ‡¶ï‡ßç‡¶∞‡¶∏ ‡¶™‡ßç‡¶∞‡ßã‡¶°‡¶æ‡¶ï‡ßç‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡¶¨‡¶∞‡ßç‡¶§‡ßÄ ‡¶ï‡ßã‡¶£ (‡¶è) ‡¶è‡¶¨‡¶Ç ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶ï‡ßã‡¶£ (‡¶¨‡¶ø) ‡¶è‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶≤‡¶Æ‡ßç‡¶¨ ‡¶π‡¶Ø‡¶º, ‡¶Ø‡¶æ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶∞‡¶æ‡¶∂‡¶ø ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶™‡¶∞‡¶ø‡¶ö‡¶ø‡¶§‡•§\\n\\n'\n",
            "Row 15: '[81:15 - 87:00]\\n‡¶è‡¶á ‡¶Ö‡¶Ç‡¶∂‡ßá, ‡¶è ‡¶™‡ßç‡¶≤‡¶æ‡¶∏ ‡¶¨‡¶ø ‡¶è‡¶∞ ‡¶§‡¶≤ ‡¶¨‡¶∞‡¶æ‡¶¨‡¶∞ ‡¶¨‡¶æ‡¶á‡¶∞‡ßá‡¶∞ ‡¶¶‡¶ø‡¶ï‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá, ‡¶è ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶ø ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø, ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá ‡¶¨‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶¶‡¶ø‡¶ï‡ßá ‡¶∏‡ßç‡¶•‡¶æ‡¶™‡¶® ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§ ‡¶è‡¶∞‡¶™‡¶∞, ‡¶¨‡¶ø ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶ø ‡¶è‡¶ï‡ßç‡¶∏ ‡¶è‡¶∞ ‡¶ï‡ßç‡¶∞‡¶∏ ‡¶™‡ßç‡¶∞‡¶°‡¶æ‡¶ï‡ßç‡¶ü ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶¨‡¶ø ‡¶è‡¶¨‡¶Ç ‡¶è ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶∏‡¶Æ‡¶æ‡¶® ‡¶•‡¶æ‡¶ï‡ßá‡•§ ‡¶è‡¶á ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ ‡¶Ö‡¶®‡ßÅ‡¶∏‡¶∞‡¶£ ‡¶ï‡¶∞‡ßá, ‡¶è ‡¶ì ‡¶¨‡¶ø ‡¶è‡¶∞ ‡¶®‡¶ø‡¶∞‡ßç‡¶£‡¶æ‡¶Ø‡¶º‡¶ï ‡¶∏‡¶Æ‡¶æ‡¶ß‡¶æ‡¶® ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º ‡¶è‡¶¨‡¶Ç ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ï‡ßç‡¶∞‡¶∏ ‡¶™‡ßç‡¶∞‡¶°‡¶æ‡¶ï‡¶∂‡¶®‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá ‡¶´‡¶≤‡¶æ‡¶´‡¶≤ ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡•§ ‡¶è‡¶õ‡¶æ‡¶°‡¶º‡¶æ, ‡¶Ü‡¶Ø‡¶º‡¶§ ‡¶è‡¶ï‡¶ï ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ó‡ßÅ‡¶≤‡ßã ‡¶Ø‡ßá‡¶Æ‡¶® ‡¶è ‡¶ì‡¶Ø‡¶º‡¶æ‡¶á, ‡¶¨‡¶ø ‡¶è‡¶ï‡ßç‡¶∏, ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶ø ‡¶ì‡¶Ø‡¶º‡¶æ‡¶á ‡¶è‡¶∞ ‡¶ï‡ßç‡¶∞‡¶∏ ‡¶™‡ßç‡¶∞‡ßã‡¶°‡¶æ‡¶ï‡ßç‡¶ü‡ßá‡¶∞ ‡¶ï‡ßç‡¶∞‡¶∏ ‡¶™‡ßç‡¶∞‡¶°‡¶æ‡¶ï‡ßç‡¶ü‡ßá‡¶∞ ‡¶ï‡ßç‡¶∞‡¶∏ ‡¶™‡ßç‡¶∞‡¶°‡¶ï‡ßç‡¶ü ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶™‡¶¶‡ßç‡¶ß‡¶§‡¶ø ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá‡•§\\n\\n'\n",
            "Row 16: '[86:51 - 95:15]\\n‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶Ü‡¶á ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶è‡¶¨‡¶Ç ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡¶¨‡¶∞‡ßç‡¶§‡ßÄ ‡¶ï‡ßã‡¶£ ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶Ü‡¶≤‡ßã‡¶ö‡¶®‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶Ø‡¶¶‡¶ø ‡¶Ü‡¶á ‡¶ï‡ßç‡¶∞‡¶∏ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶§‡¶¨‡ßá ‡¶Ü‡¶á ‡¶ï‡ßç‡¶∞‡¶∏‡¶° ‡¶π‡¶¨‡ßá ‡¶®‡¶æ, ‡¶ï‡¶æ‡¶∞‡¶£ ‡¶è‡¶ü‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶è‡¶ï‡¶ï ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡•§ ‡¶Ø‡¶¶‡¶ø ‡¶¶‡ßÅ‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶è‡¶ï‡ßá ‡¶Ö‡¶™‡¶∞‡ßá‡¶∞ ‡¶¨‡¶ø‡¶™‡¶∞‡ßÄ‡¶§ ‡¶¶‡¶ø‡¶ï‡ßá ‡¶•‡¶æ‡¶ï‡ßá, ‡¶§‡¶¨‡ßá ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡¶¨‡¶∞‡ßç‡¶§‡ßÄ ‡¶ï‡ßã‡¶£‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶π‡¶¨‡ßá ‡¶Æ‡¶æ‡¶á‡¶®‡¶æ‡¶∏ ‡¶ì‡¶Ø‡¶º‡¶æ‡¶® ‡¶á‡¶®‡¶ü‡ßÅ ‡¶ì‡¶Ø‡¶º‡¶æ‡¶®, ‡¶è‡¶¨‡¶Ç ‡¶Ø‡¶¶‡¶ø ‡¶ï‡ßç‡¶∞‡¶∏ ‡¶Ü‡¶á ‡¶π‡¶Ø‡¶º, ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶Æ‡¶æ‡¶á‡¶®‡¶æ‡¶∏ ‡¶ï‡ßá ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ø‡¶æ‡¶¨‡ßá‡•§ ‡¶è‡¶õ‡¶æ‡¶°‡¶º‡¶æ, ‡¶è ‡¶°‡¶ü ‡¶¨‡¶ø ‡¶°‡¶ü ‡¶™‡ßç‡¶∞‡¶°‡¶æ‡¶ï‡ßç‡¶ü ‡¶¨‡¶ø‡¶®‡¶ø‡¶Æ‡¶Ø‡¶º ‡¶∏‡ßÇ‡¶§‡ßç‡¶∞ (‡¶è ‡¶°‡¶ü ‡¶¨‡¶ø) ‡¶è‡¶¨‡¶Ç ‡¶∏‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶¨‡¶®‡ßç‡¶ü‡¶® ‡¶∏‡ßÇ‡¶§‡ßç‡¶∞ (‡¶¨‡¶ø) ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ï‡ßç‡¶Ø‡¶æ‡¶≤‡¶ï‡ßÅ‡¶≤‡ßá‡¶ü‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶¨‡ßá‡•§\\n\\n'\n",
            "Row 17: '[95:00 - 99:56]\\n‡¶è‡¶á ‡¶Ö‡¶Ç‡¶∂‡ßá ‡¶¨‡¶ï‡ßç‡¶§‡¶æ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡¶õ‡ßá‡¶® ‡¶ï‡¶ø‡¶≠‡¶æ‡¶¨‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶Æ‡¶§‡¶≤‡ßá‡¶∞ ‡¶≤‡¶Æ‡ßç‡¶¨ ‡¶¨‡¶∞‡¶æ‡¶¨‡¶∞ ‡¶è‡¶ï‡¶ï ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶®‡¶ø‡¶∞‡ßç‡¶£‡¶Ø‡¶º ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶Ø‡¶º‡•§ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá, ‡¶è ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶≤‡ßá, ‡¶è‡¶ï‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§ ‡¶è‡¶∞‡¶™‡¶∞, ‡¶§‡¶ø‡¶®‡¶ø ‡¶¨‡¶≤‡ßá‡¶® ‡¶Ø‡ßá, ‡¶Ø‡¶¶‡¶ø ‡¶è ‡¶è‡¶¨‡¶Ç ‡¶è ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶è‡¶ï‡¶á ‡¶§‡¶≤‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá, ‡¶§‡¶¨‡ßá ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶¨‡ßá‡•§ ‡¶§‡¶ø‡¶®‡¶ø ‡¶¨‡¶≤‡ßá‡¶®, ‡¶Ø‡¶¶‡¶ø ‡¶¶‡ßÅ‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶•‡¶æ‡¶ï‡ßá, ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶≤‡¶Æ‡ßç‡¶¨‡ßá‡¶∞ ‡¶¶‡¶ø‡¶ï ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø, ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá ‡¶è ‡¶ì ‡¶¨‡¶ø ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶ó‡ßÅ‡¶£ ‡¶ï‡¶∞‡¶≤‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶è‡¶ï‡¶ï ‡¶≠‡¶ø‡¶ï‡ßç‡¶ü‡¶∞ ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡•§ ‡¶è‡¶∞‡¶™‡¶∞ ‡¶§‡¶ø‡¶®‡¶ø ‡¶Ü‡¶∞‡¶ì ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡ßá‡¶®, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶§‡¶≤‡ßá‡¶∞ ‡¶≤‡¶Æ‡ßç‡¶¨ ‡¶è‡¶¨‡¶Ç ‡¶≠‡ßá‡¶§‡¶∞‡ßá‡¶∞ ‡¶¶‡¶ø‡¶ï‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá‡•§\\n\\n'\n",
            "Row 18: '[99:50 - 107:20]\\n‡¶è‡¶á ‡¶Ö‡¶Ç‡¶∂‡ßá, ‡¶¶‡ßÅ‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶≤‡¶Æ‡ßç‡¶¨ ‡¶è‡¶ï‡¶ï ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá, ‡¶è ‡¶™‡ßç‡¶≤‡¶æ‡¶∏ ‡¶¨‡¶ø ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶§‡ßá ‡¶¨‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶è ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶ø ‡¶è‡¶ï‡¶á ‡¶∏‡¶æ‡¶•‡ßá ‡¶≤‡¶Æ‡ßç‡¶¨‡•§ ‡¶è‡¶∞‡¶™‡¶∞, ‡¶¶‡ßÅ‡¶á‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶∏‡¶Æ‡¶æ‡¶®‡ßç‡¶§‡¶∞‡¶æ‡¶≤ ‡¶π‡¶≤‡ßá ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ï‡ßç‡¶∞‡¶∏ ‡¶ó‡ßÅ‡¶£‡¶´‡¶≤ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶¨‡ßá, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶∂‡ßÇ‡¶®‡ßç‡¶Ø ‡¶π‡¶¨‡ßá‡•§ ‡¶è‡¶á ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ ‡¶¶‡ßç‡¶¨‡¶ø‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶ø‡¶ï ‡¶™‡¶¶‡ßç‡¶ß‡¶§‡¶ø‡¶§‡ßá ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º, ‡¶Ø‡ßá‡¶Æ‡¶® ‡¶¨‡¶ø ‡¶è‡¶ï‡ßç‡¶∏, ‡¶¨‡¶ø ‡¶ì‡¶Ø‡¶º‡¶æ‡¶á, ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶ø ‡¶ì‡¶Ø‡¶º‡¶æ‡¶á ‡¶è‡¶ï‡ßç‡¶∏‡•§ ‡¶è‡¶á ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶∏‡¶¶‡ßÉ‡¶∂‡¶ï‡ßã‡¶£‡ßÄ ‡¶§‡ßç‡¶∞‡¶ø‡¶≠‡ßÅ‡¶ú ‡¶§‡ßà‡¶∞‡¶ø ‡¶π‡¶¨‡ßá, ‡¶Ø‡¶æ ‡¶∏‡¶Æ‡¶æ‡¶®‡ßç‡¶§‡¶∞‡¶æ‡¶≤ ‡¶π‡¶¨‡ßá‡•§\\n\\n'\n",
            "Row 19: '[107:15 - 112:49]\\n‡¶è‡¶á ‡¶Ö‡¶Ç‡¶∂‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶§‡ßç‡¶∞‡¶ø‡¶≠‡ßÅ‡¶ú‡ßá‡¶∞ ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡¶´‡¶≤ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá, ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶æ‡¶Æ‡¶æ‡¶®‡ßç‡¶§‡¶∞‡¶ø‡¶ï‡ßá‡¶∞ ‡¶¶‡ßÅ‡¶ü‡¶ø ‡¶∏‡¶®‡ßç‡¶®‡¶ø‡¶π‡¶ø‡¶§ ‡¶¨‡¶æ‡¶π‡ßÅ ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡ßá‡¶∂ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶¶‡ßÅ‡¶ü‡¶ø ‡¶¨‡¶æ‡¶π‡ßÅ‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶ï‡ßã‡¶£ ‡¶•‡¶ø‡¶ü‡¶æ ‡¶•‡¶æ‡¶ï‡ßá‡•§ ‡¶è‡¶∞‡¶™‡¶∞, ‡¶è‡¶ï‡¶ü‡¶ø ‡¶Ü‡¶Ø‡¶º‡¶§‡¶æ‡¶ï‡¶æ‡¶∞ ‡¶ò‡¶®‡¶¨‡¶∏‡ßç‡¶§‡ßÅ‡¶∞ ‡¶Ü‡¶Ø‡¶º‡¶§‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø, ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá ‡¶è ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§ ‡¶§‡¶æ‡¶∞‡¶™‡¶∞, ‡¶§‡ßç‡¶∞‡¶ø‡¶≠‡ßÅ‡¶ú‡ßá‡¶∞ ‡¶∂‡ßÄ‡¶∞‡ßç‡¶∑ ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶æ‡¶π‡ßÅ‡¶∞ ‡¶¶‡ßà‡¶∞‡ßç‡¶ò‡ßç‡¶Ø ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶™‡¶∞, ‡¶§‡ßç‡¶∞‡¶ø‡¶≠‡ßÅ‡¶ú‡¶ü‡¶ø‡¶∞ ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡¶´‡¶≤ ‡¶®‡¶ø‡¶∞‡ßç‡¶£‡¶Ø‡¶º‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø, ‡¶è ‡¶•‡ßá‡¶ï‡ßá ‡¶¨‡¶ø ‡¶è‡¶∞ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶® ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡¶´‡¶≤ ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º ‡¶è‡¶¨‡¶Ç ‡¶è‡¶∞ ‡¶Ü‡¶Ø‡¶º‡¶§‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶Ø‡¶º‡•§\\n\\n'\n",
            "Row 20: '[112:42 - 117:45]\\n‡¶è‡¶á ‡¶Ö‡¶Ç‡¶∂‡ßá, ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶æ‡¶Æ‡¶æ‡¶®‡ßç‡¶§‡¶∞‡¶ø‡¶ï ‡¶Ü‡¶ï‡ßÉ‡¶§‡¶ø‡¶∞ ‡¶ò‡¶®‡¶¨‡¶∏‡ßç‡¶§‡ßÅ‡¶∞ ‡¶§‡¶ø‡¶®‡¶ü‡¶ø ‡¶¨‡¶æ‡¶π‡ßÅ (‡¶è, ‡¶¨‡¶ø, ‡¶∏‡¶ø, ‡¶°‡¶ø) ‡¶è‡¶¨‡¶Ç ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Ü‡¶Ø‡¶º‡¶§‡¶® ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶Ü‡¶≤‡ßã‡¶ö‡¶®‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá, ‡¶§‡¶ø‡¶®‡¶ü‡¶ø ‡¶¨‡¶æ‡¶π‡ßÅ‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶°‡¶ü ‡¶ï‡¶∞‡ßá ‡¶Ü‡¶Ø‡¶º‡¶§‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶è ‡¶™‡ßç‡¶≤‡¶æ‡¶∏ ‡¶¨‡¶ø ‡¶è‡¶¨‡¶Ç ‡¶∏‡¶ø ‡¶è‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶°‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§ ‡¶è‡¶∞‡¶™‡¶∞, ‡¶§‡¶ø‡¶®‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶°‡¶ü ‡¶ï‡¶∞‡¶≤‡ßá, ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶â‡¶ö‡ßç‡¶ö‡¶§‡¶æ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º, ‡¶Ø‡¶æ ‡¶∏‡¶Æ‡¶æ‡¶®‡ßç‡¶§‡¶∞‡¶ø‡¶ï ‡¶Ü‡¶ï‡¶æ‡¶∞‡ßá‡¶∞ ‡¶ò‡¶®‡¶¨‡¶∏‡ßç‡¶§‡ßÅ‡¶ï‡ßá ‡¶è‡¶ï ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ‡¶§‡ßá ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶Ü‡¶∏‡¶§‡ßá ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø ‡¶ï‡¶∞‡ßá‡•§ ‡¶è‡¶á ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ ‡¶Ö‡¶®‡ßÅ‡¶∏‡¶∞‡¶£ ‡¶ï‡¶∞‡ßá, ‡¶§‡¶ø‡¶®‡¶ü‡¶ø ‡¶è‡¶ï‡¶ï ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ï‡ßá ‡¶è‡¶ï‡¶á ‡¶∏‡¶Æ‡¶§‡¶≤‡ßá ‡¶∏‡ßç‡¶•‡¶æ‡¶™‡¶® ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º ‡¶è‡¶¨‡¶Ç ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶â‡¶ö‡ßç‡¶ö‡¶§‡¶æ ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£ ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡•§\\n\\n'\n",
            "Row 21: '[117:38 - 124:30]\\n‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶§‡¶ø‡¶®‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶∏‡¶Æ‡¶§‡¶≤‡ßÄ ‡¶π‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶∂‡¶∞‡ßç‡¶§ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá, ‡¶§‡¶ø‡¶®‡¶ü‡¶ø ‡¶∏‡¶Æ‡¶æ‡¶®‡ßç‡¶§‡¶∞‡¶ø‡¶ï ‡¶Ü‡¶ï‡ßÉ‡¶§‡¶ø‡¶∞ ‡¶ò‡¶®‡¶¨‡¶∏‡ßç‡¶§‡ßÅ‡¶∞ ‡¶Ü‡¶Ø‡¶º‡¶§‡¶® ‡¶®‡¶ø‡¶∞‡ßç‡¶£‡¶Ø‡¶º ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶è‡¶ï‡¶á ‡¶∏‡¶Æ‡¶§‡¶≤‡ßá ‡¶•‡¶æ‡¶ï‡ßá‡•§ ‡¶è‡¶∞‡¶™‡¶∞, ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶∞‡¶≤‡¶∞‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø, ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá ‡¶ï‡ßç‡¶∞‡¶∏ ‡¶ó‡ßÅ‡¶£ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º ‡¶è‡¶¨‡¶Ç ‡¶™‡¶∞‡ßá ‡¶°‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§ ‡¶è‡¶á ‡¶Æ‡¶æ‡¶®‡¶ó‡ßÅ‡¶≤‡ßã ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá, ‡¶§‡ßç‡¶∞‡¶ø‡¶≠‡ßÅ‡¶ú‡ßá‡¶∞ ‡¶§‡¶ø‡¶®‡¶ü‡¶ø ‡¶¨‡¶æ‡¶π‡ßÅ ‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ ‡¶•‡¶æ‡¶ï‡ßá, ‡¶Ø‡¶æ‡¶∞ ‡¶Ü‡¶Ø‡¶º‡¶§‡¶® ‡¶∂‡ßÇ‡¶®‡ßç‡¶Ø ‡¶π‡¶¨‡ßá ‡¶®‡¶æ‡•§ ‡¶§‡ßç‡¶∞‡¶ø‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶ø‡¶ï ‡¶∏‡ßç‡¶•‡¶æ‡¶®‡ßá ‡¶∏‡¶∞‡¶≤‡¶∞‡ßá‡¶ñ‡¶æ ‡¶•‡¶æ‡¶ï‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá ‡¶®‡¶æ, ‡¶ï‡¶æ‡¶∞‡¶£ ‡¶è‡¶ü‡¶ø ‡¶∞‡¶ø‡¶Ø‡¶º‡ßá‡¶≤ ‡¶≤‡¶æ‡¶á‡¶´‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶™‡ßç‡¶∞‡¶Ø‡ßã‡¶ú‡ßç‡¶Ø ‡¶®‡¶Ø‡¶º‡•§\\n\\n'\n",
            "Row 22: '[124:21 - 130:30]\\n‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶§‡ßç‡¶∞‡¶ø‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶ø‡¶ï ‡¶∏‡¶∞‡¶≤‡¶∞‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶è‡¶¨‡¶Ç ‡¶§‡¶æ‡¶∞ ‡¶â‡¶™‡¶∞‡¶∏‡ßç‡¶• ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ‡¶∞ ‡¶∏‡ßç‡¶•‡¶æ‡¶®‡¶æ‡¶ô‡ßç‡¶ï ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá, ‡¶è‡¶ï‡¶ü‡¶ø ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ ‡¶è‡¶¨‡¶Ç ‡¶§‡¶æ‡¶∞ ‡¶∏‡ßç‡¶•‡¶æ‡¶®‡¶æ‡¶ô‡ßç‡¶ï ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶Ü‡¶≤‡ßã‡¶ö‡¶®‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶è‡¶ï‡ßç‡¶∏ ‡¶è‡¶¨‡¶Ç ‡¶ì ‡¶è‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï ‡¶∞‡¶Ø‡¶º‡ßá‡¶õ‡ßá, ‡¶Ø‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶§‡ßç‡¶∞‡¶ø‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶ø‡¶ï ‡¶∏‡ßç‡¶•‡¶æ‡¶®‡¶æ‡¶ô‡ßç‡¶ï ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶§‡ßá ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø ‡¶ï‡¶∞‡ßá‡•§ ‡¶è‡¶∞‡¶™‡¶∞, ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶∞‡¶≤ ‡¶∞‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶â‡¶™‡¶∞‡¶∏‡ßç‡¶• ‡¶∏‡¶Æ‡¶∏‡ßç‡¶§ ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ‡¶∞ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶∞‡¶≤‡¶∞‡ßá‡¶ñ‡¶æ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶¨‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ö‡¶≤‡¶ï ‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§ ‡¶è‡¶á ‡¶∏‡¶Æ‡ßÄ‡¶ï‡¶∞‡¶£‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá, ‡¶è ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ‡¶∞ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶®‡ßá‡¶∞ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º ‡¶è‡¶¨‡¶Ç ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡•§\\n\\n'\n",
            "Row 23: '[130:07 - 135:50]\\n‡¶è‡¶á ‡¶Ö‡¶Ç‡¶∂‡ßá, ‡¶è‡¶ï‡¶ü‡¶ø ‡¶§‡ßç‡¶∞‡¶ø‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶ø‡¶ï ‡¶∏‡¶∞‡¶≤‡¶∞‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶è‡¶¨‡¶Ç ‡¶ï‡¶æ‡¶∞‡ßç‡¶§‡ßá‡¶∏‡ßÄ‡¶Ø‡¶º ‡¶∏‡¶Æ‡ßÄ‡¶ï‡¶∞‡¶£‡ßá‡¶∞ ‡¶Ü‡¶≤‡ßã‡¶ö‡¶®‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá, ‡¶è ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ ‡¶ó‡¶æ‡¶Æ‡ßÄ ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶æ‡¶®‡ßç‡¶§‡¶∞‡¶æ‡¶≤ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶®‡¶ø‡¶∞‡ßç‡¶£‡¶Ø‡¶º ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ‡¶∞ ‡¶§‡ßç‡¶∞‡¶ø‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶ø‡¶ï ‡¶∏‡ßç‡¶•‡¶æ‡¶®‡¶æ‡¶ô‡ßç‡¶ï ‡¶¨‡¶æ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶® ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§ ‡¶è‡¶∞‡¶™‡¶∞, ‡¶è, ‡¶¨‡¶ø, ‡¶è‡¶¨‡¶Ç ‡¶ì‡¶Ø‡¶º‡¶æ‡¶á ‡¶è‡¶∞ ‡¶§‡ßç‡¶∞‡¶ø‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶ø‡¶ï ‡¶∏‡ßç‡¶•‡¶æ‡¶®‡¶æ‡¶Ç‡¶ï ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø, ‡¶Ü‡¶á ‡¶ï‡ßç‡¶Ø‡¶æ‡¶™ ‡¶è‡¶¨‡¶Ç ‡¶ü‡¶ø ‡¶è‡¶∞ ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶® ‡¶ï‡¶∞‡ßá, ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡•§ ‡¶è‡¶á ‡¶∏‡¶Æ‡ßÄ‡¶ï‡¶∞‡¶£‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá, ‡¶è ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶ø ‡¶è‡¶∞ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶¨‡ßá‡•§\\n\\n'\n",
            "Row 24: '[135:45 - 143:08]\\n‡¶è‡¶á ‡¶Ö‡¶Ç‡¶∂‡ßá, ‡¶¶‡ßÅ‡¶ü‡¶ø ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ‡¶∞ ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶Ü‡¶ï‡ßÉ‡¶§‡¶ø ‡¶è‡¶¨‡¶Ç ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶∏‡ßç‡¶•‡¶æ‡¶®‡¶æ‡¶ô‡ßç‡¶ï ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá, ‡¶¶‡ßÅ‡¶á‡¶ü‡¶ø ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ‡¶∞ ‡¶∏‡ßç‡¶•‡¶æ‡¶®‡¶æ‡¶ô‡ßç‡¶ï (‡¶è‡¶ï‡ßç‡¶∏, ‡¶ì‡¶Ø‡¶º‡¶æ‡¶á, ‡¶ú‡ßá‡¶°) ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶è‡¶ï‡ßç‡¶∏, ‡¶ì‡¶Ø‡¶º‡¶æ‡¶á ‡¶è‡¶¨‡¶Ç ‡¶ú‡ßá‡¶° ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶∏‡¶Æ‡¶æ‡¶® ‡¶π‡¶Ø‡¶º‡•§ ‡¶è‡¶∞‡¶™‡¶∞, ‡¶∏‡¶∞‡¶≤‡¶∞‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶â‡¶™‡¶∞‡¶∏‡ßç‡¶• ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶∏‡ßç‡¶•‡¶æ‡¶®‡¶æ‡¶ô‡ßç‡¶ï ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§ ‡¶è‡¶á ‡¶∏‡¶Æ‡ßÄ‡¶ï‡¶∞‡¶£‡¶ü‡¶ø ‡¶¶‡ßç‡¶¨‡¶ø‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶ø‡¶ï ‡¶∏‡¶∞‡¶≤‡¶∞‡ßá‡¶ñ‡¶æ ‡¶è‡¶¨‡¶Ç ‡¶è‡¶∞ ‡¶∏‡¶Æ‡¶æ‡¶®‡ßç‡¶§‡¶∞‡¶æ‡¶≤ ‡¶∞‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶™‡ßç‡¶∞‡¶Ø‡ßã‡¶ú‡ßç‡¶Ø‡•§ ‡¶è‡¶á ‡¶∏‡¶∞‡¶≤‡¶∞‡ßá‡¶ñ‡¶æ‡¶ü‡¶ø ‡¶∏‡¶∞‡¶≤‡¶∞‡ßá‡¶ñ‡¶æ ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶ö‡¶ø‡¶π‡ßç‡¶®‡¶ø‡¶§ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º ‡¶è‡¶¨‡¶Ç ‡¶è‡¶∞ ‡¶â‡¶™‡¶∞‡¶∏‡ßç‡¶• ‡¶Ø‡ßá‡¶ï‡ßã‡¶®‡ßã ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ‡¶∞ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶® ‡¶®‡¶ø‡¶∞‡ßç‡¶£‡¶Ø‡¶º ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡•§\\n\\n'\n",
            "Row 25: '[143:02 - 148:49]\\n‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶∞‡¶≤‡¶∞‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶á‡¶ï‡ßÅ‡¶Ø‡¶º‡ßá‡¶∂‡¶® ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ‡¶∞ ‡¶∏‡ßç‡¶•‡¶æ‡¶®‡¶æ‡¶ô‡ßç‡¶ï ‡¶¨‡¶æ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶® ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá, ‡¶¶‡ßÅ‡¶ü‡¶ø ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ ‡¶™‡¶ø ‡¶è‡¶¨‡¶Ç ‡¶ï‡¶ø‡¶â ‡¶è‡¶∞ ‡¶∏‡ßç‡¶•‡¶æ‡¶®‡¶æ‡¶ô‡ßç‡¶ï ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶ü‡¶ø ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶è‡¶ï ‡¶π‡¶≤‡ßá ‡¶™‡¶ø ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ‡¶∞ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶® ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶π‡¶¨‡ßá‡•§ ‡¶è‡¶∞‡¶™‡¶∞, ‡¶§‡¶ø‡¶®‡¶ü‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶≤‡¶¨‡ßç‡¶ß‡¶ø ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø, ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶≤‡¶¨‡ßç‡¶ß‡¶ø‡¶∞ ‡¶¨‡¶ø‡¶™‡¶∞‡ßÄ‡¶§ ‡¶¨‡¶æ‡¶π‡ßÅ‡¶∞ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶® ‡¶®‡¶ø‡¶∞‡ßç‡¶£‡¶Ø‡¶º ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡•§ ‡¶è‡¶á ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ó‡ßÅ‡¶≤‡ßã ‡¶è‡¶ï‡¶∏‡¶æ‡¶•‡ßá ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶≤‡ßá ‡¶™‡ßÉ‡¶•‡¶ø‡¶¨‡ßÄ‡¶∞ ‡¶¶‡ßà‡¶∞‡ßç‡¶ò‡ßç‡¶Ø ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º, ‡¶Ø‡ßá‡¶Æ‡¶® ‡¶™‡¶ø + ‡¶ï‡¶ø‡¶â + ‡¶ï‡¶∑‡ßç‡¶ü ‡¶ú‡¶ø‡¶∞‡ßã‡•§\\n\\n'\n",
            "Row 26: '[148:42 - 156:45]\\n‡¶è‡¶á ‡¶Ö‡¶Ç‡¶∂‡ßá, ‡¶è‡¶¨‡¶ø‡¶∏‡¶ø ‡¶§‡ßç‡¶∞‡¶ø‡¶≠‡ßÅ‡¶ú‡ßá‡¶∞ ‡¶¨‡¶ø‡¶∏‡¶ø ‡¶¨‡¶æ‡¶π‡ßÅ‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ‡¶∞ ‡¶≤‡¶¨‡ßç‡¶ß‡¶ø ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶Ü‡¶≤‡ßã‡¶ö‡¶®‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá, ‡¶è‡¶¨‡¶ø ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶ø‡¶∏‡¶ø ‡¶§‡ßç‡¶∞‡¶ø‡¶≠‡ßÅ‡¶ú‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ ‡¶°‡¶ø ‡¶π‡¶≤‡ßá, ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶≤‡¶¨‡ßç‡¶ß‡¶ø‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡¶Æ‡¶æ ‡¶¶‡ßç‡¶¨‡¶ø‡¶ó‡ßÅ‡¶£ ‡¶π‡¶¨‡ßá‡•§ ‡¶è‡¶∞‡¶™‡¶∞, ‡¶¨‡¶ø‡¶∏‡¶ø ‡¶è‡¶¨‡¶Ç ‡¶è‡¶°‡¶ø ‡¶¨‡¶æ‡¶π‡ßÅ‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡¶¨‡¶∞‡ßç‡¶§‡ßÄ ‡¶Æ‡¶ß‡ßç‡¶Ø‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ (‡¶°‡¶ø) ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßá ‡¶è‡¶¨‡¶ø ‡¶™‡ßç‡¶≤‡¶æ‡¶∏ ‡¶¨‡¶ø ‡¶è‡¶¨‡¶Ç ‡¶è‡¶∏‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡¶ï‡ßá ‡¶è ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶ø ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá ‡¶™‡ßç‡¶∞‡¶ï‡¶æ‡¶∂ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶è‡¶¨‡¶ø + ‡¶¨‡¶ø ‡¶°‡¶ø = ‡¶è‡¶∏‡¶ø ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞, ‡¶è‡¶¨‡¶Ç ‡¶è‡¶¨‡¶ø = ‡¶è + ‡¶¨‡¶ø‡•§\\n\\n'\n",
            "Row 27: '[156:00 - 162:57]\\n‡¶è‡¶á ‡¶Ö‡¶Ç‡¶∂‡ßá, ‡¶§‡ßç‡¶∞‡¶ø‡¶≠‡ßÅ‡¶ú‡ßá‡¶∞ ‡¶ì ‡¶Ü‡¶∞ ‡¶è‡¶® ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶è‡¶¨‡¶Ç ‡¶§‡ßç‡¶∞‡¶ø‡¶≠‡ßÅ‡¶ú‡ßá‡¶∞ ‡¶∏‡¶Ç‡¶Ø‡ßã‡¶ú‡¶ï ‡¶∏‡¶∞‡¶≤‡¶∞‡ßá‡¶ñ‡¶æ ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶Ü‡¶≤‡ßã‡¶ö‡¶®‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá, ‡¶™‡¶ø ‡¶ï‡¶ø‡¶â ‡¶™‡ßç‡¶≤‡¶æ‡¶∏ ‡¶™‡¶ø ‡¶Ü‡¶∞ ‡¶è‡¶¨‡¶Ç ‡¶™‡¶ø ‡¶è‡¶≤ ‡¶∏‡¶Æ‡¶æ‡¶® ‡¶™‡¶ø ‡¶è‡¶≤ ‡¶è‡¶¨‡¶Ç ‡¶™‡¶ø ‡¶Ü‡¶∞ ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶™‡¶ø ‡¶ï‡¶ø‡¶â + ‡¶™‡¶ø ‡¶Ü‡¶∞ = ‡¶™‡¶ø ‡¶è‡¶≤ + ‡¶™‡¶ø ‡¶è‡¶≤‡•§ ‡¶è‡¶∞‡¶™‡¶∞, ‡¶™‡¶ø‡¶è‡¶≤ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶ì ‡¶Ü‡¶∞‡¶è‡¶® ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø, ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá ‡¶§‡ßç‡¶∞‡¶ø‡¶≠‡ßÅ‡¶ú‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ‡¶∞ ‡¶Ö‡¶∞‡ßç‡¶ß‡ßá‡¶ï ‡¶è‡¶¨‡¶Ç ‡¶∏‡¶Æ‡¶æ‡¶®‡ßç‡¶§‡¶∞‡¶æ‡¶≤ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶Ø‡¶æ ‡¶§‡ßç‡¶∞‡¶ø‡¶≠‡ßÅ‡¶ú‡ßá‡¶∞ ‡¶§‡ßÉ‡¶§‡ßÄ‡¶Ø‡¶º ‡¶¨‡¶æ‡¶π‡ßÅ‡¶∞ ‡¶∏‡¶Æ‡¶æ‡¶®‡ßç‡¶§‡¶∞‡¶æ‡¶≤ ‡¶ì ‡¶¶‡ßà‡¶∞‡ßç‡¶ò‡ßç‡¶Ø‡ßá‡¶∞ ‡¶§‡ßç‡¶∞‡¶ø‡¶≠‡ßÅ‡¶ú‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶™‡ßç‡¶∞‡¶Ø‡ßã‡¶ú‡ßç‡¶Ø‡•§\\n\\n'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:37:16] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:37:17] \"GET /api/status/1756468506036 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:37:23] \"GET /api/download/1756468506036/final_text HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:37:52] \"GET /api/download/1756468506036/summary_excel HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:39:57] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [29/Aug/2025 12:40:03] \"GET / HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "# @title Default title text\n",
        "# Google Colab Flask Video Processor Setup\n",
        "# Run each cell in sequence\n",
        "\n",
        "# ===== CELL 1: Install Dependencies =====\n",
        "# ===== CELL 1: Installation with specific versions =====\n",
        "!pip install flask flask-cors pyngrok\n",
        "!pip install transformers openpyxl pandas\n",
        "!pip install git+https://github.com/csebuetnlp/normalizer\n",
        "!pip install banglanlptoolkit\n",
        "!pip install moviepy\n",
        "!pip install SpeechRecognition==3.10.0\n",
        "!pip install pydub==0.25.1\n",
        "\n",
        "!apt-get update\n",
        "!apt-get install -y ffmpeg\n",
        "\n",
        "# ===== CELL 2: Setup Ngrok (for public URL) =====\n",
        "from pyngrok import ngrok\n",
        "import getpass\n",
        "\n",
        "# Set your ngrok auth token\n",
        "ngrok_token = getpass.getpass(\"Enter your ngrok auth token: \")\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "\n",
        "# ===== CELL 3: Fixed Flask Application =====\n",
        "from flask import Flask, request, jsonify, send_file, render_template_string\n",
        "from flask_cors import CORS\n",
        "import os\n",
        "import io\n",
        "import json\n",
        "import tempfile\n",
        "import threading\n",
        "import time\n",
        "from datetime import datetime\n",
        "import traceback\n",
        "import zipfile\n",
        "import re\n",
        "import shutil\n",
        "\n",
        "# Import libraries\n",
        "import speech_recognition as sr\n",
        "from tqdm.auto import tqdm\n",
        "from pydub import AudioSegment\n",
        "from pydub.utils import make_chunks\n",
        "from moviepy.editor import VideoFileClip\n",
        "import pandas as pd\n",
        "import torch\n",
        "import re\n",
        "import gc\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from moviepy.editor import VideoFileClip\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "# Try to import BNLP toolkit (optional)\n",
        "try:\n",
        "    from bnlp import BasicTokenizer\n",
        "    try:\n",
        "        from bnlp.stemmer import BanglaStemmer\n",
        "    except ImportError:\n",
        "        BanglaStemmer = None\n",
        "    TOKENIZER = BasicTokenizer()\n",
        "    STEMMER = BanglaStemmer() if BanglaStemmer else None\n",
        "except ImportError:\n",
        "    TOKENIZER = None\n",
        "    STEMMER = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Enable tqdm for pandas\n",
        "tqdm.pandas()\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def split_into_sentences(text):\n",
        "    \"\"\"Split Bangla text into sentences\"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return []\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    # Split by Bangla sentence endings\n",
        "    sentences = re.split(r'[‡•§?!]+', text)\n",
        "\n",
        "    # Clean and filter sentences\n",
        "    cleaned_sentences = []\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.strip()\n",
        "        if sentence and len(sentence) > 3:  # Minimum sentence length\n",
        "            cleaned_sentences.append(sentence)\n",
        "\n",
        "    return cleaned_sentences\n",
        "\n",
        "class EfficientCombinedTFIDF:\n",
        "    def __init__(self):\n",
        "        self.document_unigram_counts = []  # List of Counter objects for unigrams\n",
        "        self.document_bigram_counts = []   # List of Counter objects for bigrams\n",
        "        self.document_names = []\n",
        "        self.global_unigram_vocab = set()\n",
        "        self.global_bigram_vocab = set()\n",
        "        self.unigram_document_frequencies = {}\n",
        "        self.bigram_document_frequencies = {}\n",
        "        self.total_documents = 0\n",
        "        self.original_forms = {}  # Mapping of normalized -> original forms\n",
        "\n",
        "    def add_document(self, doc_name, normalized_unigrams_list, normalized_bigrams_list, original_forms_mapping):\n",
        "        \"\"\"\n",
        "        Add a document's unigrams and bigrams (already processed and normalized)\n",
        "        \"\"\"\n",
        "        # Count unigrams and bigrams in this document\n",
        "        unigram_counts = Counter(normalized_unigrams_list)\n",
        "        bigram_counts = Counter(normalized_bigrams_list)\n",
        "\n",
        "        self.document_unigram_counts.append(unigram_counts)\n",
        "        self.document_bigram_counts.append(bigram_counts)\n",
        "        self.document_names.append(doc_name)\n",
        "\n",
        "        # Update global vocabularies\n",
        "        unique_unigrams_in_doc = set(unigram_counts.keys())\n",
        "        unique_bigrams_in_doc = set(bigram_counts.keys())\n",
        "\n",
        "        self.global_unigram_vocab.update(unique_unigrams_in_doc)\n",
        "        self.global_bigram_vocab.update(unique_bigrams_in_doc)\n",
        "\n",
        "        # Update document frequencies\n",
        "        for unigram in unique_unigrams_in_doc:\n",
        "            self.unigram_document_frequencies[unigram] = self.unigram_document_frequencies.get(unigram, 0) + 1\n",
        "\n",
        "        for bigram in unique_bigrams_in_doc:\n",
        "            self.bigram_document_frequencies[bigram] = self.bigram_document_frequencies.get(bigram, 0) + 1\n",
        "\n",
        "        self.total_documents += 1\n",
        "\n",
        "        # Store original forms mapping\n",
        "        self.original_forms.update(original_forms_mapping)\n",
        "\n",
        "        print(f\"Added document: {doc_name} with {len(normalized_unigrams_list)} unigrams, {len(normalized_bigrams_list)} bigrams\")\n",
        "\n",
        "    def calculate_pmi(self, bigram, doc_index):\n",
        "        \"\"\"\n",
        "        Calculate Pointwise Mutual Information for a bigram\n",
        "        \"\"\"\n",
        "        if ' ' not in bigram:\n",
        "            return 0  # Not a bigram\n",
        "\n",
        "        word1, word2 = bigram.split(' ', 1)\n",
        "\n",
        "        # Get counts from current document\n",
        "        unigram_counts = self.document_unigram_counts[doc_index]\n",
        "        bigram_counts = self.document_bigram_counts[doc_index]\n",
        "\n",
        "        # Calculate total tokens in document\n",
        "        total_unigrams = sum(unigram_counts.values())\n",
        "\n",
        "        # Get frequencies\n",
        "        bigram_freq = bigram_counts.get(bigram, 0)\n",
        "        word1_freq = unigram_counts.get(word1, 0)\n",
        "        word2_freq = unigram_counts.get(word2, 0)\n",
        "\n",
        "        if bigram_freq == 0 or word1_freq == 0 or word2_freq == 0:\n",
        "            return 0\n",
        "\n",
        "        # Calculate probabilities\n",
        "        p_bigram = bigram_freq / total_unigrams\n",
        "        p_word1 = word1_freq / total_unigrams\n",
        "        p_word2 = word2_freq / total_unigrams\n",
        "\n",
        "        # Calculate PMI\n",
        "        if p_word1 * p_word2 > 0:\n",
        "            pmi = math.log(p_bigram / (p_word1 * p_word2))\n",
        "            return pmi\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def decide_unigram_vs_bigram(self, bigram, doc_index, min_pmi=1.0, min_freq=3):\n",
        "        \"\"\"\n",
        "        Decide whether to keep unigrams or bigram based on PMI and frequency\n",
        "        Returns: ('keep_bigram', 'keep_unigrams', 'keep_both') and decision reason\n",
        "        \"\"\"\n",
        "        if ' ' not in bigram:\n",
        "            return 'keep_unigrams', 'Not a valid bigram'\n",
        "\n",
        "        words = bigram.split(' ', 1)\n",
        "        if len(words) != 2:\n",
        "            return 'keep_unigrams', 'Invalid bigram format'\n",
        "\n",
        "        word1, word2 = words\n",
        "\n",
        "        # Get counts\n",
        "        bigram_counts = self.document_bigram_counts[doc_index]\n",
        "        unigram_counts = self.document_unigram_counts[doc_index]\n",
        "\n",
        "        bigram_freq = bigram_counts.get(bigram, 0)\n",
        "        word1_freq = unigram_counts.get(word1, 0)\n",
        "        word2_freq = unigram_counts.get(word2, 0)\n",
        "\n",
        "        # Check minimum frequency threshold\n",
        "        if bigram_freq < min_freq:\n",
        "            return 'keep_unigrams', f'Bigram too rare (freq={bigram_freq})'\n",
        "\n",
        "        # Calculate PMI\n",
        "        pmi = self.calculate_pmi(bigram, doc_index)\n",
        "\n",
        "        # Decision logic\n",
        "        if pmi > min_pmi:\n",
        "            # Strong association - keep bigram, remove constituent unigrams\n",
        "            return 'keep_bigram', f'Strong association (PMI={pmi:.2f})'\n",
        "        elif pmi > 0.5:\n",
        "            # Moderate association - keep both\n",
        "            return 'keep_both', f'Moderate association (PMI={pmi:.2f})'\n",
        "        else:\n",
        "            # Weak association - keep individual unigrams\n",
        "            return 'keep_unigrams', f'Weak association (PMI={pmi:.2f})'\n",
        "\n",
        "    def calculate_tfidf_with_decisions(self, doc_index, top_k=None, percentage_based=True,\n",
        "                                     base_percentage=2.0, min_keywords=5, max_keywords=50):\n",
        "        \"\"\"\n",
        "        Calculate TF-IDF scores with unigram vs bigram decisions\n",
        "        \"\"\"\n",
        "        if doc_index >= len(self.document_unigram_counts):\n",
        "            return None\n",
        "\n",
        "        unigram_counts = self.document_unigram_counts[doc_index]\n",
        "        bigram_counts = self.document_bigram_counts[doc_index]\n",
        "\n",
        "        total_unigrams = sum(unigram_counts.values())\n",
        "\n",
        "        # Calculate dynamic top_k if not provided\n",
        "        if top_k is None and percentage_based:\n",
        "            calculated_k = int(total_unigrams * (base_percentage / 100))\n",
        "            top_k = max(min_keywords, min(calculated_k, max_keywords))\n",
        "            print(f\"Document length: {total_unigrams} tokens, Dynamic keywords: {top_k}\")\n",
        "        elif top_k is None:\n",
        "            top_k = 20\n",
        "\n",
        "        # Process all terms and make decisions\n",
        "        final_keywords = []\n",
        "        eliminated_unigrams = set()  # Track which unigrams to skip\n",
        "        decision_log = []\n",
        "\n",
        "        # First pass: Process all bigrams and make decisions\n",
        "        for bigram, count in bigram_counts.items():\n",
        "            decision, reason = self.decide_unigram_vs_bigram(bigram, doc_index)\n",
        "\n",
        "            if decision == 'keep_bigram':\n",
        "                # Calculate TF-IDF for bigram\n",
        "                tf = count / total_unigrams\n",
        "                df = self.bigram_document_frequencies[bigram]\n",
        "                idf = math.log(self.total_documents / df)\n",
        "                tfidf_score = tf * idf\n",
        "\n",
        "                # Get original forms\n",
        "                bigram_words = bigram.split()\n",
        "                original_parts = []\n",
        "                has_mappings = False\n",
        "\n",
        "                for word in bigram_words:\n",
        "                    if word in self.original_forms and len(self.original_forms[word]) > 1:\n",
        "                        original_parts.append(f\"{word}‚Üê{sorted(list(self.original_forms[word]))}\")\n",
        "                        has_mappings = True\n",
        "                    else:\n",
        "                        original_parts.append(word)\n",
        "\n",
        "                original_forms = \" | \".join(original_parts) if has_mappings else bigram\n",
        "\n",
        "                final_keywords.append({\n",
        "                    'Keyword': bigram,\n",
        "                    'TF-IDF_Score': round(tfidf_score, 4),\n",
        "                    'TF': round(tf, 4),\n",
        "                    'IDF': round(idf, 4),\n",
        "                    'Count_in_Doc': count,\n",
        "                    'IDF_Count': df,\n",
        "                    'Term_Type': 'Bigram',\n",
        "                    'Decision': decision,\n",
        "                    'Decision_Reason': reason,\n",
        "                    'PMI': round(self.calculate_pmi(bigram, doc_index), 3),\n",
        "                    'Original_Forms': original_forms\n",
        "                })\n",
        "\n",
        "                # Mark constituent unigrams for elimination\n",
        "                word1, word2 = bigram.split(' ', 1)\n",
        "                eliminated_unigrams.add(word1)\n",
        "                eliminated_unigrams.add(word2)\n",
        "\n",
        "            elif decision == 'keep_both':\n",
        "                # Calculate TF-IDF for bigram\n",
        "                tf = count / total_unigrams\n",
        "                df = self.bigram_document_frequencies[bigram]\n",
        "                idf = math.log(self.total_documents / df)\n",
        "                tfidf_score = tf * idf\n",
        "\n",
        "                # Get original forms\n",
        "                bigram_words = bigram.split()\n",
        "                original_parts = []\n",
        "                has_mappings = False\n",
        "\n",
        "                for word in bigram_words:\n",
        "                    if word in self.original_forms and len(self.original_forms[word]) > 1:\n",
        "                        original_parts.append(f\"{word}‚Üê{sorted(list(self.original_forms[word]))}\")\n",
        "                        has_mappings = True\n",
        "                    else:\n",
        "                        original_parts.append(word)\n",
        "\n",
        "                original_forms = \" | \".join(original_parts) if has_mappings else bigram\n",
        "\n",
        "                final_keywords.append({\n",
        "                    'Keyword': bigram,\n",
        "                    'TF-IDF_Score': round(tfidf_score, 4),\n",
        "                    'TF': round(tf, 4),\n",
        "                    'IDF': round(idf, 4),\n",
        "                    'Count_in_Doc': count,\n",
        "                    'IDF_Count': df,\n",
        "                    'Term_Type': 'Bigram',\n",
        "                    'Decision': decision,\n",
        "                    'Decision_Reason': reason,\n",
        "                    'PMI': round(self.calculate_pmi(bigram, doc_index), 3),\n",
        "                    'Original_Forms': original_forms\n",
        "                })\n",
        "\n",
        "                # Don't eliminate unigrams in this case\n",
        "\n",
        "            decision_log.append((bigram, decision, reason))\n",
        "\n",
        "        # Second pass: Process remaining unigrams\n",
        "        for unigram, count in unigram_counts.items():\n",
        "            if unigram not in eliminated_unigrams:\n",
        "                # Calculate TF-IDF for unigram\n",
        "                tf = count / total_unigrams\n",
        "                df = self.unigram_document_frequencies[unigram]\n",
        "                idf = math.log(self.total_documents / df)\n",
        "                tfidf_score = tf * idf\n",
        "\n",
        "                # Get original forms\n",
        "                original_forms = unigram\n",
        "                if unigram in self.original_forms and len(self.original_forms[unigram]) > 1:\n",
        "                    original_forms = f\"{unigram}‚Üê{sorted(list(self.original_forms[unigram]))}\"\n",
        "\n",
        "                final_keywords.append({\n",
        "                    'Keyword': unigram,\n",
        "                    'TF-IDF_Score': round(tfidf_score, 4),\n",
        "                    'TF': round(tf, 4),\n",
        "                    'IDF': round(idf, 4),\n",
        "                    'Count_in_Doc': count,\n",
        "                    'IDF_Count': df,\n",
        "                    'Term_Type': 'Unigram',\n",
        "                    'Decision': 'keep_unigrams',\n",
        "                    'Decision_Reason': 'No competing bigram or bigram rejected',\n",
        "                    'PMI': 0.0,\n",
        "                    'Original_Forms': original_forms\n",
        "                })\n",
        "\n",
        "        # Sort by TF-IDF score\n",
        "        final_keywords.sort(key=lambda x: x['TF-IDF_Score'], reverse=True)\n",
        "\n",
        "        # Calculate relative importance\n",
        "        if final_keywords:\n",
        "            max_tfidf = final_keywords[0]['TF-IDF_Score']\n",
        "            for item in final_keywords:\n",
        "                item['Relative_Importance'] = round(item['TF-IDF_Score'] / max_tfidf, 4) if max_tfidf > 0 else 0\n",
        "\n",
        "        # Create results DataFrame\n",
        "        results_df = pd.DataFrame(final_keywords[:top_k])\n",
        "        results_df.attrs['document_name'] = self.document_names[doc_index]\n",
        "        results_df.attrs['document_index'] = doc_index\n",
        "        results_df.attrs['total_keywords_available'] = len(final_keywords)\n",
        "        results_df.attrs['document_length'] = total_unigrams\n",
        "        results_df.attrs['eliminated_unigrams'] = len(eliminated_unigrams)\n",
        "        results_df.attrs['decision_log'] = decision_log\n",
        "\n",
        "        return results_df\n",
        "\n",
        "    def analyze_all_documents(self, top_k=None, save_to_excel=True, output_path=None):\n",
        "        \"\"\"\n",
        "        Analyze keywords for all documents with unigram vs bigram decisions\n",
        "        \"\"\"\n",
        "        print(f\"Analyzing combined unigram-bigram keywords for all {self.total_documents} documents...\")\n",
        "\n",
        "        all_analyses = {}\n",
        "        excel_data = []\n",
        "        decision_stats = {'keep_bigram': 0, 'keep_unigrams': 0, 'keep_both': 0}\n",
        "\n",
        "        for doc_idx in range(self.total_documents):\n",
        "            keywords_df = self.calculate_tfidf_with_decisions(doc_idx, top_k=top_k)\n",
        "\n",
        "            if keywords_df is not None and not keywords_df.empty:\n",
        "                doc_name = keywords_df.attrs.get('document_name', f'Document_{doc_idx}')\n",
        "                all_analyses[doc_name] = keywords_df\n",
        "\n",
        "                # Collect decision statistics\n",
        "                decision_log = keywords_df.attrs.get('decision_log', [])\n",
        "                for _, decision, _ in decision_log:\n",
        "                    decision_stats[decision] = decision_stats.get(decision, 0) + 1\n",
        "\n",
        "                # Prepare data for Excel\n",
        "                for idx, row in keywords_df.iterrows():\n",
        "                    excel_row = {\n",
        "                        'Document_Name': doc_name,\n",
        "                        'Document_Index': doc_idx,\n",
        "                        'Rank': idx + 1,\n",
        "                        **row.to_dict()\n",
        "                    }\n",
        "                    excel_data.append(excel_row)\n",
        "\n",
        "            if (doc_idx + 1) % 25 == 0:\n",
        "                print(f\"Processed {doc_idx + 1}/{self.total_documents} documents...\")\n",
        "\n",
        "        print(\"‚úì Combined unigram-bigram analysis complete!\")\n",
        "\n",
        "        # Print decision statistics\n",
        "        print(f\"\\nDecision Statistics:\")\n",
        "        print(f\"  - Keep bigram (eliminate unigrams): {decision_stats.get('keep_bigram', 0)}\")\n",
        "        print(f\"  - Keep unigrams (eliminate bigram): {decision_stats.get('keep_unigrams', 0)}\")\n",
        "        print(f\"  - Keep both: {decision_stats.get('keep_both', 0)}\")\n",
        "\n",
        "        if save_to_excel and excel_data:\n",
        "            if output_path is None:\n",
        "                output_path = '/content/drive/MyDrive/tfidf/all_document_combined_keywords.xlsx'\n",
        "\n",
        "            try:\n",
        "                # Create Excel file with multiple sheets\n",
        "                with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
        "                    # Summary sheet\n",
        "                    summary_df = pd.DataFrame(excel_data)\n",
        "                    summary_df.to_excel(writer, sheet_name='All_Documents_Combined', index=False)\n",
        "\n",
        "                    # Individual document sheets (first 10 documents)\n",
        "                    for i, (doc_name, keywords_df) in enumerate(list(all_analyses.items())[:10]):\n",
        "                        sheet_name = f'Doc_{i+1}'[:31]  # Excel sheet name limit\n",
        "                        keywords_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "\n",
        "                    # Decision summary sheet\n",
        "                    decision_df = pd.DataFrame([\n",
        "                        {'Decision_Type': k, 'Count': v} for k, v in decision_stats.items()\n",
        "                    ])\n",
        "                    decision_df.to_excel(writer, sheet_name='Decision_Summary', index=False)\n",
        "\n",
        "                print(f\"Results saved to: {output_path}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving to Excel: {e}\")\n",
        "\n",
        "        return all_analyses\n",
        "\n",
        "    def get_vocabulary_stats(self):\n",
        "        \"\"\"Get statistics about the vocabulary\"\"\"\n",
        "        total_unigrams = sum(sum(doc_counts.values()) for doc_counts in self.document_unigram_counts)\n",
        "        total_bigrams = sum(sum(doc_counts.values()) for doc_counts in self.document_bigram_counts)\n",
        "\n",
        "        avg_unigrams = total_unigrams / self.total_documents if self.total_documents > 0 else 0\n",
        "        avg_bigrams = total_bigrams / self.total_documents if self.total_documents > 0 else 0\n",
        "\n",
        "        return {\n",
        "            'total_documents': self.total_documents,\n",
        "            'unique_unigrams': len(self.global_unigram_vocab),\n",
        "            'unique_bigrams': len(self.global_bigram_vocab),\n",
        "            'total_unigrams': total_unigrams,\n",
        "            'total_bigrams': total_bigrams,\n",
        "            'avg_unigrams_per_doc': round(avg_unigrams, 1),\n",
        "            'avg_bigrams_per_doc': round(avg_bigrams, 1),\n",
        "            'terms_with_mappings': len(self.original_forms)\n",
        "        }\n",
        "\n",
        "class EnhancedBanglaCombinedKeywordExtractor:\n",
        "    def __init__(self, stopwords_file_path='/content/drive/MyDrive/tfidf/stop.xlsx'):\n",
        "        \"\"\"\n",
        "        Initialize the Enhanced Bangla Combined (Unigram + Bigram) Keyword Extractor\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if BasicTokenizer is None:\n",
        "                raise ImportError(\"BasicTokenizer not available\")\n",
        "\n",
        "            self.tokenizer = BasicTokenizer()\n",
        "\n",
        "            if BanglaStemmer is not None:\n",
        "                self.stemmer = BanglaStemmer()\n",
        "                self.use_stemmer = True\n",
        "                print(\"Tokenizer and Stemmer initialized successfully\")\n",
        "            else:\n",
        "                self.stemmer = None\n",
        "                self.use_stemmer = False\n",
        "                print(\"Tokenizer initialized successfully (Stemmer not available - will use suffix removal)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing BNLP components: {e}\")\n",
        "            self.tokenizer = None\n",
        "            self.stemmer = None\n",
        "            self.use_stemmer = False\n",
        "            print(\"Using fallback tokenization (simple split)\")\n",
        "\n",
        "        # Common Bangla suffixes for normalization (sorted by length for proper matching)\n",
        "        self.bangla_suffixes = [\n",
        "            '‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá','‡¶ó‡ßÅ‡¶≤‡¶ø‡¶ï‡ßá','‡¶ó‡ßÅ‡¶≤‡¶æ‡¶ï‡ßá','‡¶ó‡ßÅ‡¶≤‡ßã', '‡¶ó‡ßÅ‡¶≤‡¶ø', '‡¶ó‡ßÅ‡¶≤‡¶æ', '‡ßá‡¶∞‡¶æ', '‡ßá‡¶¶‡ßá‡¶∞', '‡¶¶‡ßá‡¶∞', '‡¶∞‡¶æ',\n",
        "            '‡¶è‡¶∞', '‡ßá‡¶∞', '‡¶∞', '‡¶ï‡ßá', '‡¶§‡ßá', '‡¶Ø‡¶º‡ßá','‡¶¨‡¶æ‡¶®', '‡¶Æ‡¶æ‡¶®',\n",
        "            '‡¶ñ‡¶æ‡¶®‡¶æ', '‡¶ñ‡¶æ‡¶®‡¶ø', '‡¶ü‡ßÅ‡¶ï‡ßÅ', '‡¶ü‡¶ø‡¶ï‡ßá', '‡¶ü‡¶æ‡¶ï‡ßá', '‡¶ü‡¶ø', '‡¶ü‡¶æ',\n",
        "            '‡¶ì‡¶Ø‡¶º‡¶æ‡¶≤‡¶æ', '‡¶ì‡¶Ø‡¶º‡¶æ‡¶≤‡¶ø', '‡ßá'\n",
        "        ]\n",
        "        self.bangla_suffixes.sort(key=len, reverse=True)\n",
        "\n",
        "        # Initialize stopwords\n",
        "        self.stopwords = set()\n",
        "        if stopwords_file_path:\n",
        "            self._load_custom_stopwords(stopwords_file_path)\n",
        "\n",
        "        # Initialize variables\n",
        "        self.document_paths = []\n",
        "        self.raw_documents = []\n",
        "        self.vocabulary_mapping = {}\n",
        "\n",
        "        # Initialize the efficient TF-IDF calculator\n",
        "        self.tfidf_calculator = EfficientCombinedTFIDF()\n",
        "\n",
        "    def _load_custom_stopwords(self, file_path):\n",
        "        \"\"\"Load custom stopwords from Excel file\"\"\"\n",
        "        try:\n",
        "            df = pd.read_excel(file_path, header=None)\n",
        "            custom_stops = df.iloc[:, 0].dropna().astype(str).str.strip().tolist()\n",
        "            custom_stops = [stop for stop in custom_stops if stop]\n",
        "\n",
        "            # Tokenize stopwords only if tokenizer is available\n",
        "            if self.tokenizer and BasicTokenizer is not None:\n",
        "                tokenized_stopwords = set()\n",
        "                for stop in custom_stops:\n",
        "                    tokens = self.tokenizer.tokenize(stop)\n",
        "                    tokenized_stopwords.update(tokens)\n",
        "                self.stopwords.update(tokenized_stopwords)\n",
        "                print(f\"Successfully loaded and tokenized {len(custom_stops)} stopwords\")\n",
        "            else:\n",
        "                # If no tokenizer, keep stopwords as full words\n",
        "                self.stopwords.update(custom_stops)\n",
        "                print(f\"Successfully loaded {len(custom_stops)} stopwords (no tokenization - tokenizer not available)\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Stopwords file not found at {file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading custom stopwords: {e}\")\n",
        "\n",
        "    def smart_suffix_removal(self, word, corpus_vocabulary=None):\n",
        "        \"\"\"\n",
        "        Smart suffix removal that checks if the root exists in corpus\n",
        "        \"\"\"\n",
        "        if len(word) < 3:\n",
        "            return word, False\n",
        "\n",
        "        # Try stemmer first if available\n",
        "        if self.use_stemmer and self.stemmer is not None:\n",
        "            try:\n",
        "                stem = self.stemmer.stem(word)\n",
        "                if stem and len(stem) > 1:\n",
        "                    if corpus_vocabulary is None or stem in corpus_vocabulary:\n",
        "                        return stem, True\n",
        "                    else:\n",
        "                        return word, False  # Keep original if stem not prominent\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # Fallback to suffix removal\n",
        "        for suffix in self.bangla_suffixes:\n",
        "            if word.endswith(suffix) and len(word) > len(suffix) + 1:\n",
        "                root = word[:-len(suffix)]\n",
        "                if len(root) > 1:\n",
        "                    # Check if root exists in corpus vocabulary\n",
        "                    if corpus_vocabulary is None or root in corpus_vocabulary:\n",
        "                        return root, True\n",
        "                    else:\n",
        "                        return word, False  # Keep original\n",
        "\n",
        "        return word, False\n",
        "\n",
        "    def create_unigrams_and_bigrams_from_raw(self, text):\n",
        "        \"\"\"\n",
        "        Create both unigrams and bigrams from raw text within sentences only\n",
        "        \"\"\"\n",
        "        if not text or not isinstance(text, str):\n",
        "            return [], []\n",
        "\n",
        "        # Split by sentence delimiters\n",
        "        sentences = re.split(r'[‡•§!?]+', text)\n",
        "        all_unigrams = []\n",
        "        all_bigrams = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "            if not sentence:\n",
        "                continue\n",
        "\n",
        "            # Clean sentence but keep basic structure\n",
        "            sentence = re.sub(r'[^\\u0980-\\u09FF\\s]', ' ', sentence)\n",
        "            sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
        "\n",
        "            if not sentence:\n",
        "                continue\n",
        "\n",
        "            # Tokenize the sentence\n",
        "            try:\n",
        "                if self.tokenizer is not None:\n",
        "                    tokens = self.tokenizer.tokenize(sentence)\n",
        "                else:\n",
        "                    tokens = sentence.split()\n",
        "            except Exception:\n",
        "                tokens = sentence.split()\n",
        "\n",
        "            # Filter tokens (basic filtering, no stopword removal yet)\n",
        "            filtered_tokens = []\n",
        "            for token in tokens:\n",
        "                if token and len(token) >= 2:\n",
        "                    filtered_tokens.append(token)\n",
        "\n",
        "            # Create unigrams\n",
        "            all_unigrams.extend(filtered_tokens)\n",
        "\n",
        "            # Create bigrams within this sentence\n",
        "            for i in range(len(filtered_tokens) - 1):\n",
        "                bigram = (filtered_tokens[i], filtered_tokens[i + 1])\n",
        "                all_bigrams.append(bigram)\n",
        "\n",
        "        return all_unigrams, all_bigrams\n",
        "\n",
        "    def build_corpus_vocabulary(self, min_frequency=3):\n",
        "        \"\"\"\n",
        "        Build a vocabulary of prominent terms from all documents\n",
        "        \"\"\"\n",
        "        all_tokens = []\n",
        "\n",
        "        # First pass: collect all tokens with basic processing\n",
        "        for doc_path in self.document_paths:\n",
        "            try:\n",
        "                with open(doc_path, 'r', encoding='utf-8') as file:\n",
        "                    content = file.read()\n",
        "\n",
        "                # Basic text cleaning\n",
        "                content = re.sub(r'[^\\u0980-\\u09FF\\s‡•§!?]', ' ', content)\n",
        "                content = re.sub(r'\\s+', ' ', content).strip()\n",
        "\n",
        "                if self.tokenizer is not None:\n",
        "                    tokens = self.tokenizer.tokenize(content)\n",
        "                else:\n",
        "                    tokens = content.split()\n",
        "\n",
        "                # Basic filtering\n",
        "                filtered_tokens = []\n",
        "                for token in tokens:\n",
        "                    if len(token) >= 2 and not re.match(r'^[‡•§!?]+$', token):\n",
        "                        filtered_tokens.append(token)\n",
        "\n",
        "                all_tokens.extend(filtered_tokens)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {doc_path}: {e}\")\n",
        "\n",
        "        # Count frequencies and find prominent terms\n",
        "        token_counts = Counter(all_tokens)\n",
        "        prominent_terms = {token for token, count in token_counts.items() if count >= min_frequency}\n",
        "\n",
        "        print(f\"Built corpus vocabulary: {len(prominent_terms)} prominent terms\")\n",
        "        return prominent_terms\n",
        "\n",
        "    def preprocess_text_combined(self, text, corpus_vocabulary):\n",
        "        \"\"\"\n",
        "        Preprocess text to create filtered and normalized unigrams and bigrams\n",
        "        \"\"\"\n",
        "        if not text or not isinstance(text, str):\n",
        "            return [], [], {}\n",
        "\n",
        "        # Step 1: Create unigrams and bigrams from raw text\n",
        "        raw_unigrams, raw_bigrams = self.create_unigrams_and_bigrams_from_raw(text)\n",
        "\n",
        "        if not raw_unigrams and not raw_bigrams:\n",
        "            return [], [], {}\n",
        "\n",
        "        # Step 2: Process unigrams\n",
        "        processed_unigrams = []\n",
        "        mapping_info = {}\n",
        "\n",
        "        for word in raw_unigrams:\n",
        "            # Normalize\n",
        "            norm_word, was_merged = self.smart_suffix_removal(word, corpus_vocabulary)\n",
        "\n",
        "            # Check stopword after normalization\n",
        "            if norm_word not in self.stopwords:\n",
        "                processed_unigrams.append(norm_word)\n",
        "\n",
        "                # Update mapping info\n",
        "                if was_merged and norm_word != word:\n",
        "                    if norm_word not in mapping_info:\n",
        "                        mapping_info[norm_word] = set()\n",
        "                    mapping_info[norm_word].add(word)\n",
        "\n",
        "        # Step 3: Process bigrams\n",
        "        processed_bigrams = []\n",
        "\n",
        "        for word1, word2 in raw_bigrams:\n",
        "            # Normalize both words\n",
        "            norm_word1, was_merged1 = self.smart_suffix_removal(word1, corpus_vocabulary)\n",
        "            norm_word2, was_merged2 = self.smart_suffix_removal(word2, corpus_vocabulary)\n",
        "\n",
        "            # Check stopwords after normalization\n",
        "            if norm_word1 not in self.stopwords and norm_word2 not in self.stopwords:\n",
        "                bigram_string = f\"{norm_word1} {norm_word2}\"\n",
        "                processed_bigrams.append(bigram_string)\n",
        "\n",
        "                # Update mapping info\n",
        "                if was_merged1 and norm_word1 != word1:\n",
        "                    if norm_word1 not in mapping_info:\n",
        "                        mapping_info[norm_word1] = set()\n",
        "                    mapping_info[norm_word1].add(word1)\n",
        "\n",
        "                if was_merged2 and norm_word2 != word2:\n",
        "                    if norm_word2 not in mapping_info:\n",
        "                        mapping_info[norm_word2] = set()\n",
        "                    mapping_info[norm_word2].add(word2)\n",
        "\n",
        "        return processed_unigrams, processed_bigrams, mapping_info\n",
        "\n",
        "    def load_documents_from_folder(self, folder_path):\n",
        "        \"\"\"\n",
        "        Load and preprocess all .txt files with combined unigram and bigram processing\n",
        "        \"\"\"\n",
        "        self.document_paths = []\n",
        "        self.raw_documents = []\n",
        "        self.vocabulary_mapping = {}\n",
        "\n",
        "        if not os.path.exists(folder_path):\n",
        "            print(f\"Error: Folder {folder_path} does not exist\")\n",
        "            return\n",
        "\n",
        "        txt_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
        "\n",
        "        if not txt_files:\n",
        "            print(f\"No .txt files found in {folder_path}\")\n",
        "            return\n",
        "\n",
        "        print(f\"Found {len(txt_files)} .txt files.\")\n",
        "\n",
        "        # First, build corpus vocabulary\n",
        "        print(\"Building corpus vocabulary...\")\n",
        "        self.document_paths = [os.path.join(folder_path, f) for f in txt_files]\n",
        "        corpus_vocabulary = self.build_corpus_vocabulary(min_frequency=2)\n",
        "\n",
        "        # Now process documents with combined processing\n",
        "        print(\"Processing documents with combined unigram-bigram extraction...\")\n",
        "        processed_count = 0\n",
        "\n",
        "        for i, filename in enumerate(txt_files):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                    content = file.read()\n",
        "\n",
        "                self.raw_documents.append(content)\n",
        "\n",
        "                # Combined preprocessing\n",
        "                processed_unigrams, processed_bigrams, doc_mappings = self.preprocess_text_combined(content, corpus_vocabulary)\n",
        "\n",
        "                if processed_unigrams or processed_bigrams:\n",
        "                    # Add to efficient TF-IDF calculator\n",
        "                    doc_name = filename\n",
        "                    self.tfidf_calculator.add_document(doc_name, processed_unigrams, processed_bigrams, doc_mappings)\n",
        "\n",
        "                    # Update global vocabulary mapping\n",
        "                    for normalized, originals in doc_mappings.items():\n",
        "                        if normalized not in self.vocabulary_mapping:\n",
        "                            self.vocabulary_mapping[normalized] = set()\n",
        "                        self.vocabulary_mapping[normalized].update(originals)\n",
        "\n",
        "                    processed_count += 1\n",
        "                    if processed_count % 20 == 0:\n",
        "                        print(f\"Processed {processed_count}/{len(txt_files)} documents...\")\n",
        "                else:\n",
        "                    print(f\"‚ö† Skipped (empty after processing): {filename}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚úó Error processing {filename}: {e}\")\n",
        "\n",
        "        print(f\"\\nSuccessfully processed {processed_count} documents\")\n",
        "        print(f\"Vocabulary mappings created: {len(self.vocabulary_mapping)} terms\")\n",
        "\n",
        "        # Show some mapping examples\n",
        "        if self.vocabulary_mapping:\n",
        "            print(\"\\nSample suffix mappings:\")\n",
        "            for normalized, originals in list(self.vocabulary_mapping.items())[:10]:\n",
        "                if len(originals) > 1:\n",
        "                    print(f\"  {normalized} <- {list(originals)}\")\n",
        "\n",
        "    def get_keywords_for_document(self, doc_index, top_k=None):\n",
        "        \"\"\"\n",
        "        Get keywords for a specific document using the combined calculator with decisions\n",
        "        \"\"\"\n",
        "        return self.tfidf_calculator.calculate_tfidf_with_decisions(doc_index, top_k=top_k)\n",
        "\n",
        "    def analyze_all_documents(self, top_k=None, save_to_excel=True, output_path=None):\n",
        "        \"\"\"\n",
        "        Analyze keywords for all documents using the combined calculator with decisions\n",
        "        \"\"\"\n",
        "        return self.tfidf_calculator.analyze_all_documents(top_k=top_k, save_to_excel=save_to_excel, output_path=output_path)\n",
        "\n",
        "    def get_statistics(self):\n",
        "        \"\"\"Get overall statistics\"\"\"\n",
        "        stats = self.tfidf_calculator.get_vocabulary_stats()\n",
        "        stats['terms_with_suffix_mappings'] = len(self.vocabulary_mapping)\n",
        "        return stats\n",
        "\n",
        "    def get_decision_analysis(self, doc_index):\n",
        "        \"\"\"\n",
        "        Get detailed decision analysis for a specific document\n",
        "        \"\"\"\n",
        "        keywords_df = self.get_keywords_for_document(doc_index)\n",
        "        if keywords_df is None:\n",
        "            return None\n",
        "\n",
        "        decision_log = keywords_df.attrs.get('decision_log', [])\n",
        "\n",
        "        analysis = {\n",
        "            'document_name': keywords_df.attrs.get('document_name'),\n",
        "            'eliminated_unigrams': keywords_df.attrs.get('eliminated_unigrams', 0),\n",
        "            'total_keywords': keywords_df.attrs.get('total_keywords_available', 0),\n",
        "            'decisions': {}\n",
        "        }\n",
        "\n",
        "        for bigram, decision, reason in decision_log:\n",
        "            analysis['decisions'][bigram] = {\n",
        "                'decision': decision,\n",
        "                'reason': reason,\n",
        "                'pmi': round(self.tfidf_calculator.calculate_pmi(bigram, doc_index), 3)\n",
        "            }\n",
        "\n",
        "        return analysis\n",
        "\n",
        "# Enhanced main function for combined analysis\n",
        "def enhanced_combined_main():\n",
        "    \"\"\"Enhanced main function for combined unigram-bigram analysis with PMI decisions\"\"\"\n",
        "\n",
        "    # Initialize the enhanced extractor\n",
        "    print(\"Initializing Enhanced Bangla Combined Keyword Extractor...\")\n",
        "    extractor = EnhancedBanglaCombinedKeywordExtractor(\n",
        "        stopwords_file_path='/content/drive/MyDrive/tfidf/temp.xlsx'\n",
        "    )\n",
        "\n",
        "    # Load documents\n",
        "    folder_path = '/content/drive/MyDrive/tfidf/Concatenated_Summary'\n",
        "    print(f\"\\nLoading documents from: {folder_path}\")\n",
        "    extractor.load_documents_from_folder(folder_path)\n",
        "\n",
        "    if extractor.tfidf_calculator.total_documents == 0:\n",
        "        print(\"No documents were processed. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Analyze all documents and save to Excel\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"ANALYZING ALL DOCUMENTS - COMBINED UNIGRAM-BIGRAM WITH PMI DECISIONS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    all_results = extractor.analyze_all_documents(\n",
        "        save_to_excel=True,\n",
        "        output_path='/content/drive/MyDrive/tfidf/all_document_combined_keywords.xlsx'\n",
        "    )\n",
        "\n",
        "    # Print overall statistics\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"OVERALL STATISTICS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    stats = extractor.get_statistics()\n",
        "    print(f\"üìä Total documents processed: {stats['total_documents']}\")\n",
        "    print(f\"üìö Total unigram vocabulary size: {stats['unique_unigrams']}\")\n",
        "    print(f\"üìö Total bigram vocabulary size: {stats['unique_bigrams']}\")\n",
        "    print(f\"üîó Terms with suffix mappings: {stats['terms_with_suffix_mappings']}\")\n",
        "    print(f\"üìù Average unigrams per document: {stats['avg_unigrams_per_doc']}\")\n",
        "    print(f\"üìù Average bigrams per document: {stats['avg_bigrams_per_doc']}\")\n",
        "    print(f\"üìà Total unigrams processed: {stats['total_unigrams']}\")\n",
        "    print(f\"üìà Total bigrams processed: {stats['total_bigrams']}\")\n",
        "\n",
        "    # Show decision analysis for first document\n",
        "    if extractor.tfidf_calculator.total_documents > 0:\n",
        "        print(f\"\\nüîç DECISION ANALYSIS SAMPLE (First Document):\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        decision_analysis = extractor.get_decision_analysis(0)\n",
        "        if decision_analysis:\n",
        "            print(f\"Document: {decision_analysis['document_name']}\")\n",
        "            print(f\"Eliminated unigrams: {decision_analysis['eliminated_unigrams']}\")\n",
        "            print(f\"Total keywords: {decision_analysis['total_keywords']}\")\n",
        "\n",
        "            print(f\"\\nTop 10 PMI Decisions:\")\n",
        "            decisions = decision_analysis['decisions']\n",
        "            sorted_decisions = sorted(decisions.items(), key=lambda x: abs(x[1]['pmi']), reverse=True)\n",
        "\n",
        "            for i, (bigram, info) in enumerate(sorted_decisions[:10]):\n",
        "                print(f\"  {i+1:2d}. {bigram:<25} ‚Üí {info['decision']:<15} (PMI: {info['pmi']:6.2f}) - {info['reason']}\")\n",
        "\n",
        "    # Show most frequently merged terms\n",
        "    if extractor.vocabulary_mapping:\n",
        "        print(f\"\\nüîÑ TOP SUFFIX MERGING EXAMPLES:\")\n",
        "        print(\"-\" * 40)\n",
        "        merged_terms = [(k, v) for k, v in extractor.vocabulary_mapping.items() if len(v) > 1]\n",
        "        merged_terms.sort(key=lambda x: len(x[1]), reverse=True)\n",
        "\n",
        "        for normalized, originals in merged_terms[:15]:\n",
        "            print(f\"  {normalized} ‚Üê {list(originals)}\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"üéâ ENHANCED COMBINED UNIGRAM-BIGRAM ANALYSIS COMPLETE!\")\n",
        "    print(\"üìÅ Check the Excel file for detailed results with PMI-based decisions\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    return extractor\n",
        "\n",
        "class VideoProcessor:\n",
        "    def __init__(self, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "        self.device = device\n",
        "        self.model, self.tokenizer = None, None   # Summary model\n",
        "        self.model2, self.tokenizer2 = None, None # Title model\n",
        "\n",
        "        # Always initialize punct_agent\n",
        "        self.punct_agent = None\n",
        "\n",
        "        # Initialize punctuation agent\n",
        "        try:\n",
        "            from banglanlptoolkit import BanglaPunctuation\n",
        "            self.punct_agent = BanglaPunctuation()\n",
        "            print(\"‚úÖ BanglaPunctuation loaded\")\n",
        "        except ImportError:\n",
        "            print(\"‚ö†Ô∏è BanglaPunctuation not available, using fallback\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Unexpected error loading BanglaPunctuation: {e}\")\n",
        "\n",
        "    def _load_summary_model(self):\n",
        "        if self.model is None:\n",
        "            # Free title model if loaded\n",
        "            if self.model2 is not None:\n",
        "                del self.model2\n",
        "                del self.tokenizer2\n",
        "                self.model2, self.tokenizer2 = None, None\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            model_path = \"/content/drive/My Drive/Thesis_Dataset/fine_tuned_bangla_t5\"\n",
        "            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "            self.model.to(self.device)\n",
        "\n",
        "\n",
        "    def _load_title_model(self):\n",
        "        if self.model2 is None:\n",
        "            # Free summary model if loaded\n",
        "            if self.model is not None:\n",
        "                del self.model\n",
        "                del self.tokenizer\n",
        "                self.model, self.tokenizer = None, None\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            self.model2 = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "                \"/content/drive/My Drive/Thesis_Dataset/academic_title_model\"\n",
        "            )\n",
        "            self.tokenizer2 = AutoTokenizer.from_pretrained(\n",
        "                \"/content/drive/My Drive/Thesis_Dataset/academic_title_model\"\n",
        "            )\n",
        "            self.model2.to(self.device)\n",
        "\n",
        "\n",
        "\n",
        "    def safe_normalize(self, text):\n",
        "        \"\"\"Safe normalization that works with or without bnunicodenormalizer\"\"\"\n",
        "        try:\n",
        "            from bnunicodenormalizer import normalize\n",
        "            return normalize(text)\n",
        "        except ImportError:\n",
        "            import unicodedata\n",
        "            if not text or not isinstance(text, str):\n",
        "                return text\n",
        "            text = unicodedata.normalize('NFC', text)\n",
        "            text = ' '.join(text.split())\n",
        "            return text.strip()\n",
        "        except Exception:\n",
        "            return text\n",
        "\n",
        "    def extract_audio_from_video(self, video_path, audio_output_path):\n",
        "        \"\"\"Extract audio from video file and save as WAV\"\"\"\n",
        "        try:\n",
        "            print(f\"üé¨ Loading video: {video_path}\")\n",
        "\n",
        "            if not os.path.exists(video_path):\n",
        "                raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
        "\n",
        "            video = VideoFileClip(video_path)\n",
        "            duration = video.duration\n",
        "            print(f\"‚è±Ô∏è Video duration: {duration/60:.1f} minutes\")\n",
        "\n",
        "            if video.audio is None:\n",
        "                raise Exception(\"Video has no audio track!\")\n",
        "\n",
        "            os.makedirs(os.path.dirname(audio_output_path), exist_ok=True)\n",
        "\n",
        "            print(f\"üéµ Extracting audio to: {audio_output_path}\")\n",
        "            video.audio.write_audiofile(\n",
        "                audio_output_path,\n",
        "                codec='pcm_s16le',\n",
        "                verbose=False,\n",
        "                logger=None\n",
        "            )\n",
        "\n",
        "            video.close()\n",
        "\n",
        "            if os.path.exists(audio_output_path):\n",
        "                file_size_mb = os.path.getsize(audio_output_path) / (1024 * 1024)\n",
        "                print(f\"‚úÖ Audio extracted successfully! ({file_size_mb:.1f} MB)\")\n",
        "                return True\n",
        "            else:\n",
        "                raise Exception(\"Audio extraction failed - output file not created\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error extracting audio: {e}\")\n",
        "            raise\n",
        "\n",
        "    def add_punctuation(self, raw_text):\n",
        "        \"\"\"Add punctuation to the raw text\"\"\"\n",
        "        if self.punct_agent:\n",
        "            try:\n",
        "                raw_text = ' '.join(raw_text.split())\n",
        "                return self.punct_agent.add_punctuation(raw_text)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Fallback punctuation logic\n",
        "        text = raw_text.strip()\n",
        "        if not text.endswith(('‡•§', '?', '!')):\n",
        "            text += '‡•§'\n",
        "        return text\n",
        "\n",
        "    def format_timestamp(self, ms):\n",
        "        \"\"\"Convert milliseconds to MM:SS format\"\"\"\n",
        "        seconds = ms // 1000\n",
        "        minutes = seconds // 60\n",
        "        seconds = seconds % 60\n",
        "        return f\"{minutes:02d}:{seconds:02d}\"\n",
        "\n",
        "    def smart_chunk_amplify(self, chunk, max_boost_db=8, target_level_db=-20):\n",
        "        \"\"\"\n",
        "        Enhanced per-chunk amplification with normalization\n",
        "        - Applies amplification-normalization strategy per chunk\n",
        "        - Prevents clipping with safety margin\n",
        "        - Optimizes each chunk individually\n",
        "        - Maintains consistent loudness across chunks\n",
        "        \"\"\"\n",
        "        # Initial measurements\n",
        "        current_avg = chunk.dBFS\n",
        "        peak_level = chunk.max_dBFS\n",
        "\n",
        "        # Determine desired boost based on average level\n",
        "        if current_avg > -15:\n",
        "            # Already loud enough\n",
        "            desired_boost = 0\n",
        "        elif current_avg > -25:\n",
        "            # Moderately quiet\n",
        "            desired_boost = min(3, max_boost_db)\n",
        "        elif current_avg > -35:\n",
        "            # Quiet\n",
        "            desired_boost = min(6, max_boost_db)\n",
        "        else:\n",
        "            # Very quiet\n",
        "            desired_boost = max_boost_db\n",
        "\n",
        "        # Apply amplification if needed\n",
        "        if desired_boost > 0:\n",
        "            amplified_chunk = chunk + desired_boost\n",
        "        else:\n",
        "            amplified_chunk = chunk\n",
        "\n",
        "        # Normalization step (safety check to prevent clipping)\n",
        "        if amplified_chunk.max_dBFS > -3:  # Leave 3dB headroom\n",
        "            normalized_chunk = amplified_chunk.normalize(headroom=3.0)\n",
        "            return normalized_chunk, desired_boost, current_avg, peak_level, True\n",
        "        else:\n",
        "            return amplified_chunk, desired_boost, current_avg, peak_level, False\n",
        "\n",
        "\n",
        "    def transcribe_audio(self, audio_path, chunk_length_ms=45000, max_tokens=512,\n",
        "                                        apply_per_chunk_amplification=True, max_boost_db=8):\n",
        "        \"\"\"\n",
        "        Enhanced transcription function with:\n",
        "        - First: Audio chunking (45 sec) to get sentence-level timestamps\n",
        "        - Second: Token-based chunking (512 tokens) with proper time mapping\n",
        "        - Smart per-chunk amplification for better audio quality\n",
        "        - Overlapping chunks (one sentence overlap)\n",
        "        \"\"\"\n",
        "        # Initialize recognizer with optimized settings\n",
        "        recognizer = sr.Recognizer()\n",
        "\n",
        "        # Optimize recognizer settings for better accuracy\n",
        "        recognizer.energy_threshold = 300\n",
        "        recognizer.dynamic_energy_threshold = True\n",
        "        recognizer.pause_threshold = 0.5\n",
        "\n",
        "        print(f\"‚úÖ Optimized speech recognizer initialized\")\n",
        "        print(f\"‚ö° Per-chunk amplification: {apply_per_chunk_amplification}\")\n",
        "        print(f\"üåç Target language: Bengali (bn-BD)\")\n",
        "        print(f\"üéØ Audio chunk size: {chunk_length_ms/1000:.1f} seconds\")\n",
        "        print(f\"üìù Token-based final chunks: {max_tokens} tokens max\")\n",
        "\n",
        "        try:\n",
        "            audio = AudioSegment.from_wav(audio_path)\n",
        "            print(f\"‚úÖ Audio loaded: {len(audio)/1000:.1f} seconds, Avg: {audio.dBFS:.1f} dB, Peak: {audio.max_dBFS:.1f} dB\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading audio file: {e}\")\n",
        "            return None\n",
        "\n",
        "        chunks = make_chunks(audio, chunk_length_ms)\n",
        "        all_chunks_text = []\n",
        "        audio_chunks_timestamps = []\n",
        "        amplification_stats = []\n",
        "        failed_chunks = 0\n",
        "\n",
        "        print(f\"üìù Step 1: Transcribing audio in {len(chunks)} short chunks for sentence mapping...\")\n",
        "\n",
        "        # Step 1: Transcribe all audio chunks (45 seconds each)\n",
        "        for i, chunk in enumerate(tqdm(chunks, desc=\"Transcribing audio chunks\")):\n",
        "            chunk_filename = f\"temp_chunk_{i}.wav\"\n",
        "\n",
        "            start_time_ms = i * chunk_length_ms\n",
        "            end_time_ms = min((i + 1) * chunk_length_ms, len(audio))\n",
        "            audio_chunks_timestamps.append((start_time_ms, end_time_ms))\n",
        "\n",
        "            try:\n",
        "                # Apply smart per-chunk amplification\n",
        "                if apply_per_chunk_amplification:\n",
        "                    processed_chunk, boost_applied, orig_avg, orig_peak, is_normalized = self.smart_chunk_amplify(\n",
        "                        chunk, max_boost_db=max_boost_db\n",
        "                    )\n",
        "\n",
        "                    amplification_stats.append({\n",
        "                        'Chunk_ID': i + 1,\n",
        "                        'Original_Avg_dB': round(orig_avg, 1),\n",
        "                        'Original_Peak_dB': round(orig_peak, 1),\n",
        "                        'Boost_Applied_dB': round(boost_applied, 1),\n",
        "                        'Final_Avg_dB': round(processed_chunk.dBFS, 1),\n",
        "                        'Final_Peak_dB': round(processed_chunk.max_dBFS, 1),\n",
        "                        'Was_Normalized': is_normalized\n",
        "                    })\n",
        "                else:\n",
        "                    processed_chunk = chunk\n",
        "                    boost_applied = 0\n",
        "\n",
        "                processed_chunk.export(chunk_filename, format=\"wav\")\n",
        "\n",
        "                with sr.AudioFile(chunk_filename) as source:\n",
        "                    audio_data = recognizer.record(source)\n",
        "                    try:\n",
        "                        text = recognizer.recognize_google(audio_data, language=\"bn-BD\")\n",
        "                        text_clean = ' '.join(text.split())\n",
        "                        punctuated_text = self.add_punctuation(text_clean)\n",
        "                        all_chunks_text.append(punctuated_text)\n",
        "                    except sr.UnknownValueError:\n",
        "                        all_chunks_text.append(\"[Unrecognized Audio]\")\n",
        "                        failed_chunks += 1\n",
        "                    except sr.RequestError as e:\n",
        "                        all_chunks_text.append(f\"[Request Error: {e}]\")\n",
        "                        failed_chunks += 1\n",
        "\n",
        "                if os.path.exists(chunk_filename):\n",
        "                    os.remove(chunk_filename)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing chunk {i}: {e}\")\n",
        "                all_chunks_text.append(f\"[Processing Error: {e}]\")\n",
        "                failed_chunks += 1\n",
        "                if os.path.exists(chunk_filename):\n",
        "                    os.remove(chunk_filename)\n",
        "\n",
        "        # Step 2: Create sentence-level mapping with timestamps and carry-over handling\n",
        "        print(\"üîÑ Step 2: Creating sentence-level timestamp mapping with carry-over handling...\")\n",
        "\n",
        "        sentence_data = []\n",
        "        sentence_id = 1\n",
        "        carry_over_text = \"\"  # For incomplete sentences across audio chunks\n",
        "        carry_over_start_ms = 0\n",
        "\n",
        "        for chunk_idx, (text, (start_ms, end_ms)) in enumerate(zip(all_chunks_text, audio_chunks_timestamps)):\n",
        "            if text.startswith(\"[\"):\n",
        "                # Error chunk - treat as one sentence, but handle carry-over first\n",
        "                if carry_over_text.strip():\n",
        "                    # Save carry-over as incomplete sentence before error chunk\n",
        "                    sentence_data.append({\n",
        "                        'sentence_id': sentence_id,\n",
        "                        'text': carry_over_text.strip(),\n",
        "                        'start_ms': carry_over_start_ms,\n",
        "                        'end_ms': start_ms,  # End at start of error chunk\n",
        "                        'chunk_duration_ms': start_ms - carry_over_start_ms,\n",
        "                        'source_chunk_id': chunk_idx,\n",
        "                        'is_error': False,\n",
        "                        'is_complete': False\n",
        "                    })\n",
        "                    sentence_id += 1\n",
        "                    carry_over_text = \"\"\n",
        "\n",
        "                # Add error chunk\n",
        "                sentence_data.append({\n",
        "                    'sentence_id': sentence_id,\n",
        "                    'text': text,\n",
        "                    'start_ms': start_ms,\n",
        "                    'end_ms': end_ms,\n",
        "                    'chunk_duration_ms': end_ms - start_ms,\n",
        "                    'source_chunk_id': chunk_idx + 1,\n",
        "                    'is_error': True,\n",
        "                    'is_complete': True\n",
        "                })\n",
        "                sentence_id += 1\n",
        "                continue\n",
        "\n",
        "            # Combine carry-over text with current chunk\n",
        "            full_text = carry_over_text + \" \" + text if carry_over_text else text\n",
        "            full_text = full_text.strip()\n",
        "\n",
        "            # Split into sentences\n",
        "            sentences = split_into_sentences(full_text)\n",
        "            if not sentences:\n",
        "                # No sentences found, treat entire text as carry-over\n",
        "                if not carry_over_text:\n",
        "                    carry_over_start_ms = start_ms\n",
        "                carry_over_text = full_text\n",
        "                continue\n",
        "\n",
        "            # Check if the last sentence is complete\n",
        "            last_sentence_complete = False\n",
        "            if full_text.strip():\n",
        "                if re.search(r'[‡•§?!]\\s*$', full_text.strip()):\n",
        "                    last_sentence_complete = True\n",
        "\n",
        "            if last_sentence_complete or chunk_idx == len(all_chunks_text) - 1:\n",
        "                complete_sentences = sentences\n",
        "                new_carry_over = \"\"\n",
        "            else:\n",
        "                if len(sentences) > 1:\n",
        "                    complete_sentences = sentences[:-1]\n",
        "                    new_carry_over = sentences[-1] if sentences else \"\"\n",
        "                else:\n",
        "                    complete_sentences = []\n",
        "                    new_carry_over = sentences[0] if sentences else full_text\n",
        "\n",
        "            if complete_sentences:\n",
        "                chunk_duration_ms = end_ms - start_ms\n",
        "                actual_start_ms = carry_over_start_ms if carry_over_text else start_ms\n",
        "\n",
        "                if carry_over_text:\n",
        "                    total_duration_ms = end_ms - carry_over_start_ms\n",
        "                    time_per_sentence = total_duration_ms / len(complete_sentences) if complete_sentences else total_duration_ms\n",
        "                else:\n",
        "                    time_per_sentence = chunk_duration_ms / len(complete_sentences) if complete_sentences else chunk_duration_ms\n",
        "\n",
        "                for sent_idx, sentence in enumerate(complete_sentences):\n",
        "                    if sentence.strip():\n",
        "                        sent_start_ms = actual_start_ms + (sent_idx * time_per_sentence)\n",
        "                        sent_end_ms = actual_start_ms + ((sent_idx + 1) * time_per_sentence)\n",
        "\n",
        "                        sentence_data.append({\n",
        "                            'sentence_id': sentence_id,\n",
        "                            'text': sentence.strip(),\n",
        "                            'start_ms': int(sent_start_ms),\n",
        "                            'end_ms': int(sent_end_ms),\n",
        "                            'chunk_duration_ms': int(sent_end_ms - sent_start_ms),\n",
        "                            'source_chunk_id': chunk_idx + 1,\n",
        "                            'is_error': False,\n",
        "                            'is_complete': True\n",
        "                        })\n",
        "                        sentence_id += 1\n",
        "\n",
        "            carry_over_text = new_carry_over\n",
        "            if carry_over_text:\n",
        "                if complete_sentences:\n",
        "                    carry_over_start_ms = int(actual_start_ms + (len(complete_sentences) * time_per_sentence))\n",
        "                else:\n",
        "                    carry_over_start_ms = start_ms\n",
        "\n",
        "        if carry_over_text.strip():\n",
        "            last_end_ms = audio_chunks_timestamps[-1][1] if audio_chunks_timestamps else carry_over_start_ms\n",
        "            sentence_data.append({\n",
        "                'sentence_id': sentence_id,\n",
        "                'text': carry_over_text.strip(),\n",
        "                'start_ms': carry_over_start_ms,\n",
        "                'end_ms': last_end_ms,\n",
        "                'chunk_duration_ms': last_end_ms - carry_over_start_ms,\n",
        "                'source_chunk_id': len(all_chunks_text),\n",
        "                'is_error': False,\n",
        "                'is_complete': False\n",
        "            })\n",
        "        print(f\"‚úÖ Created {len(sentence_data)} sentences with timestamps\")\n",
        "\n",
        "        # Step 3: Create token-based chunks with overlapping and carry-over handling\n",
        "        print(f\"üîÑ Step 3: Creating token-based chunks (max {max_tokens} tokens) with overlap and carry-over...\")\n",
        "\n",
        "        def count_tokens(text):\n",
        "            try:\n",
        "                tokens = self.tokenizer.encode(text)\n",
        "                return len(tokens)\n",
        "            except:\n",
        "                return len(text) // 4\n",
        "\n",
        "        token_based_chunks = []\n",
        "        current_chunk_sentences = []\n",
        "        current_chunk_tokens = 0\n",
        "        previous_last_sentence = None\n",
        "        incomplete_sentence_buffer = []\n",
        "        max_tokens = 512\n",
        "\n",
        "        for sentence in sentence_data:\n",
        "            sentence_text = sentence['text']\n",
        "            sentence_tokens = count_tokens(sentence_text)\n",
        "            is_complete = sentence.get('is_complete', True)\n",
        "\n",
        "            if not is_complete and not sentence.get('is_error', False):\n",
        "                incomplete_sentence_buffer.append(sentence)\n",
        "                continue\n",
        "\n",
        "            if incomplete_sentence_buffer:\n",
        "                merged_text_parts = [s['text'] for s in incomplete_sentence_buffer] + [sentence_text]\n",
        "                merged_text = ' '.join(merged_text_parts).strip()\n",
        "\n",
        "                merged_sentence = {\n",
        "                    'sentence_id': incomplete_sentence_buffer[0]['sentence_id'],\n",
        "                    'text': merged_text,\n",
        "                    'start_ms': incomplete_sentence_buffer[0]['start_ms'],\n",
        "                    'end_ms': sentence['end_ms'],\n",
        "                    'is_complete': True,\n",
        "                    'is_error': sentence.get('is_error', False),\n",
        "                    'merged_from_count': len(incomplete_sentence_buffer) + 1\n",
        "                }\n",
        "\n",
        "                incomplete_sentence_buffer = []\n",
        "                sentence_to_process = merged_sentence\n",
        "                sentence_tokens = count_tokens(merged_text)\n",
        "            else:\n",
        "                sentence_to_process = sentence\n",
        "\n",
        "            overlap_tokens = 0\n",
        "            if previous_last_sentence is not None:\n",
        "                overlap_tokens = count_tokens(previous_last_sentence['text'])\n",
        "\n",
        "            total_tokens_needed = current_chunk_tokens + sentence_tokens + overlap_tokens\n",
        "\n",
        "            if (total_tokens_needed > max_tokens and current_chunk_sentences) or (sentence_tokens + overlap_tokens > max_tokens):\n",
        "                if current_chunk_sentences:\n",
        "                    chunk_sentences_with_overlap = []\n",
        "                    has_overlap = False\n",
        "                    overlap_sentence_text = \"\"\n",
        "                    actual_overlap_tokens = 0\n",
        "\n",
        "                    if previous_last_sentence is not None and len(token_based_chunks) > 0:\n",
        "                        overlap_text = previous_last_sentence['text']\n",
        "                        actual_overlap_tokens = count_tokens(overlap_text)\n",
        "\n",
        "                        if current_chunk_tokens + actual_overlap_tokens <= max_tokens:\n",
        "                            overlap_sentence_dict = {\n",
        "                                'text': overlap_text,\n",
        "                                'sentence_id': previous_last_sentence['sentence_id'],\n",
        "                                'start_ms': previous_last_sentence['start_ms'],\n",
        "                                'end_ms': previous_last_sentence['end_ms']\n",
        "                            }\n",
        "                            chunk_sentences_with_overlap.append(overlap_sentence_dict)\n",
        "                            has_overlap = True\n",
        "                            overlap_sentence_text = overlap_text\n",
        "\n",
        "                    chunk_sentences_with_overlap.extend(current_chunk_sentences)\n",
        "\n",
        "                    chunk_start_time = chunk_sentences_with_overlap[0]['start_ms']\n",
        "                    chunk_end_time = chunk_sentences_with_overlap[-1]['end_ms']\n",
        "                    chunk_text = '‡•§'.join([s['text'] for s in chunk_sentences_with_overlap]) + '‡•§'\n",
        "\n",
        "                    final_token_count = count_tokens(chunk_text)\n",
        "\n",
        "                    token_based_chunks.append({\n",
        "                        'text': chunk_text,\n",
        "                        'start_ms': chunk_start_time,\n",
        "                        'end_ms': chunk_end_time,\n",
        "                        'token_count': final_token_count,\n",
        "                        'sentence_count': len(current_chunk_sentences),\n",
        "                        'first_sentence_id': current_chunk_sentences[0]['sentence_id'],\n",
        "                        'last_sentence_id': current_chunk_sentences[-1]['sentence_id'],\n",
        "                        'has_overlap': has_overlap,\n",
        "                        'overlap_sentence': overlap_sentence_text,\n",
        "                        'overlap_tokens': actual_overlap_tokens,\n",
        "                        'base_tokens': current_chunk_tokens,\n",
        "                        'has_merged_sentences': any(s.get('merged_from_count', 0) > 1 for s in current_chunk_sentences)\n",
        "                    })\n",
        "\n",
        "                    previous_last_sentence = current_chunk_sentences[-1]\n",
        "\n",
        "                current_chunk_sentences = [sentence_to_process]\n",
        "                current_chunk_tokens = sentence_tokens\n",
        "\n",
        "                if sentence_tokens + overlap_tokens > max_tokens:\n",
        "                    print(f\"‚ö†Ô∏è Warning: Sentence {sentence_to_process.get('sentence_id', 'unknown')} with overlap exceeds max_tokens ({sentence_tokens + overlap_tokens} > {max_tokens})\")\n",
        "            else:\n",
        "                current_chunk_sentences.append(sentence_to_process)\n",
        "                current_chunk_tokens += sentence_tokens\n",
        "\n",
        "        if incomplete_sentence_buffer:\n",
        "            merged_text = ' '.join([s['text'] for s in incomplete_sentence_buffer]).strip()\n",
        "            merged_sentence = {\n",
        "                'sentence_id': incomplete_sentence_buffer[0]['sentence_id'],\n",
        "                'text': merged_text,\n",
        "                'start_ms': incomplete_sentence_buffer[0]['start_ms'],\n",
        "                'end_ms': incomplete_sentence_buffer[-1]['end_ms'],\n",
        "                'is_complete': False,\n",
        "                'is_error': False,\n",
        "                'merged_from_count': len(incomplete_sentence_buffer)\n",
        "            }\n",
        "            current_chunk_sentences.append(merged_sentence)\n",
        "            current_chunk_tokens += count_tokens(merged_text)\n",
        "\n",
        "        if current_chunk_sentences:\n",
        "            chunk_sentences_with_overlap = []\n",
        "            has_overlap = False\n",
        "            overlap_sentence_text = \"\"\n",
        "            actual_overlap_tokens = 0\n",
        "\n",
        "            if previous_last_sentence is not None and len(token_based_chunks) > 0:\n",
        "                overlap_text = previous_last_sentence['text']\n",
        "                actual_overlap_tokens = count_tokens(overlap_text)\n",
        "\n",
        "                if current_chunk_tokens + actual_overlap_tokens <= max_tokens:\n",
        "                    overlap_sentence_dict = {\n",
        "                        'text': overlap_text,\n",
        "                        'sentence_id': previous_last_sentence['sentence_id'],\n",
        "                        'start_ms': previous_last_sentence['start_ms'],\n",
        "                        'end_ms': previous_last_sentence['end_ms']\n",
        "                    }\n",
        "                    chunk_sentences_with_overlap.append(overlap_sentence_dict)\n",
        "                    has_overlap = True\n",
        "                    overlap_sentence_text = overlap_text\n",
        "\n",
        "            chunk_sentences_with_overlap.extend(current_chunk_sentences)\n",
        "\n",
        "            chunk_start_time = chunk_sentences_with_overlap[0]['start_ms']\n",
        "            chunk_end_time = chunk_sentences_with_overlap[-1]['end_ms']\n",
        "            chunk_text = '‡•§'.join([s['text'] for s in chunk_sentences_with_overlap]) + '‡•§'\n",
        "\n",
        "            final_token_count = count_tokens(chunk_text)\n",
        "\n",
        "            token_based_chunks.append({\n",
        "                'text': chunk_text,\n",
        "                'start_ms': chunk_start_time,\n",
        "                'end_ms': chunk_end_time,\n",
        "                'token_count': final_token_count,\n",
        "                'sentence_count': len(current_chunk_sentences),\n",
        "                'first_sentence_id': current_chunk_sentences[0]['sentence_id'],\n",
        "                'last_sentence_id': current_chunk_sentences[-1]['sentence_id'],\n",
        "                'has_overlap': has_overlap,\n",
        "                'overlap_sentence': overlap_sentence_text,\n",
        "                'overlap_tokens': actual_overlap_tokens,\n",
        "                'base_tokens': current_chunk_tokens,\n",
        "                'has_merged_sentences': any(s.get('merged_from_count', 0) > 1 for s in current_chunk_sentences)\n",
        "            })\n",
        "\n",
        "        print(f\"‚úÖ Created {len(token_based_chunks)} token-based chunks\")\n",
        "\n",
        "        # Step 4: Create DataFrames\n",
        "        print(\"üìä Step 4: Preparing output data...\")\n",
        "\n",
        "        token_chunk_data = []\n",
        "        for i, chunk in enumerate(token_based_chunks):\n",
        "            token_chunk_data.append({\n",
        "                \"Chunk_ID\": i + 1,\n",
        "                \"Start_Time\": self.format_timestamp(chunk[\"start_ms\"]),\n",
        "                \"End_Time\": self.format_timestamp(chunk[\"end_ms\"]),\n",
        "                \"Duration_MS\": chunk[\"end_ms\"] - chunk[\"start_ms\"],\n",
        "                \"Text\": chunk[\"text\"],\n",
        "                \"Token_Count\": chunk[\"token_count\"],\n",
        "                \"Sentence_Count\": chunk[\"sentence_count\"],\n",
        "                \"First_Sentence_ID\": chunk[\"first_sentence_id\"],\n",
        "                \"Last_Sentence_ID\": chunk[\"last_sentence_id\"],\n",
        "                \"Has_Overlap\": chunk[\"has_overlap\"],\n",
        "                \"Overlap_Sentence\": chunk[\"overlap_sentence\"],\n",
        "                \"Overlap_Tokens\": chunk.get(\"overlap_tokens\", 0),\n",
        "                \"Base_Tokens\": chunk.get(\"base_tokens\", 0),\n",
        "                \"Has_Merged_Sentences\": chunk.get(\"has_merged_sentences\", False)\n",
        "            })\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        return pd.DataFrame(token_chunk_data)\n",
        "\n",
        "\n",
        "    def generate_summary(self, text):\n",
        "        try:\n",
        "            self._load_summary_model()  # ensure summary model is loaded\n",
        "\n",
        "            input_text = f\"summarize: {text}\"\n",
        "            inputs = self.tokenizer(\n",
        "                input_text,\n",
        "                max_length=512,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=\"max_length\"\n",
        "            ).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                summary_ids = self.model.generate(\n",
        "                    inputs['input_ids'],\n",
        "                    attention_mask=inputs['attention_mask'],\n",
        "                    max_length=200,\n",
        "                    min_length=100,\n",
        "                    length_penalty=1.0,\n",
        "                    num_beams=4,\n",
        "                    no_repeat_ngram_size=3,\n",
        "                    early_stopping=True,\n",
        "                    pad_token_id=self.tokenizer.pad_token_id,\n",
        "                    do_sample=False\n",
        "                )\n",
        "\n",
        "            summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "            return summary if summary.strip() else text[:200] + \"...\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating summary: {e}\")\n",
        "            return text[:200] + \"...\" if len(text) > 200 else text\n",
        "\n",
        "    def generate_title(self, summary):\n",
        "        self._load_title_model()  # ensure title model is loaded\n",
        "\n",
        "        # Handle long summaries\n",
        "        tokens = self.tokenizer2.encode(summary, add_special_tokens=False)\n",
        "        if len(tokens) > 1024:\n",
        "            tokens = tokens[:1024]\n",
        "            summary = self.tokenizer2.decode(tokens, skip_special_tokens=True)\n",
        "\n",
        "        inputs = self.tokenizer2(summary, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            title_ids = self.model2.generate(inputs['input_ids'], num_beams=4, min_length=5, max_length=30)\n",
        "\n",
        "        return self.tokenizer2.decode(title_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    def concatenate_pairs(self, df):\n",
        "        \"\"\"\n",
        "        Concatenate pairs of rows in the dataframe\n",
        "        - Combines text/summaries from two consecutive rows\n",
        "        - Updates Start_Time and End_Time accordingly\n",
        "        \"\"\"\n",
        "        new_rows = []\n",
        "\n",
        "        for i in range(0, len(df), 2):\n",
        "            if i + 1 < len(df):\n",
        "                # Pair exists - concatenate two rows\n",
        "                row1 = df.iloc[i]\n",
        "                row2 = df.iloc[i + 1]\n",
        "\n",
        "                # Combine text (use Summary if available, otherwise Text)\n",
        "                text1 = row1.get('Summary', row1.get('Text', ''))\n",
        "                text2 = row2.get('Summary', row2.get('Text', ''))\n",
        "                combined_text = f\"{text1} {text2}\"\n",
        "\n",
        "                # Create new row with combined data\n",
        "                new_row = {\n",
        "                    'Chunk_ID': f\"combined_{i//2 + 1}\",\n",
        "                    'Text': combined_text,\n",
        "                    'Start_Time': row1.get('Start_Time', ''),\n",
        "                    'End_Time': row2.get('End_Time', '')\n",
        "                }\n",
        "\n",
        "                # Preserve any other columns from the first row\n",
        "                for col in df.columns:\n",
        "                    if col not in ['Chunk_ID', 'Text', 'Start_Time', 'End_Time', 'Summary']:\n",
        "                        new_row[col] = row1.get(col, '')\n",
        "\n",
        "                new_rows.append(new_row)\n",
        "            else:\n",
        "                # Odd row - keep as is\n",
        "                row = df.iloc[i].copy()\n",
        "                row['Chunk_ID'] = f\"single_{i//2 + 1}\"\n",
        "\n",
        "                # Use Summary if available, otherwise Text\n",
        "                if 'Summary' in row and pd.notna(row['Summary']):\n",
        "                    row['Text'] = row['Summary']\n",
        "\n",
        "                new_rows.append(row.to_dict())\n",
        "\n",
        "        return pd.DataFrame(new_rows)\n",
        "\n",
        "    # Function to count tokens\n",
        "    def count_tokens(self, text, tokenizer):\n",
        "        return len(tokenizer.encode(text, truncation=False, padding=False))\n",
        "\n",
        "    def word_count(self,text):\n",
        "      if pd.isna(text):\n",
        "        return 0\n",
        "      return len(str(text).split())\n",
        "\n",
        "    def clean_text(self,text):\n",
        "      if pd.isna(text):\n",
        "        return \"\"\n",
        "      return re.sub(r\"\\[[a-zA-Z _]+\\]\", \" \", str(text))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def process_complete_pipeline(self, video_file_path, temp_dir, max_chunks=50, job_id=None,top_k=20):\n",
        "        try:\n",
        "            # File paths\n",
        "            audio_path = os.path.join(temp_dir, \"extracted_audio.wav\")\n",
        "            transcription_path = os.path.join(temp_dir, \"transcription.xlsx\")\n",
        "            summary_path = os.path.join(temp_dir, \"summaries.xlsx\")\n",
        "            final_text_path = os.path.join(temp_dir, \"final_summary.txt\")\n",
        "\n",
        "            def update_status(progress, message):\n",
        "                \"\"\"Helper function to update both local results and global status\"\"\"\n",
        "                if job_id and job_id in processing_status:\n",
        "                    processing_status[job_id].update({\n",
        "                        'progress': progress,\n",
        "                        'message': message,\n",
        "                        'status': 'processing'\n",
        "                    })\n",
        "\n",
        "            # Step 1: Extract audio\n",
        "            update_status(10, 'Extracting audio from video...')\n",
        "            self.extract_audio_from_video(video_file_path, audio_path)\n",
        "\n",
        "            # Step 2: Transcribe\n",
        "            update_status(30, 'Transcribing audio...')\n",
        "            transcription_df = self.transcribe_audio(audio_path)\n",
        "            transcription_df[\"Text\"] = transcription_df[\"Text\"].apply(self.clean_text)\n",
        "\n",
        "            # Save transcription\n",
        "            transcription_df.to_excel(transcription_path, index=False)\n",
        "\n",
        "            # Step 3: Generate summaries with recursive logic\n",
        "            update_status(50, 'Processing summaries...')\n",
        "\n",
        "            # Check if we need recursive summarization\n",
        "            if len(transcription_df) > max_chunks:\n",
        "                update_status(55, f'Input has {len(transcription_df)} chunks (>{max_chunks}). Starting recursive summarization...')\n",
        "                print(f\"‚ö° Input has {len(transcription_df)} rows (>{max_chunks}). Starting recursive summarization...\")\n",
        "\n",
        "                # Perform recursive summarization\n",
        "                final_df, total_iterations = self.recursive_summarization_with_concatenation(\n",
        "                    transcription_df, max_chunks, job_id\n",
        "                )\n",
        "\n",
        "                update_status(75, f'Completed recursive summarization in {total_iterations} iterations')\n",
        "                print(f\"üéØ Completed recursive summarization in {total_iterations} iterations\")\n",
        "\n",
        "            else:\n",
        "                update_status(60, f'Input has ‚â§{max_chunks} chunks. Generating summaries directly...')\n",
        "                print(f\"üìÑ Input has ‚â§{max_chunks} rows. Generating summaries directly...\")\n",
        "\n",
        "                # Direct summarization for smaller datasets\n",
        "                transcription_df['Final_Summary'] = transcription_df['Text'].apply(\n",
        "                    lambda x: self.generate_summary(str(x)) if not str(x).startswith('[') else x\n",
        "                )\n",
        "                final_df = transcription_df\n",
        "                total_iterations = 1\n",
        "\n",
        "            # Step 4: Save final summaries\n",
        "            update_status(80, 'Saving summary results...')\n",
        "\n",
        "            # Ensure Final_Summary column exists\n",
        "            if 'Final_Summary' not in final_df.columns:\n",
        "                final_df['Final_Summary'] = final_df['Text'].apply(\n",
        "                    lambda x: self.generate_summary(str(x)) if not str(x).startswith('[') else x\n",
        "                )\n",
        "\n",
        "            # Save summaries\n",
        "            final_df.to_excel(summary_path, index=False)\n",
        "            print(f\"DataFrame shape: {final_df.shape}\")\n",
        "            print(f\"Number of rows: {len(final_df)}\")\n",
        "            total_text_words = final_df[\"Text\"].apply(self.word_count).sum()\n",
        "            total_summary_words = final_df[\"Final_Summary\"].apply(self.word_count).sum()\n",
        "            compression_ratio = total_summary_words / total_text_words if total_text_words > 0 else 0\n",
        "            # Step 5: Create final text with timestamps\n",
        "            update_status(90, 'Creating final summary document...')\n",
        "\n",
        "            tmp_text=\"\"\n",
        "            for index, row in final_df.iterrows():\n",
        "\n",
        "                summary = row.get('Final_Summary', row.get('Text', ''))\n",
        "                tmp_text += f\"{summary}\"\n",
        "\n",
        "\n",
        "\n",
        "            TARGET_FOLDER = \"/content/drive/MyDrive/tfidf/Concatenated_Summary\"\n",
        "            EXCEL_PATH = \"/content/drive/MyDrive/tfidf/all_document_combined_keywords.xlsx\"\n",
        "\n",
        "            tmp_text=tmp_text.strip()\n",
        "            title=self.generate_title(tmp_text)\n",
        "            SOURCE_FILE_PATH=\"/content/drive/MyDrive/tfidf/new.txt\"\n",
        "\n",
        "            with open(SOURCE_FILE_PATH, 'w', encoding='utf-8') as file:\n",
        "              file.write(tmp_text)  # Or tmp_text if that's what you're using\n",
        "\n",
        "            print(f\"File saved successfully to: {SOURCE_FILE_PATH}\")\n",
        "\n",
        "            source_filename = os.path.basename(SOURCE_FILE_PATH)\n",
        "            target_file_path = os.path.join(TARGET_FOLDER, source_filename)\n",
        "\n",
        "            try:\n",
        "              shutil.copy2(SOURCE_FILE_PATH, target_file_path)\n",
        "              print(f\"‚úÖ File copied to: {target_file_path}\")\n",
        "\n",
        "            except Exception as e:\n",
        "              print(f\"‚ùå Error copying file: {e}\")\n",
        "              exit()\n",
        "\n",
        "            try:\n",
        "              extractor = enhanced_combined_main()  # This will process the TARGET_FOLDER\n",
        "              if extractor is None:\n",
        "                print(\"‚ùå enhanced_combined_main() failed\")\n",
        "                exit()\n",
        "            except Exception as e:\n",
        "              print(f\"‚ùå Error in enhanced_combined_main(): {e}\")\n",
        "              exit()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            try:\n",
        "              if not os.path.exists(EXCEL_PATH):\n",
        "                print(f\"‚ùå Excel file not found: {EXCEL_PATH}\")\n",
        "                exit()\n",
        "\n",
        "              df = pd.read_excel(EXCEL_PATH)\n",
        "              target_filename = source_filename  # This already has extension like doc1.txt\n",
        "              matching_rows = df[df['Document_Name'] == target_filename]\n",
        "              if matching_rows.empty:\n",
        "                unique_docs = df['Document_Name'].unique()\n",
        "                for i, doc in enumerate(unique_docs[:10]):\n",
        "                  print(f\"   {i+1}. {doc}\")\n",
        "                if len(unique_docs) > 10:\n",
        "                  print(f\"   ... and {len(unique_docs) - 10} more\")\n",
        "              else:\n",
        "                matching_rows = matching_rows.sort_values('Rank')\n",
        "                keywords = matching_rows['Keyword'].tolist()\n",
        "                print(\"Extracted Keywords:\")\n",
        "                for keyword in keywords:\n",
        "                  print(keyword)\n",
        "                doc_name = os.path.splitext(target_filename)[0]\n",
        "                output_filename = f\"{doc_name}_combined_keywords.xlsx\"\n",
        "                output_path = os.path.join(\"/content/drive/MyDrive/tfidf/\", output_filename)\n",
        "                with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
        "                  matching_rows.to_excel(writer, sheet_name='Keywords', index=False)\n",
        "                  decision_summary = matching_rows.groupby(['Term_Type', 'Decision']).size().reset_index(name='Count')\n",
        "                  decision_summary.to_excel(writer, sheet_name='Decision_Summary', index=False)\n",
        "                  pmi_analysis = matching_rows[matching_rows['PMI'] != 0].sort_values('PMI', ascending=False)\n",
        "                  if not pmi_analysis.empty:\n",
        "                    pmi_analysis.to_excel(writer, sheet_name='PMI_Analysis', index=False)\n",
        "                print(f\"üíæ Combined keywords saved to: {output_path}\")\n",
        "            except Exception as e:\n",
        "              print(f\"‚ùå Error extracting keywords: {e}\")\n",
        "            try:\n",
        "              os.remove(target_file_path)\n",
        "            except Exception as e:\n",
        "              print(f\"‚ö†Ô∏è  Warning: Could not remove file: {e}\")\n",
        "\n",
        "            keywords_text = \", \".join(keywords)\n",
        "            final_text = f\"Title: {title}\\n\\n\"\n",
        "            final_text += f\"Important Keywords: {keywords_text}\\n\\n\"\n",
        "            final_text += f\"Compression Ratio: {compression_ratio:.4f}\\n\\n\"\n",
        "            for index, row in final_df.iterrows():\n",
        "                start_time = row.get('Start_Time', '')\n",
        "                end_time = row.get('End_Time', '')\n",
        "                summary = row.get('Final_Summary', row.get('Text', ''))\n",
        "                chunk = f\"[{start_time} - {end_time}]\\n{summary}\\n\\n\"\n",
        "                print(f\"Row {index}: {repr(chunk)}\")\n",
        "\n",
        "                final_text += chunk\n",
        "\n",
        "\n",
        "            # 2. Define a dictionary of regex patterns and replacements\n",
        "                regex_patterns = {\n",
        "    r\"‡¶ü‡ßÅ ‡¶¶‡¶ø (‡¶™‡¶æ‡¶ì‡ßü‡¶æ‡¶∞|‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞)|‡¶ü‡ßÅ ‡¶¶‡¶æ (‡¶™‡¶æ‡¶ì‡ßü‡¶æ‡¶∞|‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞)\": \"^\",     # Replace \"‡¶ü‡ßÅ ‡¶¶‡¶ø ‡¶™‡¶æ‡¶ì‡ßü‡¶æ‡¶∞\" with \"^\"\n",
        "    r\"‡¶∏‡¶Æ‡¶æ‡¶® ‡¶∏‡¶Æ‡¶æ‡¶®|‡¶á‡¶ï‡ßÅ‡¶Ø‡¶º‡¶æ‡¶≤‡¶∏ ‡¶ü‡ßÅ|‡¶á‡¶ï‡ßÅ‡ßü‡¶æ‡¶≤‡¶∏ ‡¶ü‡ßÅ|‡¶á‡¶ï‡ßÅ‡¶Ø‡¶º‡¶æ‡¶≤ ‡¶ü‡ßÅ|‡¶á‡¶ï‡ßÅ‡ßü‡¶æ‡¶≤ ‡¶ü‡ßÅ\": \"=\",\n",
        "\n",
        "                 # Replace \"‡¶¶‡¶∂\" and \"‡¶ü‡ßá‡¶®\" with \"10\"\n",
        "    r\"‡¶è‡¶ó‡¶æ‡¶∞‡ßã|‡¶è‡¶≤‡ßá‡¶≠‡ßá‡¶®\": \"11\",      # Replace \"‡¶è‡¶ó‡¶æ‡¶∞‡ßã\" and \"‡¶è‡¶≤‡ßá‡¶≠‡ßá‡¶®\" with \"11\"\n",
        "    r\"‡¶¨‡¶æ‡¶∞‡ßã|‡¶ü‡ßÅ‡¶Ø‡¶º‡ßá‡¶≤‡¶≠|‡¶ü‡ßÅ‡ßü‡ßá‡¶≤‡¶≠\": \"12\",        # Replace \"‡¶¨‡¶æ‡¶∞‡ßã\" and \"‡¶ü‡ßÅ‡ßü‡ßá‡¶≤‡¶≠\" with \"12\"\n",
        "    r\"‡¶§‡ßá‡¶∞‡ßã|‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø‡¶®\": \"13\",      # Replace \"‡¶§‡ßá‡¶∞‡ßã\" and \"‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø‡¶®\" with \"13\"\n",
        "    r\"‡¶ö‡ßã‡¶¶‡ßç‡¶¶‡ßã|‡¶´‡ßã‡¶∞‡ßç‡¶ü‡¶ø‡¶®\": \"14\",    # Replace \"‡¶ö‡ßã‡¶¶‡ßç‡¶¶‡ßã\" and \"‡¶´‡ßã‡¶∞‡ßç‡¶ü‡¶ø‡¶®\" with \"14\"\n",
        "    r\"‡¶™‡¶®‡ßá‡¶∞‡ßã|‡¶´‡¶ø‡¶´‡¶ü‡¶ø‡¶®\": \"15\",      # Replace \"‡¶™‡¶®‡ßá‡¶∞‡ßã\" and \"‡¶´‡¶ø‡¶´‡¶ü‡¶ø‡¶®\" with \"15\"\n",
        "    r\"‡¶∑‡ßã‡¶≤‡ßã|‡¶∏‡¶ø‡¶ï‡ßç‡¶∏‡¶ü‡¶ø‡¶®\": \"16\",     # Replace \"‡¶∑‡ßã‡¶≤‡ßã\" and \"‡¶∏‡¶ø‡¶ï‡ßç‡¶∏‡¶ü‡¶ø‡¶®\" with \"16\"\n",
        "    r\"‡¶∏‡¶§‡ßá‡¶∞‡ßã|‡¶∏‡ßá‡¶≠‡ßá‡¶®‡ßç‡¶ü‡¶ø‡¶®\": \"17\",   # Replace \"‡¶∏‡¶§‡ßá‡¶∞‡ßã\" and \"‡¶∏‡ßá‡¶≠‡ßá‡¶®‡ßç‡¶ü‡¶ø‡¶®\" with \"17\"\n",
        "    r\"‡¶Ü‡¶†‡¶æ‡¶∞‡ßã|‡¶è‡¶á‡¶ü‡¶ø‡¶®\": \"18\",       # Replace \"‡¶Ü‡¶†‡¶æ‡¶∞‡ßã\" and \"‡¶è‡¶á‡¶ü‡¶ø‡¶®\" with \"18\"\n",
        "    r\"‡¶ä‡¶®‡¶ø‡¶∂|‡¶®‡¶æ‡¶á‡¶®‡¶ü‡¶ø‡¶®\": \"19\",       # Replace \"‡¶ä‡¶®‡¶ø‡¶∂\" and \"‡¶®‡¶ø‡¶®‡¶ü‡¶ø‡¶®\" with \"19\"\n",
        "\n",
        "    r\"‡¶è‡¶ï‡ßÅ‡¶∂|(‡¶ü‡ßÅ‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶ü‡¶ø|‡¶ü‡ßÅ‡ßü‡ßá‡¶®‡ßç‡¶ü‡¶ø) (‡¶ì‡¶Ø‡¶º‡¶æ‡¶®|‡¶ì‡ßü‡¶æ‡¶®)\": \"21\",# Replace \"‡¶è‡¶ï‡ßÅ‡¶∂\" and \"‡¶ü‡ßÅ‡ßü‡ßá‡¶®‡ßç‡¶ü‡¶ø ‡¶ì‡ßü‡¶æ‡¶®\" with \"21\"\n",
        "    r\"‡¶¨‡¶æ‡¶á‡¶∂|(‡¶ü‡ßÅ‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶ü‡¶ø|‡¶ü‡ßÅ‡ßü‡ßá‡¶®‡ßç‡¶ü‡¶ø) ‡¶ü‡ßÅ\": \"22\",  # Replace \"‡¶¨‡¶æ‡¶á‡¶∂\" and \"‡¶ü‡ßÅ‡ßü‡ßá‡¶®‡ßç‡¶ü‡¶ø ‡¶ü‡ßÅ\" with \"22\"\n",
        "    r\"‡¶§‡ßá‡¶á‡¶∂|(‡¶ü‡ßÅ‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶ü‡¶ø|‡¶ü‡ßÅ‡ßü‡ßá‡¶®‡ßç‡¶ü‡¶ø) ‡¶•‡ßç‡¶∞‡¶ø\": \"23\",# Replace \"‡¶§‡ßá‡¶á‡¶∂\" and \"‡¶ü‡ßÅ‡ßü‡ßá‡¶®‡ßç‡¶ü‡¶ø ‡¶•‡ßç‡¶∞‡¶ø\" with \"23\"\n",
        "    r\"‡¶ö‡¶¨‡ßç‡¶¨‡¶ø‡¶∂|(‡¶ü‡ßÅ‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶ü‡¶ø|‡¶ü‡ßÅ‡ßü‡ßá‡¶®‡ßç‡¶ü‡¶ø) ‡¶´‡ßã‡¶∞\": \"24\",# Replace \"‡¶ö‡¶¨‡ßç‡¶¨‡¶ø‡¶∂\" and \"‡¶ü‡ßÅ‡ßü‡ßá‡¶®‡ßç‡¶ü‡¶ø ‡¶´‡ßã‡¶∞\" with \"24\"\n",
        "    r\"‡¶™‡¶Å‡¶ö‡¶ø‡¶∂|(‡¶ü‡ßÅ‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶ü‡¶ø|‡¶ü‡ßÅ‡ßü‡ßá‡¶®‡ßç‡¶ü‡¶ø) ‡¶´‡¶æ‡¶á‡¶≠\": \"25\",# Replace \"‡¶™‡¶Å‡¶á‡¶§‡ßç‡¶∞‡¶ø‡¶∂\" and \"‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø ‡¶´‡¶æ‡¶á‡¶≠\" with \"25\"\n",
        "    r\"‡¶õ‡¶æ‡¶¨‡ßç‡¶¨‡¶ø‡¶∂|(‡¶ü‡ßÅ‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶ü‡¶ø|‡¶ü‡ßÅ‡ßü‡ßá‡¶®‡ßç‡¶ü‡¶ø) ‡¶∏‡¶ø‡¶ï‡ßç‡¶∏\": \"26\",# Replace \"‡¶õ‡¶§‡ßç‡¶∞‡¶ø‡¶∂\" and \"‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø ‡¶∏‡¶ø‡¶ï‡ßç‡¶∏\" with \"26\"\n",
        "    r\"‡¶∏‡¶æ‡¶§‡¶æ‡¶∂|(‡¶ü‡ßÅ‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶ü‡¶ø|‡¶ü‡ßÅ‡ßü‡ßá‡¶®‡ßç‡¶ü‡¶ø) ‡¶∏‡ßá‡¶≠‡ßá‡¶®\": \"27\",# Replace \"‡¶∏‡¶æ‡¶§‡¶§‡ßç‡¶∞‡¶ø‡¶∂\" and \"‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø ‡¶∏‡ßá‡¶≠‡ßá‡¶®\" with \"27\"\n",
        "    r\"‡¶Ü‡¶ü‡¶æ‡¶∂|(‡¶ü‡ßÅ‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶ü‡¶ø|‡¶ü‡ßÅ‡ßü‡ßá‡¶®‡ßç‡¶ü‡¶ø) ‡¶è‡¶á‡¶ü\": \"28\",# Replace \"‡¶Ü‡¶ü‡¶§‡ßç‡¶∞‡¶ø‡¶∂\" and \"‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø ‡¶è‡¶á‡¶ü\" with \"28\"\n",
        "    r\"‡¶ä‡¶®‡¶§‡ßç‡¶∞‡¶ø‡¶∂|(‡¶ü‡ßÅ‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶ü‡¶ø|‡¶ü‡ßÅ‡ßü‡ßá‡¶®‡ßç‡¶ü‡¶ø) ‡¶®‡¶æ‡¶á‡¶®\": \"29\",# Replace \"‡¶ä‡¶®‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\" and \"‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø ‡¶®‡¶æ‡¶á‡¶®\" with \"29\"\n",
        "\n",
        "    r\"‡¶è‡¶ï‡¶§‡ßç‡¶∞‡¶ø‡¶∂|‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø (‡¶ì‡¶Ø‡¶º‡¶æ‡¶®|‡¶ì‡ßü‡¶æ‡¶®)\": \"31\",# Replace \"‡¶è‡¶ï‡¶§‡ßç‡¶∞‡¶ø‡¶∂\" and \"‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø ‡¶ì‡ßü‡¶æ‡¶®\" with \"31\"\n",
        "    r\"‡¶¨‡¶§‡ßç‡¶∞‡¶ø‡¶∂|‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø ‡¶ü‡ßÅ\": \"32\",   # Replace \"‡¶¨‡¶§‡ßç‡¶∞‡¶ø‡¶∂\" and \"‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø ‡¶ü‡ßÅ\" with \"32\"\n",
        "    r\"‡¶§‡ßá‡¶§‡ßç‡¶∞‡¶ø‡¶∂|‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø ‡¶•‡ßç‡¶∞‡¶ø\": \"33\",# Replace \"‡¶§‡ßá‡¶§‡ßç‡¶∞‡¶ø‡¶∂\" and \"‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø ‡¶•‡ßç‡¶∞‡¶ø\" with \"33\"\n",
        "    r\"‡¶ö‡ßå‡¶§‡ßç‡¶∞‡¶ø‡¶∂|‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø ‡¶´‡ßã‡¶∞\": \"34\",# Replace \"‡¶ö‡¶§‡ßç‡¶∞‡¶ø‡¶∂\" and \"‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø ‡¶´‡ßã‡¶∞\" with \"34\"\n",
        "    r\"‡¶™‡¶Å‡¶á‡ßü‡¶§‡ßç‡¶∞‡¶ø‡¶∂|‡¶™‡¶Å‡¶á‡¶Ø‡¶º‡¶§‡ßç‡¶∞‡¶ø‡¶∂|‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø ‡¶´‡¶æ‡¶á‡¶≠\": \"35\",# Replace \"‡¶™‡¶Å‡¶á‡¶§‡ßç‡¶∞‡¶ø‡¶∂\" and \"‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø ‡¶´‡¶æ‡¶á‡¶≠\" with \"35\"\n",
        "    r\"‡¶õ‡¶§‡ßç‡¶∞‡¶ø‡¶∂|‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø ‡¶∏‡¶ø‡¶ï‡ßç‡¶∏\": \"36\",# Replace \"‡¶õ‡¶§‡ßç‡¶∞‡¶ø‡¶∂\" and \"‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø ‡¶∏‡¶ø‡¶ï‡ßç‡¶∏\" with \"36\"\n",
        "    r\"‡¶∏‡¶æ‡¶á‡¶§‡ßç‡¶∞‡¶ø‡¶∂|‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø ‡¶∏‡ßá‡¶≠‡ßá‡¶®\": \"37\",# Replace \"‡¶∏‡¶æ‡¶§‡¶§‡ßç‡¶∞‡¶ø‡¶∂\" and \"‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø ‡¶∏‡ßá‡¶≠‡ßá‡¶®\" with \"37\"\n",
        "    r\"‡¶Ü‡¶ü‡¶§‡ßç‡¶∞‡¶ø‡¶∂|‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø ‡¶è‡¶á‡¶ü\": \"38\",# Replace \"‡¶Ü‡¶ü‡¶§‡ßç‡¶∞‡¶ø‡¶∂\" and \"‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø ‡¶è‡¶á‡¶ü\" with \"38\"\n",
        "    r\"‡¶ä‡¶®‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂|‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø ‡¶®‡¶æ‡¶á‡¶®\": \"39\",# Replace \"‡¶ä‡¶®‡¶™‡¶Å‡¶á‡¶§‡ßç‡¶∞‡¶ø‡¶∂\" and \"‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø ‡¶®‡¶æ‡¶á‡¶®\" with \"39\"\n",
        "\n",
        "    r\"‡¶è‡¶ï‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂|‡¶´‡¶∞‡¶ü‡¶ø (‡¶ì‡¶Ø‡¶º‡¶æ‡¶®|‡¶ì‡ßü‡¶æ‡¶®)\": \"41\",# Replace \"‡¶è‡¶ï‡¶§‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\" and \"‡¶´‡¶∞‡¶ü‡¶ø ‡¶ì‡ßü‡¶æ‡¶®\" with \"41\"\n",
        "    r\"‡¶¨‡¶ø‡ßü‡¶æ‡¶≤‡ßç‡¶≤‡¶ø‡¶∂|‡¶¨‡¶ø‡¶Ø‡¶º‡¶æ‡¶≤‡ßç‡¶≤‡¶ø‡¶∂|‡¶´‡¶∞‡¶ü‡¶ø ‡¶ü‡ßÅ\": \"42\",# Replace \"‡¶¨‡¶ø‡ßü‡¶æ‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\" and \"‡¶´‡¶∞‡¶ü‡¶ø ‡¶ü‡ßÅ\" with \"42\"\n",
        "    r\"‡¶§‡ßá‡¶§‡¶æ‡¶≤‡ßç‡¶≤‡¶ø‡¶∂|‡¶´‡¶∞‡¶ü‡¶ø ‡¶•‡ßç‡¶∞‡¶ø\": \"43\",# Replace \"‡¶§‡ßá‡¶§‡¶æ‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\" and \"‡¶´‡¶∞‡¶ü‡¶ø ‡¶•‡ßç‡¶∞‡¶ø\" with \"43\"\n",
        "    r\"‡¶ö‡ßÅ‡ßü‡¶æ‡¶≤‡ßç‡¶≤‡¶ø‡¶∂|‡¶ö‡ßÅ‡¶Ø‡¶º‡¶æ‡¶≤‡ßç‡¶≤‡¶ø‡¶∂|‡¶´‡¶∞‡¶ü‡¶ø ‡¶´‡ßã‡¶∞\": \"44\",# Replace \"‡¶ö‡ßÅ‡ßü‡¶æ‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\" and \"‡¶´‡¶∞‡¶ü‡¶ø ‡¶´‡ßã‡¶∞\" with \"44\"\n",
        "    r\"‡¶™‡¶Å‡ßü‡¶§‡¶æ‡¶≤‡ßç‡¶≤‡¶ø‡¶∂|‡¶™‡¶Å‡¶Ø‡¶º‡¶§‡¶æ‡¶≤‡ßç‡¶≤‡¶ø‡¶∂|‡¶´‡¶∞‡¶ü‡¶ø ‡¶´‡¶æ‡¶á‡¶≠\": \"45\",# Replace \"‡¶™‡¶Å‡¶á‡¶§‡ßç‡¶∞‡¶æ‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\" and \"‡¶´‡¶∞‡¶ü‡¶ø ‡¶´‡¶æ‡¶á‡¶≠\" with \"45\"\n",
        "    r\"‡¶õ‡ßá‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂|‡¶´‡¶∞‡¶ü‡¶ø ‡¶∏‡¶ø‡¶ï‡ßç‡¶∏\": \"46\",     # Replace \"‡¶õ‡ßá‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\" and \"‡¶´‡¶ø‡¶´‡¶ü‡¶ø\" with \"46\"\n",
        "    r\"‡¶∏‡¶æ‡¶§‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂|‡¶´‡¶∞‡¶ü‡¶ø ‡¶∏‡ßá‡¶≠‡ßá‡¶®\": \"47\",# Replace \"‡¶∏‡¶æ‡¶§‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\" and \"‡¶´‡¶ø‡¶´‡¶ü‡¶ø ‡¶ì‡ßü‡¶æ‡¶®\" with \"47\"\n",
        "    r\"‡¶Ü‡¶ü‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂|‡¶´‡¶∞‡¶ü‡¶ø ‡¶è‡¶á‡¶ü\": \"48\",  # Replace \"‡¶Ü‡¶ü‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\" and \"‡¶´‡¶ø‡¶´‡¶ü‡¶ø ‡¶ü‡ßÅ\" with \"48\"\n",
        "    r\"‡¶ä‡¶®‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶∂|‡¶´‡¶∞‡¶ü‡¶ø ‡¶®‡¶æ‡¶á‡¶®\": \"49\",# Replace \"‡¶ä‡¶®‡¶®‡¶¨‡ßç‡¶¨‡¶á\" and \"‡¶´‡¶ø‡¶´‡¶ü‡¶ø ‡¶•‡ßç‡¶∞‡¶ø\" with \"49\"\n",
        "\n",
        "    r\"‡¶è‡¶ï‡¶æ‡¶®‡ßç‡¶®|‡¶´‡¶ø‡¶´‡¶ü‡¶ø (‡¶ì‡¶Ø‡¶º‡¶æ‡¶®|‡¶ì‡ßü‡¶æ‡¶®)\": \"51\", # Replace \"‡¶è‡¶ï‡¶æ‡¶®‡ßç‡¶®\" and \"‡¶´‡¶ø‡¶´‡¶ü‡¶ø ‡¶´‡¶æ‡¶á‡¶≠\" with \"51\"\n",
        "    r\"‡¶¨‡¶æ‡ßü‡¶æ‡¶®‡ßç‡¶®|‡¶´‡¶ø‡¶´‡¶ü‡¶ø ‡¶ü‡ßÅ\": \"52\",# Replace \"‡¶¨‡¶æ‡ßü‡¶æ‡¶®‡ßç‡¶®\" and \"‡¶´‡¶ø‡¶´‡¶ü‡¶ø ‡¶∏‡¶ø‡¶ï‡ßç‡¶∏\" with \"52\"\n",
        "    r\"‡¶§‡¶ø‡¶™‡ßç‡¶™‡¶æ‡¶®‡ßç‡¶®|‡¶´‡¶ø‡¶´‡¶ü‡¶ø ‡¶•‡ßç‡¶∞‡¶ø\": \"53\",# Replace \"‡¶§‡¶ø‡¶™‡ßç‡¶™‡¶æ‡¶®‡ßç‡¶®\" and \"‡¶´‡¶ø‡¶´‡¶ü‡¶ø ‡¶∏‡ßá‡¶≠‡ßá‡¶®\" with \"53\"\n",
        "    r\"‡¶ö‡ßÅ‡ßü‡¶æ‡¶®‡ßç‡¶®|‡¶ö‡ßÅ‡¶Ø‡¶º‡¶æ‡¶®‡ßç‡¶®|‡¶´‡¶ø‡¶´‡¶ü‡¶ø ‡¶´‡ßã‡¶∞\": \"54\",# Replace \"‡¶ö‡ßÅ‡ßü‡¶æ‡¶®‡ßç‡¶®\" and \"‡¶´‡¶ø‡¶´‡¶ü‡¶ø ‡¶è‡¶á‡¶ü\" with \"54\"\n",
        "    r\"‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶®‡ßç‡¶®|‡¶´‡¶ø‡¶´‡¶ü‡¶ø ‡¶´‡¶æ‡¶á‡¶≠\": \"55\",# Replace \"‡¶™‡¶Å‡¶á‡¶™‡¶Å‡¶ö‡¶æ‡¶®‡ßç‡¶®\" and \"‡¶´‡¶ø‡¶´‡¶ü‡¶ø ‡¶®‡¶æ‡¶á‡¶®\" with \"55\"\n",
        "    r\"‡¶õ‡¶æ‡¶™‡ßç‡¶™‡¶æ‡¶®‡ßç‡¶®|‡¶´‡¶ø‡¶´‡¶ü‡¶ø ‡¶∏‡¶ø‡¶ï‡ßç‡¶∏\": \"56\",\n",
        "    r\"‡¶∏‡¶æ‡¶§‡¶æ‡¶®‡ßç‡¶®|‡¶´‡¶ø‡¶´‡¶ü‡¶ø ‡¶∏‡ßá‡¶≠‡ßá‡¶®\": \"57\",\n",
        "    r\"‡¶è‡¶ü‡¶æ‡¶®‡ßç‡¶®|‡¶´‡¶ø‡¶´‡¶ü‡¶ø ‡¶è‡¶á‡¶ü\": \"58\",\n",
        "    r\"‡¶ä‡¶®‡¶∑‡¶æ‡¶ü|‡¶´‡¶ø‡¶´‡¶ü‡¶ø ‡¶®‡¶æ‡¶á‡¶®\": \"59\",\n",
        "\n",
        "    r\"‡¶è‡¶ï‡¶∑‡¶ü‡ßç‡¶ü‡¶ø|‡¶∏‡¶ø‡¶ï‡ßç‡¶∏‡¶ü‡¶ø (‡¶ì‡¶Ø‡¶º‡¶æ‡¶®|‡¶ì‡ßü‡¶æ‡¶®)\": \"61\",    # Replace \"‡¶è‡¶ï‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\" and \"‡¶∏‡ßá‡¶≠‡ßá‡¶®‡¶ü‡¶ø\" with \"61\"\n",
        "    r\"‡¶¨‡¶æ‡¶∑‡¶ü‡ßç‡¶ü‡¶ø|‡¶∏‡¶ø‡¶ï‡ßç‡¶∏‡¶ü‡¶ø ‡¶ü‡ßÅ\": \"62\",# Replace \"‡¶¨‡¶æ‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\" and \"‡¶∏‡ßá‡¶≠‡ßá‡¶®‡¶ü‡¶ø ‡¶ì‡ßü‡¶æ‡¶®\" with \"62\"\n",
        "    r\"‡¶§‡ßá‡¶∑‡¶ü‡ßç‡¶ü‡¶ø|‡¶∏‡¶ø‡¶ï‡ßç‡¶∏‡¶ü‡¶ø ‡¶•‡ßç‡¶∞‡¶ø\": \"63\",\n",
        "    r\"‡¶ö‡ßå‡¶∑‡¶ü‡ßç‡¶ü‡¶ø|‡¶∏‡¶ø‡¶ï‡ßç‡¶∏‡¶ü‡¶ø ‡¶´‡ßã‡¶∞\": \"64\",\n",
        "    r\"‡¶™‡¶Å‡¶á‡¶∑‡¶ü‡ßç‡¶ü‡¶ø|‡¶∏‡¶ø‡¶ï‡ßç‡¶∏‡¶ü‡¶ø ‡¶´‡¶æ‡¶á‡¶≠\": \"65\",\n",
        "    r\"‡¶õ‡ßá‡¶∑‡¶ü‡ßç‡¶ü‡¶ø|‡¶∏‡¶ø‡¶ï‡ßç‡¶∏‡¶ü‡¶ø ‡¶∏‡¶ø‡¶ï‡ßç‡¶∏\": \"66\",\n",
        "    r\"‡¶∏‡¶æ‡¶§‡¶∑‡¶ü‡ßç‡¶ü‡¶ø|‡¶∏‡¶ø‡¶ï‡ßç‡¶∏‡¶ü‡¶ø ‡¶∏‡ßá‡¶≠‡ßá‡¶®\": \"67\",\n",
        "    r\"‡¶Ü‡¶ü‡¶∑‡¶ü‡ßç‡¶ü‡¶ø|‡¶∏‡¶ø‡¶ï‡ßç‡¶∏‡¶ü‡¶ø ‡¶è‡¶á‡¶ü\": \"68\",\n",
        "    r\"‡¶ä‡¶®‡¶∏‡¶§‡ßç‡¶§‡¶∞|‡¶∏‡¶ø‡¶ï‡ßç‡¶∏‡¶ü‡¶ø ‡¶®‡¶æ‡¶á‡¶®\": \"69\",\n",
        "\n",
        "    r\"‡¶è‡¶ï‡¶æ‡¶§‡ßç‡¶§‡¶∞|‡¶∏‡ßá‡¶≠‡ßá‡¶®‡ßç‡¶ü‡¶ø (‡¶ì‡¶Ø‡¶º‡¶æ‡¶®|‡¶ì‡ßü‡¶æ‡¶®)\": \"71\",\n",
        "    r\"‡¶¨‡¶æ‡¶π‡¶æ‡¶§‡ßç‡¶§‡¶∞|‡¶∏‡ßá‡¶≠‡ßá‡¶®‡ßç‡¶ü‡¶ø ‡¶ü‡ßÅ‡•§‡¶¨‡¶æ‡ßü‡¶æ‡¶§‡ßç‡¶§‡¶∞|‡¶¨‡¶ø‡¶Ø‡¶º‡¶æ‡¶§‡ßç‡¶§‡¶∞\": \"72\",\n",
        "    r\"‡¶§‡¶ø‡ßü‡¶æ‡¶§‡ßç‡¶§‡¶∞|‡¶§‡¶ø‡¶Ø‡¶º‡¶æ‡¶§‡ßç‡¶§‡¶∞|‡¶∏‡ßá‡¶≠‡ßá‡¶®‡ßç‡¶ü‡¶ø ‡¶•‡ßç‡¶∞‡¶ø\": \"73\",\n",
        "    r\"‡¶ö‡ßÅ‡ßü‡¶æ‡¶§‡ßç‡¶§‡¶∞|‡¶ö‡ßÅ‡¶Ø‡¶º‡¶æ‡¶§‡ßç‡¶§‡¶∞|‡¶∏‡ßá‡¶≠‡ßá‡¶®‡ßç‡¶ü‡¶ø ‡¶´‡ßã‡¶∞\": \"74\",\n",
        "    r\"‡¶™‡¶Å‡¶ö‡¶æ‡¶§‡ßç‡¶§‡¶∞|‡¶∏‡ßá‡¶≠‡ßá‡¶®‡ßç‡¶ü‡¶ø ‡¶´‡¶æ‡¶á‡¶≠\": \"75\",\n",
        "    r\"‡¶õ‡¶ø‡ßü‡¶æ‡¶§‡ßç‡¶§‡¶∞|‡¶õ‡¶ø‡¶Ø‡¶º‡¶æ‡¶§‡ßç‡¶§‡¶∞|‡¶∏‡ßá‡¶≠‡ßá‡¶®‡ßç‡¶ü‡¶ø ‡¶∏‡¶ø‡¶ï‡ßç‡¶∏\": \"76\",\n",
        "    r\"‡¶∏‡¶æ‡¶§‡¶æ‡¶§‡ßç‡¶§‡¶∞|‡¶∏‡ßá‡¶≠‡ßá‡¶®‡ßç‡¶ü‡¶ø ‡¶∏‡ßá‡¶≠‡ßá‡¶®\": \"77\",\n",
        "    r\"‡¶Ü‡¶ü‡¶æ‡¶§‡ßç‡¶§‡¶∞|‡¶∏‡ßá‡¶≠‡ßá‡¶®‡ßç‡¶ü‡¶ø ‡¶è‡¶á‡¶ü\": \"78\",\n",
        "    r\"‡¶ä‡¶®‡¶Ü‡¶∂‡¶ø|‡¶∏‡ßá‡¶≠‡ßá‡¶®‡ßç‡¶ü‡¶ø ‡¶®‡¶æ‡¶á‡¶®\": \"79\",\n",
        "\n",
        "    r\"‡¶è‡¶ï‡¶æ‡¶∂‡¶ø|‡¶è‡¶á‡¶ü‡¶ø (‡¶ì‡¶Ø‡¶º‡¶æ‡¶®|‡¶ì‡ßü‡¶æ‡¶®)\": \"81\",\n",
        "    r\"‡¶¨‡¶ø‡¶∞‡¶æ‡¶∂‡¶ø|‡¶è‡¶á‡¶ü‡¶ø ‡¶ü‡ßÅ\": \"82\",\n",
        "    r\"‡¶§‡¶ø‡¶∞‡¶æ‡¶∂‡¶ø|‡¶è‡¶á‡¶ü‡¶ø ‡¶•‡ßç‡¶∞‡¶ø\": \"83\",\n",
        "    r\"‡¶ö‡ßÅ‡¶∞‡¶æ‡¶∂‡¶ø|‡¶è‡¶á‡¶ü‡¶ø ‡¶´‡ßã‡¶∞\": \"84\",\n",
        "    r\"‡¶™‡¶Å‡¶ö‡¶æ‡¶∂‡¶ø|‡¶è‡¶á‡¶ü‡¶ø ‡¶´‡¶æ‡¶á‡¶≠\": \"85\",\n",
        "    r\"‡¶õ‡¶ø‡ßü‡¶æ‡¶∂‡¶ø|‡¶õ‡¶ø‡¶Ø‡¶º‡¶æ‡¶∂‡¶ø|‡¶è‡¶á‡¶ü‡¶ø ‡¶∏‡¶ø‡¶ï‡ßç‡¶∏\": \"86\",\n",
        "    r\"‡¶∏‡¶æ‡¶§‡¶æ‡¶∂‡¶ø|‡¶è‡¶á‡¶ü‡¶ø ‡¶∏‡ßá‡¶≠‡ßá‡¶®\": \"87\",\n",
        "    r\"‡¶Ü‡¶ü‡¶æ‡¶∂‡¶ø|‡¶è‡¶á‡¶ü‡¶ø ‡¶è‡¶á‡¶ü\": \"88\",\n",
        "    r\"‡¶ä‡¶®‡¶®‡¶¨‡ßç‡¶¨‡¶á|‡¶è‡¶á‡¶ü‡¶ø ‡¶®‡¶æ‡¶á‡¶®\": \"89\",\n",
        "\n",
        "    r\"‡¶è‡¶ï‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á|‡¶®‡¶æ‡¶á‡¶®‡¶ü‡¶ø (‡¶ì‡¶Ø‡¶º‡¶æ‡¶®|‡¶ì‡ßü‡¶æ‡¶®)\": \"91\",\n",
        "    r\"‡¶¨‡¶ø‡¶∞‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á|‡¶®‡¶æ‡¶á‡¶®‡¶ü‡¶ø ‡¶ü‡ßÅ\": \"92\",\n",
        "    r\"‡¶§‡¶ø‡¶∞‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á|‡¶®‡¶æ‡¶á‡¶®‡¶ü‡¶ø ‡¶•‡ßç‡¶∞‡¶ø\": \"93\",\n",
        "    r\"‡¶ö‡ßÅ‡¶∞‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á|‡¶®‡¶æ‡¶á‡¶®‡¶ü‡¶ø ‡¶´‡ßã‡¶∞\": \"94\",\n",
        "    r\"‡¶™‡¶Å‡¶ö‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á|‡¶®‡¶æ‡¶á‡¶®‡¶ü‡¶ø ‡¶´‡¶æ‡¶á‡¶≠\": \"95\",\n",
        "    r\"‡¶õ‡¶ø‡ßü‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á|‡¶õ‡¶ø‡¶Ø‡¶º‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á|‡¶®‡¶æ‡¶á‡¶®‡¶ü‡¶ø ‡¶∏‡¶ø‡¶ï‡ßç‡¶∏\": \"96\",\n",
        "    r\"‡¶∏‡¶æ‡¶§‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á|‡¶®‡¶æ‡¶á‡¶®‡¶ü‡¶ø ‡¶∏‡ßá‡¶≠‡ßá‡¶®\": \"97\",\n",
        "    r\"‡¶Ü‡¶ü‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á|‡¶®‡¶æ‡¶á‡¶®‡¶ü‡¶ø ‡¶è‡¶á‡¶ü\": \"98\",\n",
        "    r\"‡¶®‡¶ø‡¶∞‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á|‡¶®‡¶æ‡¶á‡¶®‡¶ü‡¶ø ‡¶®‡¶æ‡¶á‡¶®\": \"99\",\n",
        "\n",
        "             # Replace \"‡¶¨‡¶ø‡¶∂\" and \"‡¶ü‡ßÅ‡¶á‡¶®‡ßç‡¶ü‡¶ø\" with \"20\"\n",
        "    r\"‡¶§‡ßç‡¶∞‡¶ø‡¶∂|‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø\": \"30\",      # Replace \"‡¶§‡ßç‡¶∞‡¶ø‡¶∂\" and \"‡¶•‡¶æ‡¶∞‡ßç‡¶ü‡¶ø\" with \"30\"\n",
        "     r\"‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂|‡¶´‡¶∞‡¶ü‡¶ø\": \"40\",      # Replace \"‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\" and \"‡¶´‡¶∞‡¶ü‡¶ø\" with \"40\"\n",
        "     r\"‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶∂|‡¶´‡¶ø‡¶´‡¶ü‡¶ø\": \"50\",  # Replace \"‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶∂\" and \"‡¶´‡¶ø‡¶´‡¶ü‡¶ø ‡¶´‡ßã‡¶∞\" with \"50\"\n",
        "     r\"‡¶∑‡¶æ‡¶ü|‡¶∏‡¶ø‡¶ï‡ßç‡¶∏‡¶ü‡¶ø\": \"60\",        # Replace \"‡¶∑‡¶æ‡¶ü\" and \"‡¶∏‡¶ø‡¶ï‡ßç‡¶∏‡¶ü‡¶ø\" with \"60\"\n",
        "    r\"‡¶∏‡¶§‡ßç‡¶§‡¶∞|‡¶∏‡ßá‡¶≠‡ßá‡¶®‡ßç‡¶ü‡¶ø\": \"70\",\n",
        "     r\"‡¶®‡¶¨‡ßç‡¶¨‡¶á|‡¶®‡¶æ‡¶á‡¶®‡¶ü‡¶ø\": \"90\",\n",
        "\n",
        "     r\"‡¶ì‡¶Ø‡¶º‡¶æ‡¶®|‡¶ì‡ßü‡¶æ‡¶®\": \"1\" ,      # Replace \"‡¶ì‡ßü‡¶æ‡¶®\", \"‡¶è‡¶ï\", and \"‡¶•‡ßç‡¶∞‡¶ø\" with \"1\"\n",
        "     r\" ‡¶ü‡ßÅ‡•§\": \" 2‡•§\",\n",
        "     r\" ‡¶ü‡ßÅ\\)\": \" 2)\",\n",
        "     r\"‡¶¶‡ßÅ‡¶á\": \"2\",      # Replace \"‡¶ü‡ßÅ\", \"‡¶¶‡ßÅ‡¶á\", \"two\" with \"2\"\n",
        "     r\" ‡¶ü‡ßÅ \": \" 2 \",\n",
        "     r\" ‡¶ü‡ßÅ,\": \" 2,\",\n",
        "     r\"\\(‡¶ü‡ßÅ \": \"(2 \",\n",
        "\n",
        "    r\" ‡¶§‡¶ø‡¶® | ‡¶•‡ßç‡¶∞‡¶ø \": \" 3 \",            # Replace \"‡¶§‡¶ø‡¶®\" and \"‡¶•‡ßç‡¶∞‡¶ø\" with \"3\"\n",
        "    r\" ‡¶ö‡¶æ‡¶∞ | ‡¶´‡ßã‡¶∞ \": \" 4 \",             # Replace \"‡¶ö‡¶æ‡¶∞\" and \"‡¶´‡ßã‡¶∞\" with \"4\"\n",
        "    r\" ‡¶™‡¶æ‡¶Å‡¶ö | ‡¶´‡¶æ‡¶á‡¶≠ \": \" 5 \",           # Replace \"‡¶™‡¶æ‡¶Å‡¶ö\" and \"‡¶´‡¶æ‡¶á‡¶≠\" with \"5\"\n",
        "    r\" ‡¶õ‡ßü| ‡¶õ‡¶Ø‡¶º | ‡¶∏‡¶ø‡¶ï‡ßç‡¶∏ \": \" 6 \",            # Replace \"‡¶õ‡ßü\" and \"‡¶∏‡¶ø‡¶ï‡ßç‡¶∏\" with \"6\"\n",
        "    r\" ‡¶∏‡¶æ‡¶§ | ‡¶∏‡ßá‡¶≠‡ßá‡¶® \": \" 7 \",           # Replace \"‡¶∏‡¶æ‡¶§\" and \"‡¶∏‡ßá‡¶≠‡ßá‡¶®\" with \"7\"\n",
        "    r\" ‡¶Ü‡¶ü | ‡¶è‡¶á‡¶ü \": \" 8 \",              # Replace \"‡¶Ü‡¶ü\" and \"‡¶è‡¶á‡¶ü\" with \"8\"\n",
        "    r\" ‡¶®‡¶æ‡¶á‡¶® \": \" 9 \",             # Replace \"‡¶®‡ßü\" and \"‡¶®‡¶æ‡¶á‡¶®\" with \"9\"\n",
        "\n",
        "    r\" ‡¶§‡¶ø‡¶®,| ‡¶•‡ßç‡¶∞‡¶ø,\": \" 3,\",\n",
        "    r\" ‡¶§‡¶ø‡¶®‡•§| ‡¶•‡ßç‡¶∞‡¶ø‡•§\": \" 3‡•§\",\n",
        "    r\" ‡¶§‡¶ø‡¶®\\)| ‡¶•‡ßç‡¶∞‡¶ø\\)\": \" 3)\",\n",
        "    r\"^‡¶§‡¶ø‡¶® |^‡¶•‡ßç‡¶∞‡¶ø \": \"3 \",\n",
        "    r\"\\(‡¶§‡¶ø‡¶® |\\(‡¶•‡ßç‡¶∞‡¶ø \": \"(3 \",\n",
        "\n",
        "    # 4\n",
        "    r\" ‡¶ö‡¶æ‡¶∞,| ‡¶´‡ßã‡¶∞,\": \" 4,\",\n",
        "    r\" ‡¶ö‡¶æ‡¶∞‡•§| ‡¶´‡ßã‡¶∞‡•§\": \" 4‡•§\",\n",
        "    r\" ‡¶ö‡¶æ‡¶∞\\)| ‡¶´‡ßã‡¶∞\\)\": \" 4)\",\n",
        "    r\"^‡¶ö‡¶æ‡¶∞ |^‡¶´‡ßã‡¶∞ \": \"4 \",\n",
        "    r\"\\(‡¶ö‡¶æ‡¶∞ |\\(‡¶´‡ßã‡¶∞ \": \"(4 \",\n",
        "\n",
        "    # 5\n",
        "    r\" ‡¶™‡¶æ‡¶Å‡¶ö,| ‡¶´‡¶æ‡¶á‡¶≠,\": \" 5,\",\n",
        "    r\" ‡¶™‡¶æ‡¶Å‡¶ö‡•§| ‡¶´‡¶æ‡¶á‡¶≠‡•§\": \" 5‡•§\",\n",
        "    r\" ‡¶™‡¶æ‡¶Å‡¶ö\\)| ‡¶´‡¶æ‡¶á‡¶≠\\)\": \" 5)\",\n",
        "    r\"^‡¶™‡¶æ‡¶Å‡¶ö |^‡¶´‡¶æ‡¶á‡¶≠ \": \"5 \",\n",
        "    r\"\\(‡¶™‡¶æ‡¶Å‡¶ö |\\(‡¶´‡¶æ‡¶á‡¶≠ \": \"(5 \",\n",
        "\n",
        "    # 6\n",
        "    r\" (‡¶õ‡ßü|‡¶õ‡¶Ø‡¶º),| ‡¶∏‡¶ø‡¶ï‡ßç‡¶∏,\": \" 6,\",\n",
        "    r\" (‡¶õ‡ßü|‡¶õ‡¶Ø‡¶º)‡•§| ‡¶∏‡¶ø‡¶ï‡ßç‡¶∏‡•§\": \" 6‡•§\",\n",
        "    r\" (‡¶õ‡ßü|‡¶õ‡¶Ø‡¶º)\\)| ‡¶∏‡¶ø‡¶ï‡ßç‡¶∏\\)\": \" 6)\",\n",
        "    r\"^(‡¶õ‡ßü|‡¶õ‡¶Ø‡¶º) |^‡¶∏‡¶ø‡¶ï‡ßç‡¶∏ \": \"6 \",\n",
        "    r\"\\((‡¶õ‡ßü|‡¶õ‡¶Ø‡¶º) |\\(‡¶∏‡¶ø‡¶ï‡ßç‡¶∏ \": \"(6 \",\n",
        "\n",
        "    # 7\n",
        "    r\" ‡¶∏‡¶æ‡¶§,| ‡¶∏‡ßá‡¶≠‡ßá‡¶®,\": \" 7,\",\n",
        "    r\" ‡¶∏‡¶æ‡¶§‡•§| ‡¶∏‡ßá‡¶≠‡ßá‡¶®‡•§\": \" 7‡•§\",\n",
        "    r\" ‡¶∏‡¶æ‡¶§\\)| ‡¶∏‡ßá‡¶≠‡ßá‡¶®\\)\": \" 7)\",\n",
        "    r\"^‡¶∏‡¶æ‡¶§ |^‡¶∏‡ßá‡¶≠‡ßá‡¶® \": \"7 \",\n",
        "    r\"\\(‡¶∏‡¶æ‡¶§ |\\(‡¶∏‡ßá‡¶≠‡ßá‡¶® \": \"(7 \",\n",
        "\n",
        "    # 8\n",
        "    r\" ‡¶Ü‡¶ü,| ‡¶è‡¶á‡¶ü,\": \" 8,\",\n",
        "    r\" ‡¶Ü‡¶ü‡•§| ‡¶è‡¶á‡¶ü‡•§\": \" 8‡•§\",\n",
        "    r\" ‡¶Ü‡¶ü\\)| ‡¶è‡¶á‡¶ü\\)\": \" 8)\",\n",
        "    r\"^‡¶Ü‡¶ü |^‡¶è‡¶á‡¶ü \": \"8 \",\n",
        "    r\"\\(‡¶Ü‡¶ü |\\(‡¶è‡¶á‡¶ü \": \"(8 \",\n",
        "\n",
        "    # 9\n",
        "    r\" ‡¶®‡¶æ‡¶á‡¶®,\": \" 9,\",\n",
        "    r\" ‡¶®‡¶æ‡¶á‡¶®‡•§\": \" 9‡•§\",\n",
        "    r\" ‡¶®‡¶Ø‡¶º\\)| ‡¶®‡ßü\\)| ‡¶®‡¶æ‡¶á‡¶®\\)\": \" 9)\",\n",
        "    r\"^‡¶®‡¶Ø‡¶º |^‡¶®‡ßü |^‡¶®‡¶æ‡¶á‡¶® \": \"9 \",\n",
        "    r\"\\(‡¶®‡¶Ø‡¶º |\\(‡¶®‡ßü |\\(‡¶®‡¶æ‡¶á‡¶® \": \"(9 \",\n",
        "\n",
        "    # 10\n",
        "   r\" ‡¶¶‡¶∂,| ‡¶ü‡ßá‡¶®,\": \" 10,\",\n",
        "   r\" ‡¶¶‡¶∂‡•§| ‡¶ü‡ßá‡¶®‡•§\": \" 10‡•§\",\n",
        "   r\" ‡¶¶‡¶∂\\)| ‡¶ü‡ßá‡¶®\\)\": \" 10)\",\n",
        "   r\"^‡¶¶‡¶∂ |^‡¶ü‡ßá‡¶® \": \"10 \",\n",
        "   r\" ‡¶¶‡¶∂ | ‡¶ü‡ßá‡¶® \": \" 10 \",\n",
        "   r\"\\(‡¶¶‡¶∂ |\\(‡¶ü‡ßá‡¶® \": \"(10 \",\n",
        "\n",
        "  # 20\n",
        "    r\"‡¶ü‡ßÅ‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶ü‡¶ø|‡¶ü‡ßÅ‡ßü‡ßá‡¶®‡ßç‡¶ü‡¶ø\": \"20\",\n",
        "  r\" ‡¶¨‡¶ø‡¶∂,| ‡¶ü‡ßÅ‡¶á‡¶®‡ßç‡¶ü‡¶ø,\": \" 20,\",\n",
        "  r\" ‡¶¨‡¶ø‡¶∂‡•§| ‡¶ü‡ßÅ‡¶á‡¶®‡ßç‡¶ü‡¶ø‡•§\": \" 20‡•§\",\n",
        "  r\" ‡¶¨‡¶ø‡¶∂\\)| ‡¶ü‡ßÅ‡¶á‡¶®‡ßç‡¶ü‡¶ø\\)\": \" 20)\",\n",
        "  r\"^‡¶¨‡¶ø‡¶∂ |^‡¶ü‡ßÅ‡¶á‡¶®‡ßç‡¶ü‡¶ø \": \"20 \",\n",
        "  r\" ‡¶¨‡¶ø‡¶∂ | ‡¶ü‡ßÅ‡¶á‡¶®‡ßç‡¶ü‡¶ø \": \" 20 \",\n",
        "  r\"\\(‡¶¨‡¶ø‡¶∂ |\\(‡¶ü‡ßÅ‡¶á‡¶®‡ßç‡¶ü‡¶ø \": \"(20 \",\n",
        "\n",
        "    # 80\n",
        "    r\" ‡¶Ü‡¶∂‡¶ø,| ‡¶è‡¶á‡¶ü‡¶ø,\": \" 80,\",\n",
        "    r\" ‡¶Ü‡¶∂‡¶ø‡•§| ‡¶è‡¶á‡¶ü‡¶ø‡•§\": \" 80‡•§\",\n",
        "    r\" ‡¶Ü‡¶∂‡¶ø\\)| ‡¶è‡¶á‡¶ü‡¶ø\\)\": \" 80)\",\n",
        "    r\"^‡¶Ü‡¶∂‡¶ø |^‡¶è‡¶á‡¶ü‡¶ø \": \"80 \",\n",
        "    r\" ‡¶Ü‡¶∂‡¶ø | ‡¶è‡¶á‡¶ü‡¶ø \": \" 80 \",\n",
        "    r\"\\(‡¶Ü‡¶∂‡¶ø |\\(‡¶è‡¶á‡¶ü‡¶ø \": \"(80 \",\n",
        "\n",
        "    r\"‡¶ú‡¶ø‡¶∞‡ßã\": \"0\",\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    r\"‡ß¶\": \"0\",\n",
        "    r\"‡ßß\": \"1\",\n",
        "    r\"‡ß®\": \"2\",\n",
        "    r\"‡ß©\": \"3\",\n",
        "    r\"‡ß™\": \"4\",\n",
        "    r\"‡ß´\": \"5\",\n",
        "    r\"‡ß¨\": \"6\",\n",
        "    r\"‡ß≠\": \"7\",\n",
        "    r\"‡ßÆ\": \"8\",\n",
        "    r\"‡ßØ\": \"9\",\n",
        "\n",
        "    r\" ‡¶è \": \" a \",                 # Replace \"‡¶è\" with \"a\"\n",
        "    r\" ‡¶¨‡¶ø \": \" b \",                # Replace \"‡¶¨‡¶ø\" with \"b\"\n",
        "    r\" ‡¶∏‡¶ø \": \" c \",                # Replace \"‡¶∏‡¶ø\" with \"c\"\n",
        "    r\" ‡¶°‡¶ø \": \" d \",                # Replace \"‡¶°‡¶ø\" with \"d\"\n",
        "    r\" ‡¶á \": \" e \",                 # Replace \"‡¶á\" with \"e\"\n",
        "    r\" ‡¶è‡¶´ \": \" f \",                # Replace \"‡¶è‡¶´\" with \"f\"\n",
        "    r\" ‡¶ú‡¶ø \": \" g \",                # Replace \"‡¶ú‡¶ø\" with \"g\"\n",
        "    r\" ‡¶è‡¶á‡¶ö \": \" h \",               # Replace \"‡¶è‡¶á‡¶ö\" with \"h\"\n",
        "    r\" ‡¶Ü‡¶á \": \" i \",                # Replace \"‡¶Ü‡¶á\" with \"i\"\n",
        "    r\" ‡¶ú‡ßá \": \" j \",                # Replace \"‡¶ú‡ßá\" with \"j\"\n",
        "    r\" ‡¶ï‡ßá \": \" k \",                # Replace \"‡¶ï‡ßá\" with \"k\"\n",
        "    r\" ‡¶è‡¶≤ \": \" l \",                # Replace \"‡¶è‡¶≤\" with \"l\"\n",
        "    r\" ‡¶è‡¶Æ \": \" m \",                # Replace \"‡¶è‡¶Æ\" with \"m\"\n",
        "    r\" ‡¶è‡¶® \": \" n \",                # Replace \"‡¶è‡¶®\" with \"n\"\n",
        "   # r\"‡¶ì\": \"o\",                 # Replace \"‡¶ì\" with \"o\"\n",
        "    r\" ‡¶™‡¶ø \": \" p \",                # Replace \"‡¶™‡¶ø\" with \"p\"\n",
        "    r\" ‡¶ï‡¶ø‡¶â \": \" q \",               # Replace \"‡¶ï‡¶ø‡¶â\" with \"q\"\n",
        "    #r\"‡¶Ü‡¶∞\": \"r\",                # Replace \"‡¶Ü‡¶∞\" with \"r\"\n",
        "    r\" ‡¶è‡¶∏ \": \" s \",                # Replace \"‡¶è‡¶∏\" with \"s\"\n",
        "    r\" ‡¶ü‡¶ø \": \" t \",                # Replace \"‡¶ü‡¶ø\" with \"t\"\n",
        "    r\" ‡¶á‡¶â \": \" u \",                # Replace \"‡¶á‡¶â\" with \"u\"\n",
        "    r\" ‡¶≠‡¶ø \": \" v \",                # Replace \"‡¶≠‡¶ø\" with \"v\"\n",
        "    r\" (‡¶°‡¶¨‡ßç‡¶≤‡¶ø‡¶â‡•§‡¶°‡¶æ‡¶¨‡¶≤‡¶ø‡¶â) \": \" w \",            # Replace \"‡¶°‡¶¨‡ßç‡¶≤‡¶ø‡¶â\" with \"w\"\n",
        "    r\" ‡¶è‡¶ï‡ßç‡¶∏ \": \" x \",              # Replace \"‡¶è‡¶ï‡ßç‡¶∏\" with \"x\"\n",
        "    r\" ‡¶ì‡¶Ø‡¶º‡¶æ‡¶á | ‡¶ì‡ßü‡¶æ‡¶á \": \"y\",             # Replace \"‡¶ì‡ßü‡¶æ‡¶á\" with \"y\"\n",
        "    r\" ‡¶ú‡ßá‡¶° \": \" z \",               # Replace \"‡¶ú‡ßá‡¶°\" with \"z\"\n",
        "\n",
        "\n",
        "    #r\"^‡¶è \": \"a \",                 # Replace \"‡¶è\" with \"a\"\n",
        "    r\"^‡¶¨‡¶ø \": \"b \",                # Replace \"‡¶¨‡¶ø\" with \"b\"\n",
        "    r\"^‡¶∏‡¶ø \": \"c \",                # Replace \"‡¶∏‡¶ø\" with \"c\"\n",
        "    r\"^‡¶°‡¶ø \": \"d \",                # Replace \"‡¶°‡¶ø\" with \"d\"\n",
        "    r\"^‡¶á \": \"e \",                 # Replace \"‡¶á\" with \"e\"\n",
        "    r\"^‡¶è‡¶´ \": \"f \",                # Replace \"‡¶è‡¶´\" with \"f\"\n",
        "    r\"^‡¶ú‡¶ø \": \"g \",                # Replace \"‡¶ú‡¶ø\" with \"g\"\n",
        "    r\"^‡¶è‡¶á‡¶ö \": \"h \",               # Replace \"‡¶è‡¶á‡¶ö\" with \"h\"\n",
        "    r\"^‡¶Ü‡¶á \": \"i \",                # Replace \"‡¶Ü‡¶á\" with \"i\"\n",
        "    r\"^‡¶ú‡ßá \": \"j \",                # Replace \"‡¶ú‡ßá\" with \"j\"\n",
        "    r\"^‡¶ï‡ßá \": \"k \",                # Replace \"‡¶ï‡ßá\" with \"k\"\n",
        "    r\"^‡¶è‡¶≤ \": \"l \",                # Replace \"‡¶è‡¶≤\" with \"l\"\n",
        "    r\"^‡¶è‡¶Æ \": \"m \",                # Replace \"‡¶è‡¶Æ\" with \"m\"\n",
        "    r\"^‡¶è‡¶® \": \"n \",                # Replace \"‡¶è‡¶®\" with \"n\"\n",
        "   # r\"‡¶ì\": \"o\",                 # Replace \"‡¶ì\" with \"o\"\n",
        "    r\"^‡¶™‡¶ø \": \"p \",                # Replace \"‡¶™‡¶ø\" with \"p\"\n",
        "    r\"^‡¶ï‡¶ø‡¶â \": \"q \",               # Replace \"‡¶ï‡¶ø‡¶â\" with \"q\"\n",
        "    #r\"‡¶Ü‡¶∞\": \"r\",                # Replace \"‡¶Ü‡¶∞\" with \"r\"\n",
        "    r\"^‡¶è‡¶∏ \": \"s \",                # Replace \"‡¶è‡¶∏\" with \"s\"\n",
        "    r\"^‡¶ü‡¶ø \": \"t \",                # Replace \"‡¶ü‡¶ø\" with \"t\"\n",
        "    r\"^‡¶á‡¶â \": \"u \",                # Replace \"‡¶á‡¶â\" with \"u\"\n",
        "    r\"^‡¶≠‡¶ø \": \"v \",                # Replace \"‡¶≠‡¶ø\" with \"v\"\n",
        "    r\"^(‡¶°‡¶¨‡ßç‡¶≤‡¶ø‡¶â‡•§‡¶°‡¶æ‡¶¨‡¶≤‡¶ø‡¶â) \": \"w \",            # Replace \"‡¶°‡¶¨‡ßç‡¶≤‡¶ø‡¶â\" with \"w\"\n",
        "    r\"^‡¶è‡¶ï‡ßç‡¶∏ \": \"x \",              # Replace \"‡¶è‡¶ï‡ßç‡¶∏\" with \"x\"\n",
        "    r\"^‡¶ì‡¶Ø‡¶º‡¶æ‡¶á |^‡¶ì‡ßü‡¶æ‡¶á \": \"y \",              # Replace \"‡¶ì‡ßü‡¶æ‡¶á\" with \"y\"\n",
        "    r\"^‡¶ú‡ßá‡¶° \": \"z \",               # Replace \"‡¶ú‡ßá‡¶°\" with \"z\"\n",
        "\n",
        "    r\" ‡¶è,\": \" a,\",                 # Replace \"‡¶è\" with \"a\"\n",
        "    r\" ‡¶¨‡¶ø,\": \" b,\",                # Replace \"‡¶¨‡¶ø\" with \"b\"\n",
        "    r\" ‡¶∏‡¶ø,\": \" c,\",                # Replace \"‡¶∏‡¶ø\" with \"c\"\n",
        "    r\" ‡¶°‡¶ø,\": \" d,\",                # Replace \"‡¶°‡¶ø\" with \"d\"\n",
        "    r\" ‡¶á,\": \" e,\",                 # Replace \"‡¶á\" with \"e\"\n",
        "    r\" ‡¶è‡¶´,\": \" f,\",                # Replace \"‡¶´‡¶ø\" with \"f\"\n",
        "    r\" ‡¶ú‡¶ø,\": \" g,\",                # Replace \"‡¶ó‡¶ø\" with \"g\"\n",
        "    r\" ‡¶è‡¶á‡¶ö,\": \" h,\",               # Replace \"‡¶è‡¶á‡¶ö\" with \"h\"\n",
        "    r\" ‡¶Ü‡¶á,\": \" i,\",                # Replace \"‡¶Ü‡¶á\" with \"i\"\n",
        "    r\" ‡¶ú‡ßá,\": \" j,\",                # Replace \"‡¶ú‡ßá\" with \"j\"\n",
        "    r\" ‡¶ï‡ßá,\": \" k,\",                # Replace \"‡¶ï‡ßá\" with \"k\"\n",
        "    r\" ‡¶è‡¶≤,\": \" l,\",                # Replace \"‡¶è‡¶≤\" with \"l\"\n",
        "    r\" ‡¶è‡¶Æ,\": \" m,\",                # Replace \"‡¶è‡¶Æ\" with \"m\"\n",
        "    r\" ‡¶è‡¶®,\": \" n,\",                # Replace \"‡¶è‡¶®\" with \"n\"\n",
        "    #r\" ‡¶ì,\": \" o,\",                 # Replace \"‡¶ì\" with \"o\"\n",
        "    r\" ‡¶™‡¶ø,\": \" p,\",                # Replace \"‡¶™‡¶ø\" with \"p\"\n",
        "    r\" ‡¶ï‡¶ø‡¶â,\": \" q,\",               # Replace \"‡¶ï‡¶ø‡¶â\" with \"q\"\n",
        "    #r\" ‡¶Ü‡¶∞,\": \" r,\",                # Replace \"‡¶Ü‡¶∞\" with \"r\"\n",
        "    r\" ‡¶è‡¶∏,\": \" s,\",                # Replace \"‡¶è‡¶∏\" with \"s\"\n",
        "    r\" ‡¶ü‡¶ø,\": \" t,\",                # Replace \"‡¶ü‡¶ø\" with \"t\"\n",
        "    r\" ‡¶á‡¶â,\": \" u,\",                # Replace \"‡¶á‡¶â\" with \"u\"\n",
        "    r\" ‡¶≠‡¶ø,\": \" v,\",                # Replace \"‡¶≠‡¶ø\" with \"v\"\n",
        "    r\" (‡¶°‡¶¨‡ßç‡¶≤‡¶ø‡¶â‡•§‡¶°‡¶æ‡¶¨‡¶≤‡¶ø‡¶â),\": \" w,\",            # Replace \"‡¶°‡¶¨‡ßç‡¶≤‡¶ø‡¶â\" with \"w\"\n",
        "    r\" ‡¶è‡¶ï‡ßç‡¶∏,\": \" x,\",              # Replace \"‡¶è‡¶ï‡ßç‡¶∏\" with \"x\"\n",
        "    r\" ‡¶ì‡¶Ø‡¶º‡¶æ‡¶á,| ‡¶ì‡ßü‡¶æ‡¶á,\": \" y,\",              # Replace \"‡¶ì‡ßü‡¶æ‡¶á\" with \"y\"\n",
        "    r\" ‡¶ú‡ßá‡¶°,\": \" z,\",               # Replace \"‡¶ú‡ßá‡¶°\" with \"z\"\n",
        "\n",
        "    r\" ‡¶è‡•§\": \" a‡•§\",                 # Replace \"‡¶è\" with \"a\"\n",
        "    r\" ‡¶¨‡¶ø‡•§\": \" b‡•§\",                # Replace \"‡¶¨‡¶ø\" with \"b\"\n",
        "    r\" ‡¶∏‡¶ø‡•§\": \" c‡•§\",                # Replace \"‡¶∏‡¶ø\" with \"c\"\n",
        "    r\" ‡¶°‡¶ø‡•§\": \" d‡•§\",\n",
        "    r\" ‡¶á‡•§\": \" e‡•§\",\n",
        "    r\" ‡¶è‡¶´‡•§\": \" f‡•§\",\n",
        "    r\" ‡¶ú‡¶ø‡•§\": \" g‡•§\",\n",
        "    r\" ‡¶è‡¶á‡¶ö‡•§\": \" h‡•§\",\n",
        "    r\" ‡¶Ü‡¶á‡•§\": \" i‡•§\",\n",
        "    r\" ‡¶ú‡ßá‡•§\": \" j‡•§\",\n",
        "    r\" ‡¶ï‡ßá‡•§\": \" k‡•§\",\n",
        "    r\" ‡¶è‡¶≤‡•§\": \" l‡•§\",\n",
        "    r\" ‡¶è‡¶Æ‡•§\": \" m‡•§\",\n",
        "    r\" ‡¶è‡¶®‡•§\": \" n‡•§\",\n",
        "    r\" ‡¶ì‡•§\": \" o‡•§\",\n",
        "    r\" ‡¶™‡¶ø‡•§\": \" p‡•§\",\n",
        "    r\" ‡¶ï‡¶ø‡¶â‡•§\": \" q‡•§\",\n",
        "    r\" ‡¶Ü‡¶∞‡•§\": \" r‡•§\",\n",
        "    r\" ‡¶è‡¶∏‡•§\": \" s‡•§\",\n",
        "    r\" ‡¶ü‡¶ø‡•§\": \" t‡•§\",\n",
        "    r\" ‡¶á‡¶â‡•§\": \" u‡•§\",\n",
        "    r\" ‡¶≠‡¶ø‡•§\": \" v‡•§\",\n",
        "    r\" (‡¶°‡¶¨‡ßç‡¶≤‡¶ø‡¶â‡•§‡¶°‡¶æ‡¶¨‡¶≤‡¶ø‡¶â)‡•§\": \" w‡•§\",\n",
        "    r\" ‡¶è‡¶ï‡ßç‡¶∏‡•§\": \" x‡•§\",\n",
        "    r\" ‡¶ì‡¶Ø‡¶º‡¶æ‡¶á‡•§| ‡¶ì‡ßü‡¶æ‡¶á‡•§\": \" y‡•§\",\n",
        "    r\" ‡¶ú‡ßá‡¶°‡•§\": \" z‡•§\",\n",
        "\n",
        "    r\" ‡¶è\\)\": \" a)\",                 # Replace \"‡¶è\" with \"a\"\n",
        "    r\" ‡¶¨‡¶ø\\)\": \" b)\",                # Replace \"‡¶¨‡¶ø\" with \"b\"\n",
        "    r\" ‡¶∏‡¶ø\\)\": \" c)\",                # Replace \"‡¶∏‡¶ø\" with \"c\"\n",
        "    r\" ‡¶°‡¶ø\\)\": \" d)\",\n",
        "    r\" ‡¶á\\)\": \" e)\",\n",
        "    r\" ‡¶è‡¶´\\)\": \" f)\",\n",
        "    r\" ‡¶ú‡¶ø\\)\": \" g)\",\n",
        "    r\" ‡¶è‡¶á‡¶ö\\)\": \" h)\",\n",
        "    r\" ‡¶Ü‡¶á\\)\": \" i)\",\n",
        "    r\" ‡¶ú‡ßá\\)\": \" j)\",\n",
        "    r\" ‡¶ï‡ßá\\)\": \" k)\",\n",
        "    r\" ‡¶è‡¶≤\\)\": \" l)\",\n",
        "    r\" ‡¶è‡¶Æ\\)\": \" m)\",\n",
        "    r\" ‡¶è‡¶®\\)\": \" n)\",\n",
        "    r\" ‡¶ì\\)\": \" o)\",\n",
        "    r\" ‡¶™‡¶ø\\)\": \" p)\",\n",
        "    r\" ‡¶ï‡¶ø‡¶â\\)\": \" q)\",\n",
        "    r\" ‡¶Ü‡¶∞\\)\": \" r)\",\n",
        "    r\" ‡¶è‡¶∏\\)\": \" s)\",\n",
        "    r\" ‡¶ü‡¶ø\\)\": \" t)\",\n",
        "    r\" ‡¶á‡¶â\\)\": \" u)\",\n",
        "    r\" ‡¶≠‡¶ø\\)\": \" v)\",\n",
        "    r\" (‡¶°‡¶¨‡ßç‡¶≤‡¶ø‡¶â‡•§‡¶°‡¶æ‡¶¨‡¶≤‡¶ø‡¶â)\\)\": \" w)\",\n",
        "    r\" ‡¶è‡¶ï‡ßç‡¶∏\\)\": \" x)\",\n",
        "    r\" ‡¶ì‡¶Ø‡¶º‡¶æ‡¶á\\)| ‡¶ì‡ßü‡¶æ‡¶á\\)\": \" y)\",\n",
        "    r\" ‡¶ú‡ßá‡¶°\\)\": \" z)\",\n",
        "\n",
        "\n",
        "    r\"\\(‡¶è \": \"\\(a \",\n",
        "    r\"\\(‡¶¨‡¶ø \": \"\\(b \",\n",
        "r\"\\(‡¶∏‡¶ø \": \"\\(c \",\n",
        "r\"\\(‡¶°‡¶ø \": \"\\(d \",\n",
        "r\"\\(‡¶á \": \"\\(e \",\n",
        "r\"\\(‡¶è‡¶´ \": \"\\(f \",\n",
        "r\"\\(‡¶ú‡¶ø \": \"\\(g \",\n",
        "r\"\\(‡¶è‡¶á‡¶ö \": \"\\(h \",\n",
        "r\"\\(‡¶Ü‡¶á \": \"\\(i \",\n",
        "r\"\\(‡¶ú‡ßá \": \"\\(j \",\n",
        "r\"\\(‡¶ï‡ßá \": \"\\(k \",\n",
        "r\"\\(‡¶è‡¶≤ \": \"\\(l \",\n",
        "r\"\\(‡¶è‡¶Æ \": \"\\(m \",\n",
        "r\"\\(‡¶è‡¶® \": \"\\(n \",\n",
        "r\"\\(‡¶ì \": \"\\(o \",\n",
        "r\"\\(‡¶™‡¶ø \": \"\\(p \",\n",
        "r\"\\(‡¶ï‡¶ø‡¶â \": \"\\(q \",\n",
        "r\"\\(‡¶Ü‡¶∞ \": \"\\(r \",\n",
        "r\"\\(‡¶è‡¶∏ \": \"\\(s \",\n",
        "r\"\\(‡¶ü‡¶ø \": \"\\(t \",\n",
        "r\"\\(‡¶á‡¶â \": \"\\(u \",\n",
        "r\"\\(‡¶≠‡¶ø \": \"\\(v \",\n",
        "r\"\\((‡¶°‡¶¨‡ßç‡¶≤‡¶ø‡¶â‡•§‡¶°‡¶æ‡¶¨‡¶≤‡¶ø‡¶â) \": \"\\(w \",\n",
        "r\"\\(‡¶è‡¶ï‡ßç‡¶∏ \": \"\\(x \",\n",
        "r\"\\(‡¶ì‡¶Ø‡¶º‡¶æ‡¶á |\\(‡¶ì‡ßü‡¶æ‡¶á \": \"\\(y \",\n",
        "r\"\\(‡¶ú‡ßá‡¶° \": \"\\(z \",\n",
        "\n",
        "    r\"‡¶°‡¶ø‡¶è‡¶∏‡¶°‡¶ø\": \"dsd\",\n",
        "    r\"‡¶¨‡¶ø‡¶∏‡¶ø‡¶°‡¶ø\": \"bcd\",\n",
        "    r\"‡¶è‡¶∏‡¶ø‡¶°‡¶ø\": \"acd\",\n",
        "    r\"‡¶è‡¶¨‡¶ø‡¶∏‡¶ø\": \"abc\",\n",
        "    r\"‡¶è‡¶∏‡¶ü‡¶ø‡¶™‡¶ø\": \"STP\",\n",
        "    r\"‡¶è‡¶á‡¶ö‡¶è‡¶∏‡¶∏‡¶ø\": \"HSC\",\n",
        "    r\"‡¶è‡¶∏‡¶è‡¶∏‡¶∏‡¶ø\": \"SSC\",\n",
        "    r\"‡¶™‡¶ø‡¶™‡¶ø‡¶è‡¶Æ\": \"ppm\",\n",
        "    r\"‡¶™‡¶ø‡¶™‡¶ø‡¶¨‡¶ø\": \"ppb\",\n",
        "    r\"‡¶è‡¶∏‡¶™‡¶ø\": \"sp\",\n",
        "    r\"‡¶è‡¶¨‡¶ø\": \"ab\",\n",
        "    r\"‡¶è‡¶°‡¶ø\": \"ad\",\n",
        "    r\"‡¶è‡¶∏‡¶∏‡¶ø\": \"sc\",\n",
        "    r\"‡¶è‡¶∏‡¶è\": \"sa\",\n",
        "    r\"‡¶è‡¶∏‡¶ï‡¶ø‡¶â\": \"sq\",\n",
        "    r\"‡¶è‡¶∏‡¶Ü‡¶∞\": \"sr\",\n",
        "    r\"‡¶è‡¶∏‡¶ü‡¶ø\": \"st\",\n",
        "    r\"‡¶è‡¶∏‡¶è‡¶Æ\": \"sm\",\n",
        "    r\"‡¶è‡¶∏‡¶è‡¶®\": \"sn\",\n",
        "    r\"‡¶è‡¶∏‡¶ì\": \"so\",\n",
        "    r\"‡¶è‡¶∏‡¶°‡¶ø\": \"sd\",\n",
        "    r\"‡¶è‡¶∏‡¶á\": \"se\",\n",
        "    r\"‡¶è‡¶∏‡¶ø \": \"ac \",\n",
        "    r\"‡¶¨‡¶ø‡¶∏‡¶ø\": \"bc\",\n",
        "    r\"‡¶™‡¶ø‡¶ï‡¶ø‡¶â\": \"pq\",\n",
        "    r\"‡¶™‡¶ø‡¶Ü‡¶∞\": \"pr\",\n",
        "    r\"‡¶™‡¶ø‡¶è‡¶∏\": \"ps\",\n",
        "    r\"‡¶™‡¶ø‡¶è‡¶Æ\": \"pm\",\n",
        "    r\"‡¶™‡¶ø‡¶è‡¶á‡¶ö\": \"pH\",\n",
        "    r\"‡¶™‡¶ø‡¶è\": \"pa\",\n",
        "\n",
        "    r\"‡¶™‡¶ø‡¶¨‡¶ø\": \"pb\",\n",
        "    r\"‡¶™‡¶ø‡¶ú‡ßá‡¶°\": \"pz\",\n",
        "    r\"‡¶è‡¶ï‡ßç‡¶∏‡¶ì‡¶Ø‡¶º‡¶æ‡¶á|‡¶è‡¶ï‡ßç‡¶∏‡¶ì‡ßü‡¶æ‡¶á\": \"xy\",\n",
        "    r\"‡¶è‡¶ï‡ßç‡¶∏‡¶ì\": \"xo\",\n",
        "    r\"‡¶è‡¶ï‡ßç‡¶∏‡¶ú‡ßá‡¶°\": \"xz\",\n",
        "    r\"‡¶ì‡¶Ø‡¶º‡¶æ‡¶á‡¶ú‡ßá‡¶°|‡¶ì‡ßü‡¶æ‡¶á‡¶ú‡ßá‡¶°\": \"yz\",\n",
        "    r\"‡¶ì‡¶è\": \"oa\",\n",
        "    r\"‡¶ì‡¶¨‡¶ø\": \"ob\",\n",
        "    r\"‡¶ì‡¶∏‡¶ø\": \"oc\",\n",
        "    r\"‡¶ì‡¶°‡¶ø\": \"od\",\n",
        "    r\"‡¶ì‡¶™‡¶ø\": \"op\",\n",
        "    r\"‡¶ì‡¶ï‡¶ø‡¶â\": \"oq\",\n",
        "    r\"‡¶ì‡¶Ü‡¶∞\": \"or\",\n",
        "    r\"‡¶ì‡¶è‡¶∏\": \"os\",\n",
        "    r\"‡¶á‡¶â‡¶≠‡¶ø\": \"uv\",\n",
        "    r\"‡¶Ü‡¶á‡¶Ü‡¶∞\": \"IR\",\n",
        "    r\"‡¶á‡¶è\": \"ea\",\n",
        "    r\"‡¶á‡¶¨‡¶ø\": \"eb\",\n",
        "    r\"‡¶ï‡ßá‡¶™‡¶ø\": \"kp\",\n",
        "    r\"‡¶ï‡ßá‡¶∏‡¶ø\": \"kc\",\n",
        "    r\"‡¶è‡¶Æ‡¶∏‡¶ø‡¶ï‡¶ø‡¶â\": \"MCQ\",\n",
        "    #r\"‡¶∏‡¶ø‡¶ï‡¶ø‡¶â\": \"CQ\",\n",
        "    r\"‡¶°‡¶ø‡¶è‡¶®\": \"dn\",\n",
        "    r\"‡¶°‡¶ø‡¶è‡¶∏\": \"ds\",\n",
        "    r\"‡¶°‡¶ø‡¶ü‡¶ø\": \"dt\",\n",
        "    r\"‡¶°‡¶ø‡¶è‡¶ï‡ßç‡¶∏\": \"dx\",\n",
        "    r\"‡¶°‡¶ø‡¶ì‡¶Ø‡¶º‡¶æ‡¶á|‡¶°‡¶ø‡¶ì‡ßü‡¶æ‡¶á\": \"dy\",\n",
        "    r\"‡¶°‡¶ø‡¶ú‡ßá‡¶°\": \"dz\",\n",
        "    #r\"‡¶∏‡¶ø‡¶™‡¶ø\": \"Cp\",\n",
        "    #r\"‡¶∏‡¶ø‡¶≠‡¶ø\": \"Cv\",\n",
        "    r\"‡¶∏‡¶ø‡¶°‡¶ø\": \"cd\",\n",
        "    r\"‡¶∏‡¶ø‡¶∏‡¶ø\": \"cc\",\n",
        "    r\"‡¶°‡¶ø‡¶°‡¶ø\": \"dd\",\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    r\"‡¶ï‡¶∏‡ßá‡¶ï|‡¶ï‡ßã‡¶∏‡ßá‡¶ï\": \"cosec\",  # Replace \"‡¶ï‡¶∏‡ßá‡¶ï\" or \"‡¶ï‡ßã‡¶∏‡ßá‡¶ï\" with \"cosec\"\n",
        "    r\" ‡¶≤‡¶ó \": \" log \",              # Replace \"‡¶≤‡¶ó\" with \"log\"\n",
        "    r\"‡¶∏‡¶æ‡¶á‡¶® \": \"sin \",            # Replace \"‡¶∏‡¶æ‡¶á‡¶®\" with \"sin\"\n",
        "    r\" ‡¶ï‡¶∏ ‡•§ ‡¶ï‡¶ú \": \" cos \",          # Replace \"‡¶ï‡¶ú\" or \"‡¶ï‡¶∏\" with \"cos\"\n",
        "    r\"‡¶∏‡ßá‡¶ï \": \"sec \",            # Replace \"‡ßç‡¶∏‡ßá‡¶ï\" with \"sec\"\n",
        "\n",
        "    r\"‡¶ï‡¶ü \": \"cot \",              # Replace \"‡¶ï‡¶ü\" with \"cot\"\n",
        "    r\"‡¶á‡¶®‡¶ü‡ßÅ \": \"X \",             # Replace \"‡¶á‡¶®‡¶ü‡ßÅ\" with \"X\"\n",
        "\n",
        "\n",
        "    r\"‡¶Æ‡¶ø‡¶â(?=\\W|$)\": \"Œº\",               # \"‡¶Æ‡¶ø‡¶â\" ‚Üí \"Œº\"\n",
        "    r\"‡¶°‡ßá‡¶≤‡¶ü‡¶æ(?=\\W|$)\": \"·∫ü\",             # \"‡¶°‡ßá‡¶≤‡¶ü‡¶æ\" ‚Üí \"·∫ü\"\n",
        "    r\"‡¶≤‡ßç‡¶Ø‡¶æ‡¶Æ‡¶°‡¶æ(?=\\W|$)\": \"Œª\",           # \"‡¶≤‡ßç‡¶Ø‡¶æ‡¶Æ‡¶°‡¶æ\" ‚Üí \"Œª\"\n",
        "    r\"‡¶•‡¶ø‡¶ü‡¶æ|‡¶•‡ßá‡¶ü‡¶æ(?=\\W|$)\": \"Œ∏\",         # \"‡¶•‡¶ø‡¶ü‡¶æ\" or \"‡¶•‡ßá‡¶ü‡¶æ\" ‚Üí \"Œ∏\"\n",
        "\n",
        "    #r\"‡¶™‡¶æ‡¶á(?=\\W|$)\": \"œÄ\",\n",
        "    r\"‡¶∏‡¶ø‡¶ó‡¶Æ‡¶æ(?=\\W|$)\": \"œÉ\",      # Sigma\n",
        "   # r\" ‡¶∞‡ßã(?=\\W|$)\": \" œÅ\",         # Rho\n",
        "    r\"‡¶ì‡¶Æ‡ßá‡¶ó‡¶æ(?=\\W|$)\": \"œâ\",      # Omega\n",
        "    r\" ‡¶´‡¶æ‡¶á(?=\\W|$)\": \" œÜ\",        # Phi\n",
        "    r\"‡¶Ü‡¶≤‡¶´‡¶æ(?=\\W|$)\": \"Œ±\",        # ‡¶Ü‡¶≤‡¶´‡¶æ ‚Üí Œ±\n",
        "    r\"‡¶¨‡¶ø‡¶ü‡¶æ|‡¶¨‡ßá‡¶ü‡¶æ(?=\\W|$)\": \"Œ≤\",    # ‡¶¨‡¶ø‡¶ü‡¶æ/‡¶¨‡ßá‡¶ü‡¶æ ‚Üí Œ≤\n",
        "    r\"‡¶ó‡¶æ‡¶Æ‡¶æ(?=\\W|$)\": \"Œ≥\",         # ‡¶ó‡¶æ‡¶Æ‡¶æ ‚Üí Œ≥\n",
        "\n",
        "\n",
        "    r\"‡¶®‡ßç‡¶Ø‡¶æ‡¶®‡ßã‡¶Æ‡¶ø‡¶ü‡¶æ‡¶∞(?=\\W|$)\": \"nm\",\n",
        "    r\"‡¶Æ‡¶ø‡¶≤‡¶ø‡¶Æ‡¶ø‡¶ü‡¶æ‡¶∞(?=\\W|$)\": \"mm\",\n",
        "    r\"‡¶∏‡ßá‡¶®‡ßç‡¶ü‡¶ø‡¶Æ‡¶ø‡¶ü‡¶æ‡¶∞(?=\\W|$)\": \"cm\",\n",
        "    r\" ‡¶Æ‡¶ø‡¶ü‡¶æ‡¶∞(?=\\W|$)\": \" m\",\n",
        "    r\"‡¶ï‡¶ø‡¶≤‡ßã‡¶Æ‡¶ø‡¶ü‡¶æ‡¶∞(?=\\W|$)\": \"km\",\n",
        "\n",
        "    r\"‡¶∏‡ßá‡¶ï‡ßá‡¶®‡ßç‡¶°(?=\\W|$)\": \"s\",\n",
        "    r\"‡¶Æ‡¶ø‡¶≤‡¶ø‡¶∏‡ßá‡¶ï‡ßá‡¶®‡ßç‡¶°(?=\\W|$)\": \"ms\",\n",
        "    r\"‡¶®‡ßç‡¶Ø‡¶æ‡¶®‡ßã‡¶∏‡ßá‡¶ï‡ßá‡¶®‡ßç‡¶°(?=\\W|$)\": \"ns\",\n",
        "    r\"‡¶Æ‡¶æ‡¶á‡¶ï‡ßç‡¶∞‡ßã‡¶∏‡ßá‡¶ï‡ßá‡¶®‡ßç‡¶°(?=\\W|$)\": \"¬µs\",\n",
        "\n",
        "    r\"‡¶ï‡ßá‡¶ú‡¶ø(?=\\W|$)\": \"kg\",             # \"‡¶ï‡ßá‡¶ú‡¶ø\" ‚Üí \"kg\"\n",
        "    r\" ‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ(?=\\W|$)\": \" g\",\n",
        "    r\"‡¶Æ‡¶ø‡¶≤‡¶ø‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ(?=\\W|$)\": \"mg\",\n",
        "\n",
        "     r\"‡¶Æ‡¶æ‡¶á‡¶®‡¶æ‡¶∏(?=\\W|$)\": \"-\",\n",
        "     r\"‡¶™‡ßç‡¶≤‡¶æ‡¶∏(?=\\W|$)\": \"+\",\n",
        "     r\"‡¶ï‡¶Æ‡¶æ\": \",\",\n",
        "\n",
        "    r\"‡¶ï‡¶æ‡¶∞‡ßç‡¶¨‡¶® ‡¶°‡¶æ‡¶á ‡¶Ö‡¶ï‡ßç‡¶∏‡¶æ‡¶á‡¶°(?=\\W|$)| ‡¶ï‡¶æ‡¶∞‡ßç‡¶¨‡¶® ‡¶°‡¶æ‡¶á‡¶Ö‡¶ï‡ßç‡¶∏‡¶æ‡¶á‡¶°(?=\\W|$)\": \"CO‚ÇÇ\",\n",
        "    r\"‡¶Ö‡¶ï‡ßç‡¶∏‡¶ø‡¶ú‡ßá‡¶® ‡¶ó‡ßç‡¶Ø‡¶æ‡¶∏(?=\\W|$)\": \"O‚ÇÇ\",\n",
        "    r\"‡¶π‡¶æ‡¶á‡¶°‡ßç‡¶∞‡ßã‡¶ú‡ßá‡¶® ‡¶ó‡ßç‡¶Ø‡¶æ‡¶∏(?=\\W|$)\": \"H‚ÇÇ\",\n",
        "    r\"‡¶®‡¶æ‡¶á‡¶ü‡ßç‡¶∞‡ßã‡¶ú‡ßá‡¶® ‡¶ó‡ßç‡¶Ø‡¶æ‡¶∏(?=\\W|$)\": \"N‚ÇÇ\",\n",
        "    r\"‡¶ì‡¶ú‡ßã‡¶®(?=\\W|$)\": \"O‚ÇÉ\",\n",
        "    r\"‡¶Ö‡ßç‡¶Ø‡¶æ‡¶Æ‡ßã‡¶®‡¶ø‡¶Ø‡¶º‡¶æ (?=\\W|$)|‡¶Ö‡ßç‡¶Ø‡¶æ‡¶Æ‡ßã‡¶®‡¶ø‡ßü‡¶æ (?=\\W|$)\": \"NH‚ÇÉ \",\n",
        "    r\"‡¶Æ‡¶ø‡¶•‡ßá‡¶®(?=\\W|$)\": \"CH‚ÇÑ\",\n",
        "    r\"‡¶á‡¶•‡ßá‡¶®(?=\\W|$)\": \"C‚ÇÇH‚ÇÜ\",\n",
        "    r\"‡¶á‡¶•‡¶ø‡¶®(?=\\W|$)|(‡¶Ö‡ßç‡¶Ø‡¶æ‡¶∏‡¶ø‡¶ü‡¶ø‡¶≤‡¶ø‡¶®|‡¶Ö‡ßç‡¶Ø‡¶æ‡¶∏‡¶ø‡¶ü‡¶ø‡¶≤‡¶ø‡¶®)(?=\\W|$)\": \"C‚ÇÇH‚ÇÇ\",\n",
        "    r\"‡¶á‡¶•‡¶æ‡¶®‡¶≤(?=\\W|$)\": \"C‚ÇÇH‚ÇÖOH\",\n",
        "    r\"‡¶ó‡ßç‡¶≤‡ßÅ‡¶ï‡ßã‡¶ú(?=\\W|$)\": \"C‚ÇÜH‚ÇÅ‚ÇÇO‚ÇÜ\",\n",
        "    r\"‡¶∏‡¶æ‡¶≤‡¶´‡¶æ‡¶∞ ‡¶°‡¶æ‡¶á ‡¶Ö‡¶ï‡ßç‡¶∏‡¶æ‡¶á‡¶°(?=\\W|$)\": \"SO‚ÇÇ\",\n",
        "    r\"‡¶∏‡¶æ‡¶≤‡¶´‡¶æ‡¶∞ ‡¶ü‡ßç‡¶∞‡¶æ‡¶á ‡¶Ö‡¶ï‡ßç‡¶∏‡¶æ‡¶á‡¶°(?=\\W|$)\": \"SO‚ÇÉ\",\n",
        "    r\"‡¶π‡¶æ‡¶á‡¶°‡ßç‡¶∞‡ßã‡¶ï‡ßç‡¶≤‡ßã‡¶∞‡¶ø‡¶ï (‡¶Ö‡ßç‡¶Ø‡¶æ‡¶∏‡¶ø‡¶°|‡¶Ö‡ßç‡¶Ø‡¶æ‡¶∏‡¶ø‡¶°)(?=\\W|$)|‡¶π‡¶æ‡¶á‡¶°‡ßç‡¶∞‡ßã‡¶ï‡ßç‡¶≤‡ßã‡¶∞‡¶ø‡¶ï ‡¶è‡¶∏‡¶ø‡¶°(?=\\W|$)|‡¶è‡¶á‡¶ö‡¶∏‡¶ø‡¶è‡¶≤(?=\\W|$)\": \"HCl\",\n",
        "    r\"‡¶®‡¶æ‡¶á‡¶ü‡ßç‡¶∞‡¶ø‡¶ï (‡¶Ö‡ßç‡¶Ø‡¶æ‡¶∏‡¶ø‡¶°|‡¶Ö‡ßç‡¶Ø‡¶æ‡¶∏‡¶ø‡¶°)(?=\\W|$)|‡¶®‡¶æ‡¶á‡¶ü‡ßç‡¶∞‡¶ø‡¶ï ‡¶è‡¶∏‡¶ø‡¶°(?=\\W|$)\": \"HNO‚ÇÉ\",\n",
        "    r\"‡¶∏‡¶æ‡¶≤‡¶´‡¶ø‡¶â‡¶∞‡¶ø‡¶ï (‡¶Ö‡ßç‡¶Ø‡¶æ‡¶∏‡¶ø‡¶°|‡¶Ö‡ßç‡¶Ø‡¶æ‡¶∏‡¶ø‡¶°)(?=\\W|$)|‡¶∏‡¶æ‡¶≤‡¶´‡¶ø‡¶â‡¶∞‡¶ø‡¶ï ‡¶è‡¶∏‡¶ø‡¶°(?=\\W|$)\": \"H‚ÇÇSO‚ÇÑ\",\n",
        "    r\"(‡¶ï‡ßç‡¶Ø‡¶æ‡¶≤‡¶∏‡¶ø‡¶Ø‡¶º‡¶æ‡¶Æ|‡¶ï‡ßç‡¶Ø‡¶æ‡¶≤‡¶∏‡¶ø‡ßü‡¶æ‡¶Æ) ‡¶ï‡¶æ‡¶∞‡ßç‡¶¨‡¶®‡ßá‡¶ü(?=\\W|$)\": \"CaCO‚ÇÉ\",\n",
        "    r\"(‡¶∏‡ßã‡¶°‡¶ø‡¶Ø‡¶º‡¶æ‡¶Æ|‡¶∏‡ßã‡¶°‡¶ø‡ßü‡¶æ‡¶Æ) ‡¶ï‡ßç‡¶≤‡ßã‡¶∞‡¶æ‡¶á‡¶°(?=\\W|$)\": \"NaCl\",\n",
        "    r\"(‡¶∏‡ßã‡¶°‡¶ø‡¶Ø‡¶º‡¶æ‡¶Æ|‡¶∏‡ßã‡¶°‡¶ø‡ßü‡¶æ‡¶Æ) ‡¶¨‡¶æ‡¶á‡¶ï‡¶æ‡¶∞‡ßç‡¶¨‡ßã‡¶®‡ßá‡¶ü(?=\\W|$)\": \"NaHCO‚ÇÉ\",\n",
        "    r\"(‡¶∏‡ßã‡¶°‡¶ø‡¶Ø‡¶º‡¶æ‡¶Æ|‡¶∏‡ßã‡¶°‡¶ø‡ßü‡¶æ‡¶Æ) ‡¶ï‡¶æ‡¶∞‡ßç‡¶¨‡ßã‡¶®‡ßá‡¶ü(?=\\W|$)\": \"Na‚ÇÇCO‚ÇÉ\",\n",
        "    r\"(‡¶™‡¶ü‡¶æ‡¶∏‡¶ø‡¶Ø‡¶º‡¶æ‡¶Æ|‡¶™‡¶ü‡¶æ‡¶∏‡¶ø‡ßü‡¶æ‡¶Æ) ‡¶™‡¶æ‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ô‡ßç‡¶ó‡¶æ‡¶®‡ßá‡¶ü(?=\\W|$)\": \"KMnO‚ÇÑ\",\n",
        "    r\"‡¶π‡¶æ‡¶á‡¶°‡ßç‡¶∞‡ßã‡¶ú‡ßá‡¶® ‡¶™‡¶æ‡¶∞ ‡¶Ö‡¶ï‡ßç‡¶∏‡¶æ‡¶á‡¶°(?=\\W|$)\": \"H‚ÇÇO‚ÇÇ\",\n",
        "\n",
        "    r\"‡¶π‡¶æ‡¶á‡¶°‡ßç‡¶∞‡ßã‡¶ú‡ßá‡¶®(?=\\W|$)\": \"H\",\n",
        "    r\"(‡¶π‡¶ø‡¶≤‡¶ø‡¶Ø‡¶º‡¶æ‡¶Æ|‡¶π‡¶ø‡¶≤‡¶ø‡ßü‡¶æ‡¶Æ)(?=\\W|$)\": \"He\",\n",
        "    r\"(‡¶≤‡¶ø‡¶•‡¶ø‡¶Ø‡¶º‡¶æ‡¶Æ|‡¶≤‡¶ø‡¶•‡¶ø‡ßü‡¶æ‡¶Æ)(?=\\W|$)\": \"Li\",\n",
        "    r\"(‡¶¨‡ßá‡¶∞‡¶ø‡¶≤‡¶ø‡¶Ø‡¶º‡¶æ‡¶Æ|‡¶¨‡ßá‡¶∞‡¶ø‡¶≤‡¶ø‡ßü‡¶æ‡¶Æ)(?=\\W|$)\": \"Be\",\n",
        "    r\"‡¶¨‡ßã‡¶∞‡¶®(?=\\W|$)\": \"B\",\n",
        "    r\"‡¶ï‡¶æ‡¶∞‡ßç‡¶¨‡¶® (?=\\W|$)\": \"C \",\n",
        "    r\"‡¶®‡¶æ‡¶á‡¶ü‡ßç‡¶∞‡ßã‡¶ú‡ßá‡¶®(?=\\W|$)\": \"N\",\n",
        "    r\"‡¶Ö‡¶ï‡ßç‡¶∏‡¶ø‡¶ú‡ßá‡¶®(?=\\W|$)\": \"O\",\n",
        "    r\"‡¶´‡ßç‡¶≤‡ßã‡¶∞‡¶ø‡¶®(?=\\W|$)\": \"F\",\n",
        "    r\"(‡¶®‡¶ø‡ßü‡¶®|‡¶®‡¶ø‡¶Ø‡¶º‡¶®)(?=\\W|$)\": \"Ne\",\n",
        "    r\"(‡¶∏‡ßã‡¶°‡¶ø‡¶Ø‡¶º‡¶æ‡¶Æ|‡¶∏‡ßã‡¶°‡¶ø‡ßü‡¶æ‡¶Æ)(?=\\W|$)\": \"Na\",\n",
        "    r\"(‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ó‡¶®‡ßá‡¶∏‡¶ø‡¶Ø‡¶º‡¶æ‡¶Æ|‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ó‡¶®‡ßá‡¶∏‡¶ø‡ßü‡¶æ‡¶Æ)(?=\\W|$)\": \"Mg\",\n",
        "    r\"(‡¶Ö‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ‡¶Æ‡¶ø‡¶®‡¶ø‡¶Ø‡¶º‡¶æ‡¶Æ|‡¶Ö‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ‡¶Æ‡¶ø‡¶®‡¶ø‡ßü‡¶æ‡¶Æ)(?=\\W|$)\": \"Al\",\n",
        "    r\"‡¶∏‡¶ø‡¶≤‡¶ø‡¶ï‡¶®(?=\\W|$)\": \"Si\",\n",
        "    r\"‡¶´‡¶∏‡¶´‡¶∞‡¶æ‡¶∏(?=\\W|$)\": \"P\",\n",
        "    r\"‡¶∏‡¶æ‡¶≤‡¶´‡¶æ‡¶∞(?=\\W|$)\": \"S\",\n",
        "    r\"‡¶ï‡ßç‡¶≤‡ßã‡¶∞‡¶ø‡¶®(?=\\W|$)\": \"Cl\",\n",
        "    r\"‡¶Ü‡¶∞‡ßç‡¶ó‡¶®(?=\\W|$)\": \"Ar\",\n",
        "    r\"(‡¶™‡¶ü‡¶æ‡¶∏‡¶ø‡¶Ø‡¶º‡¶æ‡¶Æ|‡¶™‡¶ü‡¶æ‡¶∏‡¶ø‡ßü‡¶æ‡¶Æ)(?=\\W|$)\": \"K\",\n",
        "    r\"(‡¶ï‡ßç‡¶Ø‡¶æ‡¶≤‡¶∏‡¶ø‡¶Ø‡¶º‡¶æ‡¶Æ|‡¶ï‡ßç‡¶Ø‡¶æ‡¶≤‡¶∏‡¶ø‡ßü‡¶æ‡¶Æ)(?=\\W|$)\": \"Ca\",\n",
        "    r\"‡¶≤‡ßã‡¶π‡¶æ(?=\\W|$)\": \"Fe\",\n",
        "    r\"‡¶ï‡¶™‡¶æ‡¶∞|‡¶§‡¶æ‡¶Æ‡¶æ(?=\\W|$)\": \"Cu\",\n",
        "    r\"‡¶ú‡¶ø‡¶ô‡ßç‡¶ï(?=\\W|$)\": \"Zn\",\n",
        "    r\"‡¶∏‡¶ø‡¶≤‡¶≠‡¶æ‡¶∞|‡¶∞‡ßÇ‡¶™‡¶æ(?=\\W|$)\": \"Ag\",\n",
        "    r\"‡¶ó‡ßã‡¶≤‡ßç‡¶°|‡¶∏‡ßã‡¶®‡¶æ(?=\\W|$)\": \"Au\",\n",
        "    r\"‡¶Æ‡¶æ‡¶∞‡ßç‡¶ï‡¶æ‡¶∞‡¶ø|‡¶™‡¶æ‡¶∞‡¶¶(?=\\W|$)\": \"Hg\",\n",
        "    r\"‡¶≤‡ßá‡¶°|‡¶∏‡¶ø‡¶∏‡¶æ(?=\\W|$)\": \"Pb\",\n",
        "    r\" ‡¶ü‡¶ø‡¶®(?=\\W|$)\": \" Sn\",\n",
        "    r\"‡¶®‡¶ø‡¶ï‡ßá‡¶≤(?=\\W|$)\": \"Ni\",\n",
        "    r\"(‡¶ï‡ßç‡¶∞‡ßã‡¶Æ‡¶ø‡¶Ø‡¶º‡¶æ‡¶Æ|‡¶ï‡ßç‡¶∞‡ßã‡¶Æ‡¶ø‡ßü‡¶æ‡¶Æ)(?=\\W|$)\": \"Cr\",\n",
        "    r\"‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ô‡ßç‡¶ó‡¶æ‡¶®‡¶ø‡¶ú(?=\\W|$)\": \"Mn\",\n",
        "    r\"‡¶ï‡ßã‡¶¨‡¶æ‡¶≤‡ßç‡¶ü(?=\\W|$)\": \"Co\",\n",
        "    r\"‡¶™‡ßç‡¶≤‡¶æ‡¶ü‡¶ø‡¶®‡¶æ‡¶Æ(?=\\W|$)\": \"Pt\",\n",
        "    r\"(‡¶á‡¶â‡¶∞‡ßá‡¶®‡¶ø‡¶Ø‡¶º‡¶æ‡¶Æ|‡¶á‡¶â‡¶∞‡ßá‡¶®‡¶ø‡ßü‡¶æ‡¶Æ)(?=\\W|$)\": \"U\",\n",
        "    r\"(‡¶Ü‡¶Ø‡¶º‡ßã‡¶°‡¶ø‡¶®|‡¶Ü‡ßü‡ßã‡¶°‡¶ø‡¶®)(?=\\W|$)\": \"I\",\n",
        "    r\"‡¶¨‡ßç‡¶∞‡ßã‡¶Æ‡¶ø‡¶®(?=\\W|$)\": \"Br\",\n",
        "\n",
        "    r\"‡¶´‡¶ø‡¶ú‡¶ø‡¶ï‡ßç‡¶∏(?=\\W|$)\": \"Physics\",\n",
        "    r\"(‡¶∞‡¶∏‡¶æ‡¶Ø‡¶º‡¶®|‡¶∞‡¶∏‡¶æ‡ßü‡¶®)(?=\\W|$)|‡¶ï‡ßá‡¶Æ‡¶ø‡¶∏‡ßç‡¶ü‡ßç‡¶∞‡¶ø(?=\\W|$)\": \"Chemistry\",\n",
        "    r\"‡¶ú‡ßÄ‡¶¨‡¶¨‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶®(?=\\W|$)|(‡¶¨‡¶æ‡¶Ø‡¶º‡ßã‡¶≤‡¶ú‡¶ø|‡¶¨‡¶æ‡ßü‡ßã‡¶≤‡¶ú‡¶ø)(?=\\W|$)\": \"Biology\",\n",
        "    r\"‡¶â‡¶ö‡ßç‡¶ö‡¶§‡¶∞ ‡¶ó‡¶£‡¶ø‡¶§(?=\\W|$)|(‡¶π‡¶æ‡¶Ø‡¶º‡¶æ‡¶∞|‡¶π‡¶æ‡ßü‡¶æ‡¶∞) ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶•(?=\\W|$)|(‡¶π‡¶æ‡¶Ø‡¶º‡¶æ‡¶∞|‡¶π‡¶æ‡ßü‡¶æ‡¶∞) ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶•‡¶Æ‡ßá‡¶ü‡¶ø‡¶ï‡ßç‡¶∏(?=\\W|$)\": \"Higher Mathematics\",\n",
        "    r\"‡¶Æ‡ßç‡¶Ø‡¶æ‡¶•(?=\\W|$)\": \"Math\",\n",
        "    r\"‡¶á‡¶Ç‡¶∞‡ßá‡¶ú‡¶ø(?=\\W|$)\": \"English\",\n",
        "    r\"‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ(?=\\W|$)\": \"Bangla\",\n",
        "    r\"‡¶Ü‡¶á‡¶∏‡¶ø‡¶ü‡¶ø(?=\\W|$)|‡¶§‡¶•‡ßç‡¶Ø ‡¶ì ‡¶Ø‡ßã‡¶ó‡¶æ‡¶Ø‡ßã‡¶ó ‡¶™‡ßç‡¶∞‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø(?=\\W|$)\": \"ICT\",\n",
        "    r\"‡¶≠‡ßÇ‡¶ó‡ßã‡¶≤(?=\\W|$)|‡¶ú‡¶ø‡¶ì‡¶ó‡ßç‡¶∞‡¶æ‡¶´‡¶ø(?=\\W|$)\": \"Geography\",\n",
        "    r\"‡¶∏‡¶æ‡¶á‡¶®‡ßç‡¶∏(?=\\W|$)\": \"Science\",\n",
        "    r\"‡¶ï‡¶Æ‡ßç‡¶™‡¶ø‡¶â‡¶ü‡¶æ‡¶∞(?=\\W|$)\": \"Computer\",\n",
        "    r\"‡¶è‡¶á ‡¶Ö‡¶Ç‡¶∂‡ßá \": \"\"\n",
        "}\n",
        "\n",
        "\n",
        "                for pattern, replacement in regex_patterns.items():\n",
        "                  final_text = re.sub(pattern, replacement, final_text)\n",
        "                  regex_pat={\n",
        "    r\"([a-zA-Z‚ÇÄ‚ÇÅ‚ÇÇ‚ÇÉ‚ÇÑ‚ÇÖ‚ÇÜ‚Çá‚Çà‚Çâ+-])‡ßá\": r\"\\1‡¶è\",\n",
        "    r\"([a-z0-9ŒªŒ∏œÄœÉœÅœâœÜŒ±Œ≤Œ≥]) (‡¶∏‡ßç‡¶ï‡ßü‡¶æ‡¶∞|‡¶∏‡ßç‡¶ï‡¶Ø‡¶º‡¶æ‡¶∞|‡¶∏‡ßç‡¶ï‡ßã‡ßü‡¶æ‡¶∞|‡¶∏‡ßç‡¶ï‡ßã‡¶Ø‡¶º‡¶æ‡¶∞)\": r\"\\1¬≤\",\n",
        "    r\"([a-z0-9ŒªŒ∏œÄœÉœÅœâœÜŒ±Œ≤Œ≥]) ‡¶ï‡¶ø‡¶â‡¶¨\": r\"\\1¬≥\",\n",
        "    r\"(‡¶∞‡ßÅ‡¶ü ‡¶ì‡¶≠‡¶æ‡¶∞‡•§‡¶∞‡ßÅ‡¶ü) ([a-z0-9ŒªŒ∏œÄœÉœÅœâœÜŒ±Œ≤Œ≥])\": r\"‚àö\\2\",\n",
        "    r\"([a-zŒªŒ∏œÄœÉœÅœâœÜŒ±Œ≤Œ≥]) ‡¶®‡¶ü\": r\"\\1‚ÇÄ\",\n",
        "    r\"‡¶ü‡¶æ‡¶® ([a-zŒªŒ∏œÄœÉœÅœâœÜŒ±Œ≤Œ≥])\": r\"tan \\1\",\n",
        "    r\"(‡¶ï‡¶∏|‡¶ï‡¶ú) ([a-zŒªŒ∏œÄœÉœÅœâœÜŒ±Œ≤Œ≥])\": r\"cos \\2\"\n",
        "\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "                for pattern, replacement in regex_pat.items():\n",
        "                  final_text = re.sub(pattern, replacement, final_text)\n",
        "\n",
        "            # Save final text\n",
        "            with open(final_text_path, 'w', encoding='utf-8') as f:\n",
        "              f.write(final_text)\n",
        "\n",
        "\n",
        "            # Final update\n",
        "            update_status(100, f'Processing completed successfully! ({total_iterations} summarization iterations)')\n",
        "\n",
        "\n",
        "        # Return final results\n",
        "            results = {\n",
        "            'status': 'completed',\n",
        "            'progress': 100,\n",
        "            'message': f'Processing completed successfully! ({total_iterations} summarization iterations)',\n",
        "            'files': {\n",
        "                'audio': audio_path,\n",
        "                'transcription': transcription_path,\n",
        "                'summary_excel': summary_path,\n",
        "                'final_text': final_text_path\n",
        "            },\n",
        "            'preview': final_text[:5000] + \"...\" if len(final_text) > 5000 else final_text,\n",
        "            'iterations': total_iterations\n",
        "        }\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "\n",
        "            error_msg = f'Processing failed: {str(e)}'\n",
        "            if job_id and job_id in processing_status:\n",
        "              processing_status[job_id].update({\n",
        "                  'status': 'error',\n",
        "                 'message': error_msg,\n",
        "                  'error': traceback.format_exc()\n",
        "              })\n",
        "            return {\n",
        "            'status': 'error',\n",
        "            'message': error_msg,\n",
        "            'error': traceback.format_exc()\n",
        "         }\n",
        "\n",
        "\n",
        "    def recursive_summarization_with_concatenation(self, df, max_chunks=50):\n",
        "        \"\"\"\n",
        "        Recursively summarize by concatenating pairs of rows until we have <= max_chunks\n",
        "        \"\"\"\n",
        "        current_df = df.copy()\n",
        "        iteration = 1\n",
        "\n",
        "        while len(current_df) > max_chunks:\n",
        "            print(f\"\\nüîÑ Iteration {iteration}: Processing {len(current_df)} rows...\")\n",
        "\n",
        "            # Generate summaries for current rows\n",
        "            print(\"üìÑ Generating summaries...\")\n",
        "            current_df['Summary'] = current_df['Text'].apply(\n",
        "                lambda x: self.generate_summary(str(x))\n",
        "            )\n",
        "\n",
        "            # Calculate token counts if tokenizer is available\n",
        "            if hasattr(self, 'tokenizer') and self.tokenizer:\n",
        "                print(\"üî¢ Calculating token counts...\")\n",
        "                current_df['Token(Text)'] = current_df['Text'].apply(\n",
        "                    lambda x: self.count_tokens(x)\n",
        "                )\n",
        "                current_df['Token(Summary)'] = current_df['Summary'].apply(\n",
        "                    lambda x: self.count_tokens(x)\n",
        "                )\n",
        "\n",
        "                # Print token statistics\n",
        "                avg_text_tokens = current_df['Token(Text)'].mean()\n",
        "                avg_summary_tokens = current_df['Token(Summary)'].mean()\n",
        "                print(f\"üìä Token Statistics - Iteration {iteration}:\")\n",
        "                print(f\"   Average Text tokens: {avg_text_tokens:.1f}\")\n",
        "                print(f\"   Average Summary tokens: {avg_summary_tokens:.1f}\")\n",
        "\n",
        "            # Prepare dataframe for saving with required columns\n",
        "            save_columns = ['Text', 'Summary', 'Start_Time', 'End_Time']\n",
        "            if hasattr(self, 'tokenizer') and self.tokenizer:\n",
        "                save_columns.extend(['Token(Text)', 'Token(Summary)'])\n",
        "\n",
        "            save_df = current_df[save_columns].copy()\n",
        "\n",
        "            # Save Excel file for this iteration\n",
        "            iteration_excel_path = os.path.join(\n",
        "                os.path.dirname(self.temp_dir) if hasattr(self, 'temp_dir') else '/tmp',\n",
        "                f\"iteration_{iteration}_results.xlsx\"\n",
        "            )\n",
        "            save_df.to_excel(iteration_excel_path, index=False)\n",
        "            print(f\"üíæ Iteration {iteration} results saved: {iteration_excel_path}\")\n",
        "\n",
        "            # Concatenate pairs of rows\n",
        "            print(\"üîó Concatenating pairs of rows...\")\n",
        "            current_df = self.concatenate_pairs(current_df)\n",
        "\n",
        "            print(f\"üî¢ After concatenation: {len(current_df)} rows\")\n",
        "\n",
        "            # Debug: Print sample data\n",
        "            print(f\"üìù Sample rows after concatenation:\")\n",
        "            for i, (idx, row) in enumerate(current_df.head(3).iterrows()):\n",
        "                print(f\"   Row {i+1} (ID: {row.get('Chunk_ID', 'N/A')}):\")\n",
        "                print(f\"     Start: {row.get('Start_Time', 'N/A')}, End: {row.get('End_Time', 'N/A')}\")\n",
        "                print(f\"     Text: {str(row.get('Text', ''))[:100]}...\")\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "            # Safety check to prevent infinite loops\n",
        "            if iteration > 10:\n",
        "                print(\"‚ö†Ô∏è Maximum iterations reached. Breaking loop.\")\n",
        "                break\n",
        "\n",
        "        # Final summarization\n",
        "        print(f\"\\n‚úÖ Final iteration: Processing {len(current_df)} rows...\")\n",
        "        current_df['Final_Summary'] = current_df['Text'].apply(\n",
        "            lambda x: self.generate_summary(str(x))\n",
        "        )\n",
        "\n",
        "        return current_df, iteration\n",
        "# Initialize the processor\n",
        "try:\n",
        "    print(\"üîÑ Initializing model...\")\n",
        "    processor = VideoProcessor()\n",
        "    print(\"‚úÖ VideoProcessor initialized successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Model initialization failed: {e}\")\n",
        "    print(\"Please check your model path or use a different model\")\n",
        "    processor = None\n",
        "\n",
        "\n",
        "\n",
        "# Flask Application\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "# Global variables\n",
        "processor = None\n",
        "processing_status = {}\n",
        "\n",
        "# Simple HTML template for Colab\n",
        "HTML_TEMPLATE = '''<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Video Processing & Summarization</title>\n",
        "    <style>\n",
        "        body { font-family: Arial, sans-serif; margin: 20px; background: #f5f5f5; }\n",
        "        .container { max-width: 800px; margin: 0 auto; background: white; padding: 20px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }\n",
        "        .upload-area { border: 2px dashed #4CAF50; padding: 30px; text-align: center; margin: 20px 0; border-radius: 10px; background: #f9fff9; }\n",
        "        .upload-area:hover { background: #f0fff0; }\n",
        "        .progress-bar { width: 100%; height: 25px; background: #f0f0f0; border-radius: 12px; overflow: hidden; margin: 10px 0; }\n",
        "        .progress-fill { height: 100%; background: linear-gradient(90deg, #4CAF50, #45a049); width: 0%; transition: width 0.3s; }\n",
        "        button { background: #007bff; color: white; border: none; padding: 12px 24px; border-radius: 6px; cursor: pointer; font-size: 16px; margin: 5px; }\n",
        "        button:hover { background: #0056b3; }\n",
        "        button:disabled { background: #ccc; cursor: not-allowed; }\n",
        "        .download-btn { background: #28a745; }\n",
        "        .download-btn:hover { background: #218838; }\n",
        "        .error { color: #dc3545; padding: 15px; background: #f8d7da; border: 1px solid #f5c6cb; border-radius: 5px; margin: 10px 0; }\n",
        "        .success { color: #155724; padding: 15px; background: #d4edda; border: 1px solid #c3e6cb; border-radius: 5px; margin: 10px 0; }\n",
        "        .preview { max-height: 300px; overflow-y: auto; border: 1px solid #ddd; padding: 15px; background: #f8f9fa; border-radius: 5px; font-family: monospace; }\n",
        "        .status-badge { display: inline-block; padding: 5px 10px; border-radius: 15px; font-size: 12px; font-weight: bold; }\n",
        "        .status-processing { background: #fff3cd; color: #856404; }\n",
        "        .status-completed { background: #d4edda; color: #155724; }\n",
        "        .status-error { background: #f8d7da; color: #721c24; }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"container\">\n",
        "        <h1>üé¨ Video Processing & Summarization</h1>\n",
        "        <p>Upload a Bengali video file to get automated transcription and summarization</p>\n",
        "\n",
        "        <div class=\"upload-area\" id=\"uploadArea\" onclick=\"document.getElementById('videoInput').click()\">\n",
        "            <input type=\"file\" id=\"videoInput\" accept=\"video/*\" style=\"display: none;\">\n",
        "            <p>üìÅ Click to select a video file or drag and drop</p>\n",
        "            <button type=\"button\">Choose Video File</button>\n",
        "        </div>\n",
        "\n",
        "        <div id=\"fileInfo\" style=\"display: none;\">\n",
        "            <h3>üìπ Selected File</h3>\n",
        "            <p><strong>Name:</strong> <span id=\"fileName\"></span></p>\n",
        "            <p><strong>Size:</strong> <span id=\"fileSize\"></span></p>\n",
        "            <button id=\"processBtn\" onclick=\"processVideo()\">üöÄ Start Processing</button>\n",
        "        </div>\n",
        "\n",
        "        <div id=\"progressSection\" style=\"display: none;\">\n",
        "            <h3>‚öôÔ∏è Processing Status</h3>\n",
        "            <div class=\"progress-bar\">\n",
        "                <div class=\"progress-fill\" id=\"progressFill\"></div>\n",
        "            </div>\n",
        "            <p id=\"progressText\">0% - Initializing...</p>\n",
        "        </div>\n",
        "\n",
        "        <div id=\"resultsSection\" style=\"display: none;\">\n",
        "            <h3>üìä Processing Results</h3>\n",
        "            <div id=\"downloadButtons\" style=\"text-align: center; margin: 20px 0;\">\n",
        "                <button class=\"download-btn\" onclick=\"downloadFile('audio')\">üéµ Audio File</button>\n",
        "                <button class=\"download-btn\" onclick=\"downloadFile('transcription')\">üìã Transcription</button>\n",
        "                <button class=\"download-btn\" onclick=\"downloadFile('summary_excel')\">üìä Summary Excel</button>\n",
        "                <button class=\"download-btn\" onclick=\"downloadFile('final_text')\">üìù Final Summary</button>\n",
        "            </div>\n",
        "\n",
        "            <h4>üîç Preview:</h4>\n",
        "            <div id=\"summaryPreview\" class=\"preview\"></div>\n",
        "        </div>\n",
        "\n",
        "        <div id=\"messages\"></div>\n",
        "    </div>\n",
        "\n",
        "    <script>\n",
        "        let selectedFile = null;\n",
        "        let currentJobId = null;\n",
        "\n",
        "        document.getElementById('videoInput').addEventListener('change', function(e) {\n",
        "            const file = e.target.files[0];\n",
        "            if (file) {\n",
        "                selectedFile = file;\n",
        "                document.getElementById('fileName').textContent = file.name;\n",
        "                document.getElementById('fileSize').textContent = (file.size / 1024 / 1024).toFixed(2) + ' MB';\n",
        "                document.getElementById('fileInfo').style.display = 'block';\n",
        "            }\n",
        "        });\n",
        "\n",
        "        async function processVideo() {\n",
        "            if (!selectedFile) {\n",
        "                showError('Please select a video file first');\n",
        "                return;\n",
        "            }\n",
        "\n",
        "            const formData = new FormData();\n",
        "            formData.append('video', selectedFile);\n",
        "\n",
        "            document.getElementById('progressSection').style.display = 'block';\n",
        "            const processBtn = document.getElementById('processBtn');\n",
        "            processBtn.disabled = true;\n",
        "            processBtn.textContent = '‚è≥ Processing...';\n",
        "\n",
        "            try {\n",
        "                const response = await fetch('/api/process', {\n",
        "                    method: 'POST',\n",
        "                    body: formData\n",
        "                });\n",
        "\n",
        "                const result = await response.json();\n",
        "\n",
        "                if (result.status === 'success') {\n",
        "                    currentJobId = result.job_id;\n",
        "                    pollProgress(result.job_id);\n",
        "                } else {\n",
        "                    throw new Error(result.message);\n",
        "                }\n",
        "\n",
        "            } catch (error) {\n",
        "                showError('Processing failed: ' + error.message);\n",
        "                processBtn.disabled = false;\n",
        "                processBtn.textContent = 'üöÄ Start Processing';\n",
        "            }\n",
        "        }\n",
        "\n",
        "        async function pollProgress(jobId) {\n",
        "            const interval = setInterval(async () => {\n",
        "                try {\n",
        "                    const response = await fetch(`/api/status/${jobId}`);\n",
        "                    const status = await response.json();\n",
        "\n",
        "                    updateProgress(status.progress || 0, status.message || 'Processing...');\n",
        "\n",
        "                    if (status.status === 'completed') {\n",
        "                        clearInterval(interval);\n",
        "                        displayResults(status);\n",
        "                        showSuccess('üéâ Video processing completed successfully!');\n",
        "                        document.getElementById('resultsSection').style.display = 'block';\n",
        "                        document.getElementById('processBtn').disabled = false;\n",
        "                        document.getElementById('processBtn').textContent = 'üöÄ Start Processing';\n",
        "                    } else if (status.status === 'error') {\n",
        "                        clearInterval(interval);\n",
        "                        showError('‚ùå Processing failed: ' + status.message);\n",
        "                        document.getElementById('processBtn').disabled = false;\n",
        "                        document.getElementById('processBtn').textContent = 'üöÄ Start Processing';\n",
        "                    }\n",
        "                } catch (error) {\n",
        "                    console.error('Error polling status:', error);\n",
        "                }\n",
        "            }, 3000);\n",
        "        }\n",
        "\n",
        "        function updateProgress(progress, message) {\n",
        "            document.getElementById('progressFill').style.width = progress + '%';\n",
        "            document.getElementById('progressText').textContent = progress + '% - ' + message;\n",
        "        }\n",
        "\n",
        "        function displayResults(status) {\n",
        "            if (status.preview) {\n",
        "                document.getElementById('summaryPreview').textContent = status.preview;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        async function downloadFile(type) {\n",
        "            if (!currentJobId) {\n",
        "                showError('No processing job found');\n",
        "                return;\n",
        "            }\n",
        "\n",
        "            try {\n",
        "                const response = await fetch(`/api/download/${currentJobId}/${type}`);\n",
        "\n",
        "                if (response.ok) {\n",
        "                    const blob = await response.blob();\n",
        "                    const url = window.URL.createObjectURL(blob);\n",
        "                    const a = document.createElement('a');\n",
        "                    a.href = url;\n",
        "                    a.download = getFileName(type);\n",
        "                    a.click();\n",
        "                    window.URL.revokeObjectURL(url);\n",
        "                } else {\n",
        "                    showError('Download failed');\n",
        "                }\n",
        "            } catch (error) {\n",
        "                showError('Download error: ' + error.message);\n",
        "            }\n",
        "        }\n",
        "\n",
        "        function getFileName(type) {\n",
        "            const names = {\n",
        "                'audio': 'extracted_audio.wav',\n",
        "                'transcription': 'transcription_chunks.xlsx',\n",
        "                'summary_excel': 'summaries_with_timestamps.xlsx',\n",
        "                'final_text': 'final_summary.txt'\n",
        "            };\n",
        "            return names[type] || 'download.txt';\n",
        "        }\n",
        "\n",
        "        function showError(message) {\n",
        "            const div = document.createElement('div');\n",
        "            div.className = 'error';\n",
        "            div.innerHTML = '‚ùå ' + message;\n",
        "            document.getElementById('messages').appendChild(div);\n",
        "            setTimeout(() => div.remove(), 8000);\n",
        "        }\n",
        "\n",
        "        function showSuccess(message) {\n",
        "            const div = document.createElement('div');\n",
        "            div.className = 'success';\n",
        "            div.innerHTML = '‚úÖ ' + message;\n",
        "            document.getElementById('messages').appendChild(div);\n",
        "            setTimeout(() => div.remove(), 8000);\n",
        "        }\n",
        "    </script>\n",
        "</body>\n",
        "</html>'''\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    \"\"\"Serve the main HTML page\"\"\"\n",
        "    return render_template_string(HTML_TEMPLATE)\n",
        "\n",
        "@app.route('/api/process', methods=['POST'])\n",
        "def process_video():\n",
        "    \"\"\"Start video processing\"\"\"\n",
        "    try:\n",
        "        if 'video' not in request.files:\n",
        "            return jsonify({'status': 'error', 'message': 'No video file provided'})\n",
        "\n",
        "        video_file = request.files['video']\n",
        "        if video_file.filename == '':\n",
        "            return jsonify({'status': 'error', 'message': 'No video file selected'})\n",
        "\n",
        "        # Create unique job ID\n",
        "        job_id = str(int(time.time() * 1000))\n",
        "\n",
        "        # Create temporary directory for this job\n",
        "        temp_dir = os.path.join('/tmp', f'video_processing_{job_id}')\n",
        "        os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "        # Save uploaded file\n",
        "        video_path = os.path.join(temp_dir, video_file.filename)\n",
        "        video_file.save(video_path)\n",
        "\n",
        "        # Initialize processing status\n",
        "        processing_status[job_id] = {\n",
        "            'status': 'queued',\n",
        "            'progress': 0,\n",
        "            'message': 'Processing queued...',\n",
        "            'temp_dir': temp_dir\n",
        "        }\n",
        "\n",
        "        # Start processing in background thread\n",
        "        thread = threading.Thread(\n",
        "            target=background_process,\n",
        "            args=(job_id, video_path, temp_dir)\n",
        "        )\n",
        "        thread.daemon = True\n",
        "        thread.start()\n",
        "\n",
        "        return jsonify({\n",
        "            'status': 'success',\n",
        "            'job_id': job_id,\n",
        "            'message': 'Processing started'\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\n",
        "            'status': 'error',\n",
        "            'message': str(e)\n",
        "        })\n",
        "\n",
        "def background_process(job_id, video_path, temp_dir):\n",
        "    \"\"\"Background processing function\"\"\"\n",
        "    try:\n",
        "        global processor\n",
        "        if processor is None:\n",
        "            processing_status[job_id].update({\n",
        "                'status': 'error',\n",
        "                'message': 'Model not initialized'\n",
        "            })\n",
        "            return\n",
        "\n",
        "        # Update status to processing\n",
        "        processing_status[job_id].update({\n",
        "            'status': 'processing',\n",
        "            'progress': 5,\n",
        "            'message': 'Starting video processing...'\n",
        "        })\n",
        "\n",
        "        # Process the video\n",
        "        result = processor.process_complete_pipeline(video_path, temp_dir, job_id=job_id)\n",
        "\n",
        "        # Update final status\n",
        "        processing_status[job_id].update(result)\n",
        "\n",
        "    except Exception as e:\n",
        "        processing_status[job_id].update({\n",
        "            'status': 'error',\n",
        "            'message': str(e),\n",
        "            'error': traceback.format_exc()\n",
        "        })\n",
        "\n",
        "@app.route('/api/status/<job_id>')\n",
        "def get_status(job_id):\n",
        "    \"\"\"Get processing status\"\"\"\n",
        "    if job_id in processing_status:\n",
        "        return jsonify(processing_status[job_id])\n",
        "    else:\n",
        "        return jsonify({\n",
        "            'status': 'error',\n",
        "            'message': 'Job not found'\n",
        "        })\n",
        "\n",
        "@app.route('/api/download/<job_id>/<file_type>')\n",
        "def download_file(job_id, file_type):\n",
        "    \"\"\"Download processed files\"\"\"\n",
        "    if job_id not in processing_status:\n",
        "        return jsonify({'error': 'Job not found'}), 404\n",
        "\n",
        "    status = processing_status[job_id]\n",
        "    if status['status'] != 'completed':\n",
        "        return jsonify({'error': 'Processing not completed'}), 400\n",
        "\n",
        "    if 'files' not in status or file_type not in status['files']:\n",
        "        return jsonify({'error': 'File not found'}), 404\n",
        "\n",
        "    file_path = status['files'][file_type]\n",
        "    if not os.path.exists(file_path):\n",
        "        return jsonify({'error': f'File does not exist: {file_path}'}), 404\n",
        "\n",
        "    # Define MIME types and download names for different file types\n",
        "    file_configs = {\n",
        "        'audio': {\n",
        "            'mimetype': 'audio/wav',\n",
        "            'download_name': 'extracted_audio.wav'\n",
        "        },\n",
        "        'transcription': {\n",
        "            'mimetype': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',\n",
        "            'download_name': 'transcription_chunks.xlsx'\n",
        "        },\n",
        "        'summary_excel': {\n",
        "            'mimetype': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',\n",
        "            'download_name': 'summaries_with_timestamps.xlsx'\n",
        "        },\n",
        "        'final_text': {\n",
        "            'mimetype': 'text/plain; charset=utf-8',\n",
        "            'download_name': 'final_summary.txt'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    config = file_configs.get(file_type, {\n",
        "        'mimetype': 'application/octet-stream',\n",
        "        'download_name': f'download_{file_type}'\n",
        "    })\n",
        "\n",
        "    try:\n",
        "        return send_file(\n",
        "            file_path,\n",
        "            as_attachment=True,\n",
        "            download_name=config['download_name'],\n",
        "            mimetype=config['mimetype']\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error sending file {file_path}: {e}\")\n",
        "        return jsonify({'error': f'Failed to send file: {str(e)}'}), 500\n",
        "\n",
        "# ===== CELL 4: Initialize and Run =====\n",
        "# Initialize the processor\n",
        "try:\n",
        "    print(\"üîÑ Initializing model...\")\n",
        "    processor = VideoProcessor()  # Will use default model\n",
        "    print(\"‚úÖ Model initialized successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Model initialization failed: {e}\")\n",
        "    print(\"Please check your model path or use a different model\")\n",
        "\n",
        "# Start the Flask app with ngrok\n",
        "if __name__ == '__main__':\n",
        "    # Create ngrok tunnel\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(f\"üåê Public URL: {public_url}\")\n",
        "    print(f\"üîó Access your app at: {public_url}\")\n",
        "\n",
        "    # Run Flask app\n",
        "    app.run(host='0.0.0.0', port=5000, debug=False)"
      ]
    }
  ]
}