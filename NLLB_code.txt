!pip install datasets
!pip install evaluate
!pip install rouge_score
import evaluate
import os
import pandas as pd
import torch
from google.colab import drive
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments
from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split

# Disable wandb logging
os.environ["WANDB_DISABLED"] = "true"

# Mount Google Drive
drive.mount('/content/drive')

# Load the dataset
df = pd.read_excel("/content/drive/My Drive/Thesis_Dataset/TenK.xlsx")  # Adjust path
df = df.rename(columns={df.columns[0]: "text", df.columns[1]: "summary"})
df = df.dropna()

# Split dataset (80% training, 20% validation)
train_df, val_df = train_test_split(df, test_size=0.20, random_state=42)

# Convert to Hugging Face Dataset format
train_dataset = Dataset.from_pandas(train_df)
val_dataset = Dataset.from_pandas(val_df)
dataset_dict = DatasetDict({"train": train_dataset, "val": val_dataset})

# Load tokenizer and model
model_name = "facebook/nllb-200-distilled-600M"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Set source and target language for NLLB-200
tokenizer.src_lang = "bn_Beng"
model.config.forced_bos_token_id = tokenizer.convert_tokens_to_ids("bn_Beng")


# Tokenization function
def preprocess_function(examples):
    inputs = tokenizer(examples['text'], max_length=512, truncation=True, padding="max_length")
    labels = tokenizer(examples['summary'], max_length=150, truncation=True, padding="max_length")

    inputs["labels"] = labels["input_ids"]

    # Replace padding tokens with -100
    inputs["labels"] = [
        [(label if label != tokenizer.pad_token_id else -100) for label in label_ids]
        for label_ids in inputs["labels"]
    ]

    return inputs

# Apply tokenization
tokenized_datasets = dataset_dict.map(preprocess_function, batched=True)

# Training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="/content/drive/MyDrive/Thesis_Dataset/results_NLLB",
    logging_dir="/content/drive/MyDrive/Thesis_Dataset/logs_NLLB",

      do_eval=True,
    eval_steps=200,  # or some suitable integer step

    learning_rate=2e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=2,
    weight_decay=0.01,
    save_strategy="no",  # add to your Seq2SeqTrainingArguments

    logging_steps=10,
    predict_with_generate=True,
    run_name="bangla_summarization_nllb"
)

# Trainer setup
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['val'],
    tokenizer=tokenizer,
)

# Train the model
trainer.train()

# Save model and tokenizer
model.save_pretrained("/content/drive/My Drive/Thesis_Dataset/fine_tuned_NLLB")
tokenizer.save_pretrained("/content/drive/My Drive/Thesis_Dataset/fine_tuned_NLLB")

# Function to generate summaries
def generate_summary(text, model, tokenizer, device):
    model.to(device)

    # Tokenize input text
    inputs = tokenizer(text, max_length=512, truncation=True, return_tensors="pt", padding="max_length").to(device)

    # Generate summary
    summary_ids = model.generate(
        inputs['input_ids'],
        max_length=200,
        num_beams=5,
        no_repeat_ngram_size=2,
        early_stopping=True
    )

    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# Define device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Move model to device
model.to(device)

# Generate summaries for validation data
val_df['generated_summary'] = val_df['text'].apply(lambda x: generate_summary(x, model, tokenizer, device))

# Compute ROUGE score
rouge = evaluate.load('rouge')
references = val_df['summary'].tolist()
predictions = val_df['generated_summary'].tolist()
final_rouge_score = rouge.compute(predictions=predictions, references=references)

# Print results
print(f"Final ROUGE Score: {final_rouge_score}")

# Save output
val_df.to_excel("/content/drive/My Drive/Thesis_Dataset/test_data_with_summaries_and_rouge_NLLB.xlsx", index=False)

# Display sample results
print(val_df[['text', 'summary', 'generated_summary']])
