{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8841b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02de2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "from transformers import MBartForConditionalGeneration, AutoTokenizer,MBart50Tokenizer,MBart50TokenizerFast,AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bac28c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Your Custom Summary Function\n",
    "def generate_summary(text, model, tokenizer, device):\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, max_length=512, truncation=True, return_tensors=\"pt\", padding=\"max_length\").to(device)\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=200,\n",
    "        min_length=100,\n",
    "        length_penalty=1.0,\n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=4,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9832e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/content/drive/My Drive/Thesis_Dataset/fine_tuned_bangla_t5\"\n",
    "\n",
    "# Load tokenizer and model correctly\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load transcribed text from Excel\n",
    "input_excel = \"/content/drive/MyDrive/Thesis_Transcription/Text_Output/chunk_transcripts.xlsx\"\n",
    "df = pd.read_excel(input_excel)\n",
    "\n",
    "# Apply your summary function to each row\n",
    "print(\"ðŸ“„ Generating summaries for all transcribed chunks...\")\n",
    "df['Summary'] = df['Text'].progress_apply(lambda x: generate_summary(str(x), model, tokenizer, device))\n",
    "\n",
    "# Save to new Excel\n",
    "output_excel_path = \"/content/drive/MyDrive/Thesis_Transcription/Text_Output/text_with_summaries_banglat5.xlsx\"\n",
    "df.to_excel(output_excel_path, index=False)\n",
    "\n",
    "print(f\"âœ… Excel with summaries saved: {output_excel_path}\")\n",
    "\n",
    "# Save all summaries as one merged text\n",
    "merged_summary_text = \"\\n\".join(df['Summary'].dropna().astype(str).tolist())\n",
    "summary_txt_path = \"/content/drive/MyDrive/Thesis_Transcription/Text_Output/merged_summary_banglat5_without_regex.txt\"\n",
    "with open(summary_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(merged_summary_text)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
